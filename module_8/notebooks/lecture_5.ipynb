{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **Лекция 5:** Компьютерное зрение. Задача Object Detection. YOLO, YOLOv2, YOLO9000, YOLOv3/v4. </center>\n",
    "----\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/yolo_img.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __План на сегодня:__\n",
    "\n",
    "----\n",
    "### 1. _YOLO и развитие YOLO. Структура и архитектура YOLOv2, YOLOv3/v4 детекторов._\n",
    "### 2. _Darknet-19 и Darknet-53. Репозиторий ultralytics._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# YOLO (You Only Look Once):\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/yolo_detection.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\"Новый\" подход к решению задачи Object Detection - вместо того, чтобы использовать классификаторы для задач обнаружения объектов (как в DPM, R-CNN, Fast R-CNN etc.) мы можем определить задачу регрессии к боксам (bounding boxes) и связанным с ними вероятностями классов (class probabilities).\n",
    "\n",
    "Сама модель YOLO состоит из одной CNN, которая делает предсказания за один проход и может быть использована и обучена в варианте end-to-end. Базовая модель может работать real-time на __скорости 45 fps__ более улучшенные версии достигают 155 fps и выше обходя другие SOTA детекторы по метрике __mAP__. Но есть нюанс, YOLO менее точен в локализации объектов, однако выдает меньше ложных срабатываний (FP, false positives).\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### Unified Detection:\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/unified_detection.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "Какова общая идея:\n",
    "\n",
    "Объединяем все компоненты детектора в единую сеть, что позволит нам:\n",
    "\n",
    "1. Обучить модель end-to-end.\n",
    "2. Делать предсказания за один проход и работать real-time.\n",
    "3. Использовать контекстную информацию со всего изображения для предсказания боксов.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "----\n",
    "\n",
    "### Grid Cell:\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/grid_cell.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "YOLO разбивает входящее изображение на сетку размером __MxM__. Если центр объекта попадает в одну из ячеек сетки, то данная ячейка будет ответственна за детекцию этого объекта.\n",
    "\n",
    "Каждая ячейка предсказывает фиксированное число B боксов (bounding boxes) и оценок (box confidence scores) для них. При это независимо от числа B в каждой ячейке может детектиться одновременно только и только один объект. Кроме того, каждая ячейка предсказывает C условных вероятностей для класса объекта (conditional class probabilities). Идея всего этого механизма показана на скриншоте:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/yolodetections.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "Каждый такой бокс состоит из пятерки координат __[x, y, w, h, confidence_score]__:\n",
    "\n",
    "\n",
    "\n",
    "* Координаты (x,y) - центр бокса относительно границ ячейки (т.е. показываю смещение)\n",
    "* Координаты (w, h) - нормализованы относительно всего изображения.\n",
    "* Все координаты находятся в диапазоне от [0, 1].\n",
    "* _Box Confidence score_ отражает уверенности модели в том, что бокс содержит объект и как точно этот объект локализован.\n",
    "* _Conditional Class Probabilities_ описывает вероятности того, к какому классу принадлежит объект (для каждой клетки одна вероятность на каждый класс независимо от числа боксов B).\n",
    "* _Class Confidence Score_ вычисляется на этапе теста для каждого бокса и дает итоговую оценку отражающую точность классификации объекта и точность локализации объекта.\n",
    "\n",
    "\n",
    "\n",
    "Как устроен весь процесс:\n",
    "\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/box_confidence.png> </center>\n",
    "\n",
    "\n",
    "\n",
    "<center> <img src=./../src/imgs/yolopipeline.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "В базовом варианте пайплайна для PASCAL VOC использовались параметры S=7, B=2, C=20 что приводит к размеру выходного тензора: (S, S, Bx5 + C) = (7, 7, 2x5 + 20) = (7, 7, 30)\n",
    "\n",
    "----\n",
    "\n",
    "### Дизайн Архитектуры:\n",
    "\n",
    "----\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/yoloarch.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "В качестве модели YOLO использует CNN состоящую из 24 сверточных слоев и 2 полносвязных слоя. Архитектура сети основана на GoogleNet, где inception модуль заменен на чередующиеся слои 1x1 сверток (для понижения размерности в глубину карт активаций) с последующим 3х3 свертками. Вход сети имеет размер 448х448, а выход представлен в виде тензора 7х7х30.\n",
    "\n",
    "Кроме того, есть еще более быстрая версия YOLO, которая состоит из 9 сверточных блоков вместо 24, использует ResNet и имеет меньшую глубину сверток.\n",
    "\n",
    "В качестве функции активации используется __leaky ReLU__ и во время обучения используется __dropout__ и аугментация данных (сдвиги, масштабирование до 20%, изменения цвета, насыщенности в пространстве HSV).\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### Loss Function - \"ответственный\" предиктор чей бокс имеет максимальный IoU с ground truth:\n",
    "\n",
    "----\n",
    "\n",
    "YOLO предсказывает несколько боксов для каждой ячейки, т.е. имеет несколько предикторов для ячейки, но во время обучения, только один предиктор отвечает за детект каждого объекта (и соответственно вносит вклад в loss). Для этого, \"ответственным\" назначается тот предиктор, чей бокс имеет максимальный IoU с ground truth. Это приводит к специализации этих предикторов: каждый предиктор обучается лучше предсказывать определенные размеры, пропорции и классы объектов, улучшая тем самым итоговую точность детекции.\n",
    "\n",
    "__YOLO исопльзует Sum of Squared Errors (SSE)__ в качестве loss function. Полная функция потерь состоит из:\n",
    "\n",
    "1. Classification Loss.\n",
    "2. Localization Loss.\n",
    "3. Confidence Loss.\n",
    "\n",
    "\n",
    "Пройдемся по всем:\n",
    "\n",
    "\n",
    "__Classification Loss:__\n",
    "\n",
    "Согласно названию отвечает за классификацию задетекченного объекта:\n",
    "\n",
    "#### $\\sum_{i=0}^{s^2} 1_i^{obj} \\sum_{c in classes} (p_i(c) - \\^{p_i}(c))^2$\n",
    "\n",
    "\n",
    "\n",
    "* $1_i^{obj}$ - равен 1, если объект присутствует в ячейке i иначе 0\n",
    "* $\\^{p_i}(c)$ - вероятность того, что объект класса с присутствует в ячейке i\n",
    "\n",
    "\n",
    "__Localization Loss:__\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/localization_loss.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "Измеряет ошибку предсказания координат.\n",
    "\n",
    "\n",
    "* $1_i^{obj}$ - равен 1, если предиктор j в ячейке i \"ответственен\" За детекцию этого объекта иначе 0\n",
    "* $\\lambda_{coord}$ - нормировочный коэффициент (равен 5)\n",
    "\n",
    "\n",
    "__Confidence Loss:__\n",
    "\n",
    "Измеряет ошибку \"объектности\", то есть то что в предсказанном боксе действительно находится искомый объект:\n",
    "\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/confidence_loss.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "__Full Loss:__\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/fullLoss.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение YOLO происходит в 2 этапа:\n",
    "\n",
    "1. Сеть предобучается на задаче классификации (ImageNet) со входом 224х224.\n",
    "2. Затем полносвязные слои заменяются сверточными, вход увеличивается до 448х448 и сеть дообучивают на задаче детекции (VOC или COCO).\n",
    "\n",
    "YOLO может выдавать несколько детекций для одного и того же объекта. Поэтому к итоговым детекциям применяется __Non-maximum suppression (NMS) что дает прирост на 2-3% в среднем к mAP:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/nms.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "Минусы YOLO:\n",
    "\n",
    "Из-за своей архитектуры (grid cell) YOLO имеет ограничения на детекцию близких друг к другу небольших объектов (например толпа лиц, стая животных и тд).\n",
    "\n",
    "\n",
    "Преимущества:\n",
    "1. Очень быстрая детекция, так как задача решается за один проход через сеть, и не используется sliding window или region proposals.\n",
    "2. Модель может быть обучена end-to-end.\n",
    "3. Используется контекст объектов, так как сеть обрабатывает изображение целиком.\n",
    "4. Хорошее обобщение на объекты из других доменов (мультики, растения, объекты которые мало встречались и тд).\n",
    "5. Дает меньш false positive срабатываний.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# YOLOv2 (YOLOv2/YOLO9000) - Better, Faster, Stronger:\n",
    "\n",
    "----\n",
    "\n",
    "Первая версия - недостаток - более низкая точность локализации объектов по сравнению с Fast/Faster R-CNN а также более низкий recall по сравнению с region proposals подходами. Поэтому было решено сфоркусироваться на улучшении recall и точности локализации при сохранении хорошей точности классификации и высокой скорости детекции.\n",
    "\n",
    "Пройдемся по улучшениям:\n",
    "\n",
    "____\n",
    "\n",
    "### 1. Batch Normalization:\n",
    "\n",
    "____\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/BN.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Добавили Batch Normalization во все сверточные слои, что усилило регуляризацию сети, и устранило необходимость в использовании dropouts что привело к увеличению mAP на 2%\n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "### 2. High Resolution Classifier:\n",
    "\n",
    "____\n",
    "\n",
    "Первая версия YOLO:\n",
    "\n",
    "* Обучаем на Imagenet со входом 224х224\n",
    "* Увеличиваем вход до 448x448 и дотюниваем на детекцию на VOC или COCO\n",
    "\n",
    "\n",
    "YOLOv2:\n",
    "\n",
    "* Сеть обучается на задаче классификации (ImageNet) со входом 224х224\n",
    "* Затем вход увеличивается до 448х448 и сеть обучается еще несколько эпох\n",
    "* После сеть дотюнивается на задаче детекции (VOC/COCO)\n",
    "\n",
    "\n",
    "Плюс еще увеличили mAP на 4%\n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "### 3. Convolutional with Anchor Boxes:\n",
    "\n",
    "____\n",
    "\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/CNNcan.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "На ранних эпохах обучения YOLO страдает от нестабильности градиента, так как пытается предсказывать боксы произвольной формы. Эти предсказания могут быть точными для одних объектов и неточными для других, что приводит к изменениям в градиенте. Кроме того, на ранних этапах обучения предикторы соревнуются друг с другом за специализацию на определенных формах боксов. В реальных ситуациях объекты не так разнообразны: машины похожи, пешеходы имеют примерно одинаковую форму и соотношение сторон, поэтому обучение будет нестабильным, если модель будет использовать несколько наиболее общих форм боксов.\n",
    "\n",
    "Что в итоге сделали:\n",
    "\n",
    "Позаимствовали идею из Faster-R-CNN и начали использовать якорные боксы (anchor boxes). Вместо предсказания боксов произвольных форм YOLO предсказывает смещения для этих якорных боксов. Если ограничить диапазоны этих смещений то можно сохранить разннобразие форм и заставить каждый якорный бокс специализироваться на конкретной форме, что в итоге приведет к более стабильному градиенту модели на разных этапах.\n",
    "\n",
    "Для этих целей, были измененые следующие части архитектуры:\n",
    "1. Удалили все полносвязные слои ответственные за предсказание боксов.\n",
    "2. Предсказание класса объекта вносится с уровня ячейки, и предсказывается на уровне бокса. Каждое предсказание включает в себя: 4 координаты бокса, оценку уверенности и 20 вероятностей для классов объекта (VOC). Таким образом каждая ячейка предсказывает 5х(4+1+20) = 125 значений, а выходной тензор будет иметь форму 7х7х125\n",
    "3. Размер входа сети изменяется с 448х448 на 416х416 для того, чтобы иметь нечетное число ячеек (7х7 вместо 8х8) так как центр изображения содержит чаще всего большой объект, нечетное количество ячеек помогает упростить его детекцию.\n",
    "4. Выходной размер сети увеличивется с 7х7 на 13х13 путем удаления одного pooling слоя.\n",
    "\n",
    "\n",
    "__Как выбрать якорные боксы (anchor boxes)__:\n",
    "\n",
    "Боксы выбираются при помощи кластеризации. Запускается K-means и центры кластеров формируют кластерные боксы.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/clusters.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "Подрбоная идея: Для того, чтобы найти топ-5 якорных боксов, которые лучше всего покрывают датасет, используется K-Means кластеризация и находятся центроиды для топ-k кластеров (5). Поскольку кластеризация идет на боксах а не на точках, обычное евклидово расстояние не очень подходит для вычисления расстояния между боксами и центрами кластеров. Поэтому используется метрика\n",
    "\n",
    "### $IoU_{d(box centroid) = 1 - IoU_{box_centroid}}$\n",
    "\n",
    "С повышением количества кластеров (якорных боксов) точность растет, однако растет и вычислительная сложность детектора.\n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "### 4. Direct Location Prediction:\n",
    "\n",
    "____\n",
    "\n",
    "Проблема с которой столкнулись при использовании якорных точек - нестабильность модели на начальных итерациях обучения (в основном из-за предсказания (x,y) координаты бокса.  Если вспомним подход Regional Proposal Network координаты вычислялись как:\n",
    "\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/drp.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "D YOLOv2 подход основан на предсказании пятерки (tx, ty, tw, th, to) и применении сигмоид для ограничения диапазона значений.\n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "### 5. Fine Grained Features:\n",
    "\n",
    "____\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/fgf.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Passthrough слой конкатенирует признаки из более ранних слоев с выходных слоем: 26х26х512 --> 13х13х2048 || 13х13х1024 = 13х13х3072\n",
    "\n",
    "____\n",
    "\n",
    "### 6. Multi-Scale Training:\n",
    "\n",
    "____\n",
    "\n",
    "После удаления всех полносвязных слоев сеть YOLOv2 становится Fully Convolutional Network (FCN) поэтому может работать на различных входных разрешениях.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/SOTAOD.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Darknet:\n",
    "\n",
    "----\n",
    "\n",
    "В YOLOv2 использовалась архитектура Darknet-19 которая имеет 5.5 млрд параметров (против 8.5 у первой версии). Точность достигает 92% на ImageNet. Архитектура для задач классификации Darknet-19 выглядит следующим образом:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/darknet.png> </center>\n",
    "\n",
    "<center> <img src=./../src/imgs/darknet_19.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "__Обучение__:\n",
    "\n",
    "1. Сеть предобучается на ImageNet-1000 на разрешении 224х224 (160 эпох)\n",
    "2. Затем вход увеличивается до 448х448 и сеть обучается еще 10 эпох\n",
    "3. Полносвязные слои удаляются, добавляется Passthrough слой и модель дотюнивается на задаче детекции 160 эпох (на COCO/VOC/OpenImages)\n",
    "\n",
    "\n",
    "Датасеты для детекции объектов имеют гораздо меньше классов чем датасеты для классификации. Чтобы увеличить число классов которые YOLOv2 может детектить начали использовать во время обучения оба типа датасетов. Когда встречается изображение из классификации, то градиент идет только через loss классификатора, а в качестве \"ответственного\" бокса выбирается тот, который выдал больший class probability. Используется несколько softmax.\n",
    "\n",
    "Возникает проблема: Как слить метки классов (labels) из разных датасетов? Для этого все классы объединяют в иерархическую структуру __WordTree__:\n",
    "\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/wordtree.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "____\n",
    "\n",
    "# YOLO9000:\n",
    "\n",
    "----\n",
    "\n",
    "Простое расширение модели YOLOv2 на 9000 классов.\n",
    "\n",
    "Используют датасеты:\n",
    "1. MS COCO (200 классов)\n",
    "2. TOP-9000 классов из ImageNet (44 класса пересекаются с COCO).\n",
    "\n",
    "\n",
    "YOLO9000 выдает mAP равный 19.7 для всех классов и mAP равный 16.0 для 156 классов которых нет в COCO.\n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "# YOLOv3/v4/v5:\n",
    "\n",
    "----\n",
    "\n",
    "YOLOv3/v4 решает задачу Multi-LAbel classification и использует logistic classifiers с binary cross entropy loss.\n",
    "\n",
    "\n",
    "__Softmax vs LogReg__:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/softmaxlogreg.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__Bounding Box Prediction__:\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/yolov3.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__Predictions Across Scales__:\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/yolov3_1.png> </center>\n",
    "<center> <img src=./../src/imgs/yolov3_3.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "__Darknet-53 / DarkNet-17__\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/darknet_53_17.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "\n",
    "# [https://www.youtube.com/watch?v=MPU2HistivI](https://www.youtube.com/watch?v=MPU2HistivI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:14:45.628293500Z",
     "start_time": "2023-09-25T14:14:45.135708200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob as glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:14:48.728156200Z",
     "start_time": "2023-09-25T14:14:48.721145700Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "# Number of epochs to train for.\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:14:50.495608700Z",
     "start_time": "2023-09-25T14:14:50.489595700Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_file(url, save_name):\n",
    "    url = url\n",
    "    if not os.path.exists(save_name):\n",
    "        file = requests.get(url)\n",
    "        open(save_name, 'wb').write(file.content)\n",
    "    else:\n",
    "        print('File already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:15:02.448987700Z",
     "start_time": "2023-09-25T14:14:52.653783500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0   893    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100   893  100   893    0     0    893      0  0:00:01  0:00:01 --:--:--   657\n",
      "\n",
      "  0 38.4M    0  3262    0     0   1631      0  6:51:50  0:00:02  6:51:48  1631\n",
      "  6 38.4M    6 2560k    0     0   853k      0  0:00:46  0:00:03  0:00:43 3478k\n",
      " 92 38.4M   92 35.4M    0     0  9069k      0  0:00:04  0:00:04 --:--:-- 20.0M\n",
      "100 38.4M  100 38.4M    0     0  9839k      0  0:00:04  0:00:04 --:--:-- 20.8M\n",
      "curl: (6) Could not resolve host: ;\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0curl: (6) Could not resolve host: unzip\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: roboflow.zip;\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0curl: (6) Could not resolve host: rm\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: roboflow.zip\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'train/images/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m dirs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dir_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dirs):\n\u001b[1;32m----> 7\u001b[0m     all_image_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdir_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/images/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, image_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_image_names):\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (j \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'train/images/'"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('/train'):\n",
    "    !curl -L \"https://public.roboflow.com/ds/xKLV14HbTF?key=aJzo7msVta\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
    "\n",
    "    dirs = ['train', 'valid', 'test']\n",
    "\n",
    "    for i, dir_name in enumerate(dirs):\n",
    "        all_image_names = sorted(os.listdir(f\"{dir_name}/images/\"))\n",
    "        for j, image_name in enumerate(all_image_names):\n",
    "            if (j % 2) == 0:\n",
    "                file_name = image_name.split('.jpg')[0]\n",
    "                os.remove(f\"{dir_name}/images/{image_name}\")\n",
    "                os.remove(f\"{dir_name}/labels/{file_name}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
