{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **Лекция 6:** Компьютерное зрение. Задача Landmark Detection. </center>\n",
    "----\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/landmark_intro_img.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __План на сегодня:__\n",
    "\n",
    "----\n",
    "### 1. _Задача Landmark Detection. Human Pose Estimation. 2D и 3D Landmarks._\n",
    "### 2. _Facial Landmarks Detection._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1:\n",
    "\n",
    "____\n",
    "\n",
    "# _Задача Landmark Detection. Human Pose Estimation. 2D и 3D Landmarks._\n",
    "\n",
    "----\n",
    "\n",
    "Landmarks (точки-ориентиры) - это ключевые точки, которые привязаны к определенным частям объекта (человека, лица, зданий, скульптур и т.д.).\n",
    "\n",
    "Начнем разбор задачи Landmark Detection с задачи __Human Pose Estimation__. Задача __Human Pose Estimation__ определяется как проблема локализации сочленений, суставов (joints), которые также известны как ключевые точки - локти, запястья и т.д., в изображениях или видеопотоке. Также задачу можно свести к поиску конкретной позы в пространстве всех поз (то есть классификации). Задачу можно решать в разной геометрии:\n",
    "\n",
    "* __2D Pose Estimation__ - то есть 2D пространство, где у нас есть набор координат (x,y) для входного изображения.\n",
    "* __3D Pose Estimation__ - то есть 3D пространство, где у нас есть набор (x,y,z) координат для входного изображения RGB.\n",
    "\n",
    "\n",
    "\n",
    "Где все это может применяться:\n",
    "\n",
    "1. Action Recognition.\n",
    "2. Animation - Motion Capture.\n",
    "3. Sport - Action Tracking, Situational Judgement (Football - VAR, Basketball etc).\n",
    "4. 2D / 3D Reconstruction (Archeology, Science, Cinema).\n",
    "\n",
    "\n",
    "[Пример работы 1.](https://www.youtube.com/watch?v=RMgrAxds3DU)\n",
    "[Пример работы 2.](https://www.youtube.com/watch?v=Dhkd_bAwwMc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# _2D Pose Estimation._\n",
    "\n",
    "----\n",
    "\n",
    "Начнем разбор задачи с 2D Pose Estimation, когда нам необходимо проставить (x,y) координаты для найденного объекта и определить позу. Но прежде чем коснуться этой задачи поговорим как всегда о том, как оценивать качество реализованного решения для таких задач.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__Метрики__:\n",
    "\n",
    "1. __PCP@0.5__ (_Percentage of Correct Parts_) - показывает какой % картинок с правильно обнаржуенными джойнтами (конечностями, соединениями). Считается отдельно для каждого джойнта (конечности, соединения), затем усредняется. __Джойнт обнаружен__ если расстояние между двумя предполагаемыми местоположениями суставов и истинными местоположениями суставов конечностей составляет не более половины длины (0.5) джойнта (конечности, соединения). Суть:\n",
    "    * Она штрафует более короткие конечности в большей степени, поскольку у более коротких конечностей пороги меньше.\n",
    "    * Чем выше показатель/метрика PCP тем лучше модель.\n",
    "\n",
    "\n",
    "2. __PCK__ (_Percentage of Correct Key-Points) - предсказанный джойнт (сустав, конечность, соединение) считается правильным, если расстояние между прогнозируемым и истинным местоположением джойнта находится в пределах определенного порога (задается на джойнт - например для головы, руки, ноги и тд). Порог может быть следующим:\n",
    "    * __PCKh@0.5__ - расстояние между предсказанной точкой и GroundTruth < 50% (head ling)\n",
    "    * __PCK@0.2__ - расстояние между предсказанной точкой и GroundTruth < 0.2  (torso diameter)\n",
    "    * __PCK__ используется как для 2D так и для 3D (PCK3D)\n",
    "    * Чем выше тем лучше модель.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Раньше задачу решали классическим ML, но из-за высокого потребления ресурсов и низкого качества, практически нигде не использовалось. Пока не научились решать задачу в контексте глубокого обучения.\n",
    "\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "----\n",
    "### __Deep Pose: Human Pose Estimation via Deep Neural Networks (CVPR'14)__:\n",
    "----\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/deepposeestimation.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Самое просто что можно сделать для решения такой задачи - у нас есть картинка на входе, нам нужно выдать например 16 локализаций точек. Что будем делать и как решать - взял просто в качестве экстрактора признаков CNN и решить задачу регрессии для (x,y) джойнтов и обучить алгоритм, таким образом на выходе мы получим k-точек. Звучит просто.\n",
    "\n",
    "[В изначальном подходе CVPR'14:](https://arxiv.org/pdf/1312.4659.pdf)\n",
    "\n",
    "__Первый шаг (Initial Stage)__:\n",
    "1. В качестве CNN бекенда выступала AlexNet (7 слоев) + 1 дополнительный регрессионный слой, который дает на выходе __2*k__ совместных джойнтов, где k - количество точек необходимы для локализации.\n",
    "2. Модель обучалась используя L2 loss функцию для регрессионной компоненты.\n",
    "\n",
    "__Второй шаг (Stage S)__:\n",
    "1. После того как предсказали (x,y) точку для джойнта, эта точка отдается дальше для решения задачи Refinement то есть определения области \"взаимодействия\" данной точки (задача схожа с Bounding Box регрессией).\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Но такая архитектура плохо справлялась с отдельными джойнтами и вовсе не детектила их. Проблема - отсутствие глобального контекста и невозможность хранить весь объем информации о пространстве признаков всего изображения (особенно для регрессии когда нам надо предсказывать огромного количество данных).\n",
    "\n",
    "\n",
    "Как попытались решить проблему эффективности дальше:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "----\n",
    "### [__Efficient Object Localization Using Convolutional Neural Networks (CVPR'15)__](https://arxiv.org/pdf/1411.4280.pdf):\n",
    "----\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/heatmapcvpr.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Предложили другой подход вместо регрессии на 2*k точек. Попытались внедрить глобальный контекст. Реализовали подход через heatmaps, где каждая подсвеченная область на карте изображения соответствует одной лендмарке. Эти heatmaps получаются в результате предсказания нейронной сети. Дальше мы можем их смержить в одно предсказание, которое будет представлять позу объекта на изображении.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Кратко как вся эта архитектура обучается:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/cvpr15.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Выходом модели будет K-тепловых карт размера $W_0 \\times H_0, {H_1, H_2, ..., H_k}$ где каждая тепловая карта $H_k$ соответствует локализации k-го ландмарка с определенной степенью уверенности.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "----\n",
    "### [__Convolutional Pose Machines (CVPR'16)__](https://arxiv.org/pdf/1602.00134.pdf):\n",
    "----\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/convolutional_pose_machines_1.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/convolutional_pose_machines_2.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "* Мультишаговая архитектура (несколько задач).\n",
    "* Используем промежуточное обучение с учителем.\n",
    "* Работает с пространственной информацией (spatial information) используя большие группы рецептивных полей (receptive fields).\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Кратко как вся эта архитектура обучается:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/convolutional_pose_machines_arrch.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Как обстоят дела с SOTA бенчмарками:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/SOTA_pose.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "На каком датасете все учится и проверяется (Human Pose Estimation задача) - [MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/)\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "----\n",
    "### [__Stacked Hourglass Networks for Human Pose Estimation__](https://arxiv.org/pdf/1603.06937.pdf):\n",
    "----\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/hourglass_networks.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Альтернативная идея работы с глобальным контекстом. Логика - сжать представление до самого маленького пространства, которое даст максимальную информативность для локализации точки и добавить скип коннекшн чтобы не потерять основную часть информации. В конце получаем набор хитмапов, которые дают наиболее локализованные и точные точки для детекта позы.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Идея архитектуры:\n",
    "* Используем скип коннекшены для того чтобы сохранять пространственную информацию\n",
    "* Используем промежуточное обучение с учителем\n",
    "* Вся сеть набор сверток с остаточными модулями (residual modules)\n",
    "* Можем забирать как локальный так и глобальный контекст\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Как выглядит архитектура такого блока:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/hourglass_architecture.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Ссылки на решения:\n",
    "\n",
    "* [Реализация для TensorFlow](https://github.com/ethanyanjiali/deep-vision/tree/master/Hourglass/tensorflow)\n",
    "* [Библиотека MMPose которая содержит архитектуру для PyTorch и другие модели](https://github.com/open-mmlab/mmpose)\n",
    "\n",
    "Минусы решения:\n",
    "1. Проблема с детектом для нескольких объектов в кадре.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "----\n",
    "### [__Deep High-Resolution Representation Learning for Human Pose Estimation (HRNet) (CVPR’19)__](https://arxiv.org/pdf/1902.09212.pdf):\n",
    "----\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/HRNET.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "* Лучше по метрикам чем Hourglass\n",
    "* Используем деконволюции вместо апсэмплинга (transopes convolutions)\n",
    "* Такая архитектура начинается с нейронной сети для карт признаков выского разрешения (первый этап), добавляем модули которые отвечают за генерацию карт признаков для низкого разрешения (второй этап) и соединяем это все мульти-признаковую параллельную нейронную сеть.\n",
    "* Задача генерации тепловых карт точек (landmarks) решается путем регрессии используя MSE в качестве функции потерь.\n",
    "\n",
    "[Реализация](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch)\n",
    "\n",
    "\n",
    "----\n",
    "### [__ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation (CVPR’21)__](https://arxiv.org/pdf/2204.12484.pdf):\n",
    "----\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/vitpose.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация и использование технологии для детекта позы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модули и библиотеки:\n",
    "\n",
    "Нам понадобится библиотека [MediaPipe](https://google.github.io/mediapipe/) которая содержит огромное количество кросс-платформерного функционала а также реализаций кастомных моделей. В настоящее время используется одна из SOTA моделей [BlazeFace](https://arxiv.org/pdf/1907.05047.pdf) автором архитектуры которой является [Андрей Вакунов](https://www.linkedin.com/in/andrey-vakunov-25194022/) (альма-маттер - Гродненский Государственный Университет Янки Купалы).\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "```\n",
    ">>> pip install mediapipe opencv-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:43:26.618783500Z",
     "start_time": "2023-09-25T14:43:23.433542600Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Функционал для рендеринга позы и точек landmarks\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Раз у нас есть вебкамера, то вместо картинок давайте поиграемся с live-видео и посмотрим как это выглядит в живую:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:42:38.192931400Z",
     "start_time": "2023-09-25T14:42:01.825537400Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m cap \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mVideoCapture(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;66;03m# аргумент - номер девайса вебкамеры\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m cap\u001B[38;5;241m.\u001B[39misOpened():\n\u001B[1;32m----> 5\u001B[0m     ret, frame \u001B[38;5;241m=\u001B[39m \u001B[43mcap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# читаем фрейм (кусочек кадра с видеопотока)\u001B[39;00m\n\u001B[0;32m      6\u001B[0m     cv2\u001B[38;5;241m.\u001B[39mimshow(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mФрейм: \u001B[39m\u001B[38;5;124m'\u001B[39m, frame)\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m cv2\u001B[38;5;241m.\u001B[39mwaitKey(\u001B[38;5;241m10\u001B[39m) \u001B[38;5;241m&\u001B[39m \u001B[38;5;241m0xFF\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mord\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mq\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Захват видеопотока вебкамерой\n",
    "cap = cv2.VideoCapture(0) # аргумент - номер девайса вебкамеры\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read() # читаем фрейм (кусочек кадра с видеопотока)\n",
    "    cv2.imshow('Фрейм: ', frame)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Реализуем детекции с использованием уже готовой модели для Pose Estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:44:13.075420300Z",
     "start_time": "2023-09-25T14:43:30.790311400Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "## Инициализируем экземпляр mediapipe который вызовет модель BlazeFace для задачи Pose Estimation\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        # Перевод картинги из BGR > RGB не забывайте всегда переводить цветовые каналы\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Детект позы:\n",
    "        results = pose.process(image)\n",
    "        # Обратный перевод в BGR для визуализации\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Рендеринг направлений\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:44:16.749958Z",
     "start_time": "2023-09-25T14:44:16.742376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({(0, 1),\n",
       "           (0, 4),\n",
       "           (1, 2),\n",
       "           (2, 3),\n",
       "           (3, 7),\n",
       "           (4, 5),\n",
       "           (5, 6),\n",
       "           (6, 8),\n",
       "           (9, 10),\n",
       "           (11, 12),\n",
       "           (11, 13),\n",
       "           (11, 23),\n",
       "           (12, 14),\n",
       "           (12, 24),\n",
       "           (13, 15),\n",
       "           (14, 16),\n",
       "           (15, 17),\n",
       "           (15, 19),\n",
       "           (15, 21),\n",
       "           (16, 18),\n",
       "           (16, 20),\n",
       "           (16, 22),\n",
       "           (17, 19),\n",
       "           (18, 20),\n",
       "           (23, 24),\n",
       "           (23, 25),\n",
       "           (24, 26),\n",
       "           (25, 27),\n",
       "           (26, 28),\n",
       "           (27, 29),\n",
       "           (27, 31),\n",
       "           (28, 30),\n",
       "           (28, 32),\n",
       "           (29, 31),\n",
       "           (30, 32)})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks\n",
    "mp_pose.POSE_CONNECTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Реализуем определение джойнтов (joints):\n",
    "\n",
    "Откуда можно взять лейблы точек [https://google.github.io/mediapipe/solutions/pose.html](https://google.github.io/mediapipe/solutions/pose.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:44:39.287148Z",
     "start_time": "2023-09-25T14:44:23.912717600Z"
    }
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "## Инициализируем экземпляр mediapipe который вызовет модель BlazeFace для задачи Pose Estimation\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        # Перевод картинги из BGR > RGB не забывайте всегда переводить цветовые каналы\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Детект позы:\n",
    "        results = pose.process(image)\n",
    "        # Обратный перевод в BGR для визуализации\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Выявим лендмарки:\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark # лежит все тут\n",
    "            # print(landmarks)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Рендеринг направлений\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:44:41.521003300Z",
     "start_time": "2023-09-25T14:44:41.516840800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:44:43.206098800Z",
     "start_time": "2023-09-25T14:44:43.201733900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "for lndmrk in mp_pose.PoseLandmark:\n",
    "    print(lndmrk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:44:45.512569900Z",
     "start_time": "2023-09-25T14:44:45.509162600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9994725584983826"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:44:47.576654Z",
     "start_time": "2023-09-25T14:44:47.573284600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 0.91986144\n",
       "y: 0.94500583\n",
       "z: -0.44118562\n",
       "visibility: 0.7961808"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:44:49.755867900Z",
     "start_time": "2023-09-25T14:44:49.752529700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 1.0874245\n",
       "y: 1.2875625\n",
       "z: -0.55753887\n",
       "visibility: 0.6846311"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Посчитаем углы между плечом и локтем для того чтобы добавить функционал для расчета подходов для поднимания гантелей :)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:44:52.211363700Z",
     "start_time": "2023-09-25T14:44:52.209349800Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "\n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "\n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:44:54.325719500Z",
     "start_time": "2023-09-25T14:44:54.323709600Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Доастанем координаты joint'ов для того чтобы реализовать функционал\n",
    "shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:44:56.683962300Z",
     "start_time": "2023-09-25T14:44:56.680744100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.7339316606521606, 0.646515965461731],\n",
       " [0.9198614358901978, 0.9450058341026306],\n",
       " [1.0874245166778564, 1.2875624895095825])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shoulder, elbow, wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:44:59.108055800Z",
     "start_time": "2023-09-25T14:44:59.105533200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174.14693135746353"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_angle(shoulder, elbow, wrist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:45:01.633153200Z",
     "start_time": "2023-09-25T14:45:01.628335300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(588, 453)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(np.multiply(elbow, [640, 480]).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим и псмотрим что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:45:53.300598600Z",
     "start_time": "2023-09-25T14:45:06.300167300Z"
    }
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Get coordinates\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "\n",
    "            angle = calculate_angle(shoulder, elbow, wrist)\n",
    "\n",
    "            cv2.putText(image, str(angle),\n",
    "                           tuple(np.multiply(elbow, [640, 480]).astype(int)),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Реализуем счетчик подходов для поднимания гантели на бицепс :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Curl counter variables\n",
    "counter = 0\n",
    "stage = None\n",
    "\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "\n",
    "            angle = calculate_angle(shoulder, elbow, wrist)\n",
    "\n",
    "            cv2.putText(image, str(angle),\n",
    "                           tuple(np.multiply(elbow, [640, 480]).astype(int)),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "            # Посчитаем подходы :\n",
    "            if angle > 160:\n",
    "                stage = \"down\"\n",
    "            if angle < 30 and stage =='down':\n",
    "                stage=\"up\"\n",
    "                counter +=1\n",
    "                print(counter)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Создадим бокс который будет отображать результат :)\n",
    "        cv2.rectangle(image, (0,0), (225,73), (245,117,16), -1)\n",
    "\n",
    "        cv2.putText(image, 'REPS', (15,12),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(counter),\n",
    "                    (10,60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.putText(image, 'STAGE', (65,12),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, stage,\n",
    "                    (60,60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практика: Добавьте функционал еще и на правую руку и выведите результат отдельно для левой и для правой руки:\n",
    "\n",
    "```\n",
    "--------\n",
    "Левая:  0\n",
    "Правая: 1\n",
    "--------\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# _3D Pose Estimation._\n",
    "\n",
    "----\n",
    "\n",
    "Почему 3D сложнее чем 2D :\n",
    "1. Огромное количество вариаций точек\n",
    "2. Недостаток 3D датасетов реальных данных\n",
    "3. Сложность определения джойнтов и ландмарков\n",
    "\n",
    "\n",
    "__Какие метрики используются:__\n",
    "\n",
    "__MPJPE (Mean Per Joint Position Error) -- это L2 расстояние, усредненное по всем лэндмаркам__\n",
    "\n",
    "\n",
    "Что имеем из исследований на данный момент:\n",
    "\n",
    "----\n",
    "### [__Unsupervised Geometry-Aware Representation for 3D Human Pose Estimation  (ECCV'19)__](https://arxiv.org/pdf/1804.01110.pdf):\n",
    "----\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/3dPose.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "В чем идея:\n",
    "1. Используем автоэнкодер для того чтобы научится 3D представлениям\n",
    "2. Используем обучение с учителем для маппинга таких репрезентаций с позами в задаче 3N регрессии где N - количество джойнтов.\n",
    "\n",
    "Во время инференса, расчитывается все латентные представления изображения и отдаем в качестве инпута нейронной сетке для того чтобы рассчитать позу.\n",
    "\n",
    "\n",
    "Как происходит обучение репрезентации:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/representation_learning_pose.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "[Реализация](https://github.com/hrhodin/UnsupervisedGeometryAwareRepresentationLearning)\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "----\n",
    "### [__3D Human Pose Machines with Self-supervised Learning__](https://arxiv.org/pdf/1901.03798.pdf):\n",
    "----\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/3dposemachinessp.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Идея:\n",
    "1. Можем извлекать репрезентативную информацию как о 2D так и о 3D/\n",
    "2. Почему бы не добавить информацию о 2D пространстве в генерацию 3D поз чтобы решить еще задачу максимального соответствия позы в любом R (ракурс поворота.)\n",
    "3. Закодируем все признаки и вытащим их через LSTM\n",
    "4. Декодируем и получим точки для позы которые соответствуют ground truth Объекту.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/3dhumanposemachinesarch.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2:\n",
    "\n",
    "____\n",
    "\n",
    "# _Задача Facial Landmarks Detection._\n",
    "\n",
    "----\n",
    "\n",
    "На чем обучают:\n",
    "1. Своя разметка\n",
    "2. [WFLW, 98 landmarks](https://wywu.github.io/projects/LAB/WFLW.html)\n",
    "3. [DLIB, 68 and 5 landmarks](https://github.com/davisking/dlib-models)\n",
    "4. [3D Facial Landmarks](https://github.com/1adrianb/face-alignment)\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Применение:\n",
    "1. Перенос стиля и лица (маски, дипфейки, замена лица и тд).\n",
    "2. Трекинг эмоций, реакций.\n",
    "3. Поиск лиц по базе, по скрытым (закрытым) фотография - деаномизация.\n",
    "4. Трекинг внимания (Face ID и тд).\n",
    "5. Выравнивание лица по бенчмарку.\n",
    "6. Трекинг только лиц в видеопотоке.\n",
    "7. Медицина - диагнозы, поиск аномалий, поиск болезней по данным снимкам и тд.\n",
    "8. Биометрия.\n",
    "\n",
    "[Реализация Face Mash](https://github.com/Yinghao-Li/3DMM-fitting)\n",
    "[Реализация HypsterizerCats](https://github.com/kairess/cat_hipsterizer)\n",
    "\n",
    "Решение задачи - такая же как у и задач пройденных выше, только для определенной области, где мы по максимуму извлекаем признаки. Самый простой подход - регрессия на 2N значений (где N - количество лэндмарков).\n",
    "\n",
    "Что еще есть из уникального:\n",
    "\n",
    "----\n",
    "### [__PFLD: A Practical Facial Landmark Detector__](https://arxiv.org/pdf/1902.10859.pdf):\n",
    "----\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/pfld.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Идея:\n",
    "1. Вспомогательная сеть, которая использует общую архитектуру и прогнозирует повороты (углы рта, области лица и кривые лица и тд).\n",
    "2. В качестве CNN бэкенда - MobileNet блочная CNN\n",
    "3. L2 loss + геометрия пространства (данные из датасета) + аугментация картинок.\n",
    "\n",
    "\n",
    "[Реализация](https://github.com/guoqiangqi/PFLD)\n",
    "\n",
    "----\n",
    "### [__Attention-Driven Cropping for Very High Resolution Facial Landmark Detection__](http://studios.disneyresearch.com/2020/06/16/attention-driven-cropping-for-very-high-resolution-facial-landmark-detection/):\n",
    "----\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/attention_based_arch.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Идея:\n",
    "1. Берем очень высокое разрешение.\n",
    "2. Получаем хитмап при помощи hourglass архитектуру и используем в качестве бенчмарка.\n",
    "3. Нарезаем на кусочки при помощи механизма внимания и отдаем все в параллельный hourglass для обучения нескольких паттернов лица.\n",
    "4. Соединяем полученные лендмарки и сравниваем с бенчмарком.\n",
    "5. Получаем лендмарки.\n",
    "\n",
    "\n",
    "Интересные проекты:\n",
    "1. [Learnable Triangulation of Human Pose](https://github.com/karfly/learnable-triangulation-pytorch)\n",
    "2. [Face Alignment using a Deeply-initialized Coarse-to-fine Ensemble of Regression Trees](https://github.com/bobetocalo/bobetocalo_eccv18)\n",
    "3. [SPIGA: Shape Preserving Facial Landmarks with Graph Attention Networks.](https://github.com/andresprados/spiga)\n",
    "4. [Cascade of Encoder-Decoder CNNs with Learned Coordinates Regressor for Robust Facial Landmarks Detection](https://github.com/bobetocalo/bobetocalo_prl19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:46:15.214324500Z",
     "start_time": "2023-09-25T14:46:15.211798400Z"
    }
   },
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим в код который мы написали выше функционал для детекта Facial Landmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:48:03.480470400Z",
     "start_time": "2023-09-25T14:46:58.337092100Z"
    }
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Задетектим facial landmards\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        # Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks,mp_holistic.FACEMESH_CONTOURS)\n",
    "        # Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        # Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        # Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного стилизуем нашу маску:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T14:49:22.876924200Z",
     "start_time": "2023-09-25T14:48:20.916622800Z"
    }
   },
   "outputs": [],
   "source": [
    "mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=2)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "\n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
