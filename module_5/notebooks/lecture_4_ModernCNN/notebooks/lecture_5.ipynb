{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:45.429200Z",
     "start_time": "2023-08-02T17:47:45.405952Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:45.988939Z",
     "start_time": "2023-08-02T17:47:45.930249Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_dataset = tv.datasets.MNIST('.', train=True, transform=tv.transforms.ToTensor(), download=True)\n",
    "test_dataset = tv.datasets.MNIST('.', train=False, transform=tv.transforms.ToTensor(), download=True)\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:46.261230Z",
     "start_time": "2023-08-02T17:47:46.248006Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = torch.Tensor([0]), 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:46.963417Z",
     "start_time": "2023-08-02T17:47:46.944372Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, trainer, num_epochs):\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            print(\"Step. time since epoch: {:.3f}. Train acc: {:.3f}. Train Loss: {:.3f}\".format(time.time() -  start,\n",
    "                (y_hat.argmax(axis=1) == y).sum().item() / y.shape[0], l.item()))\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:48.110820Z",
     "start_time": "2023-08-02T17:47:48.089643Z"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(400, 120),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T16:20:28.809141Z",
     "start_time": "2019-11-18T16:20:23.333771Z"
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.9, 5\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "train(net, train_iter, test_iter, trainer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:52.437951Z",
     "start_time": "2023-08-02T17:47:52.377534Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "transoforms = tv.transforms.Compose([\n",
    "    tv.transforms.Resize((224,224)),\n",
    "    tv.transforms.ToTensor()\n",
    "])\n",
    "train_dataset = tv.datasets.MNIST('.', train=True, transform=transoforms, download=True)\n",
    "test_dataset = tv.datasets.MNIST('.', train=False, transform=transoforms, download=True)\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:53.110523Z",
     "start_time": "2023-08-02T17:47:52.849450Z"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 96, kernel_size=11, stride=4),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(6400, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T17:50:43.964040Z",
     "start_time": "2023-08-02T17:47:53.632198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step. time since epoch: 1.129. Train acc: 0.000. Train Loss: 73.699\n",
      "Step. time since epoch: 1.750. Train acc: 0.062. Train Loss: 73.771\n",
      "Step. time since epoch: 2.375. Train acc: 0.156. Train Loss: 73.507\n",
      "Step. time since epoch: 2.988. Train acc: 0.156. Train Loss: 73.394\n",
      "Step. time since epoch: 3.601. Train acc: 0.125. Train Loss: 73.793\n",
      "Step. time since epoch: 4.198. Train acc: 0.094. Train Loss: 74.203\n",
      "Step. time since epoch: 4.825. Train acc: 0.156. Train Loss: 73.325\n",
      "Step. time since epoch: 5.474. Train acc: 0.125. Train Loss: 73.438\n",
      "Step. time since epoch: 6.083. Train acc: 0.094. Train Loss: 74.052\n",
      "Step. time since epoch: 6.688. Train acc: 0.125. Train Loss: 73.665\n",
      "Step. time since epoch: 7.307. Train acc: 0.062. Train Loss: 73.750\n",
      "Step. time since epoch: 7.910. Train acc: 0.156. Train Loss: 73.224\n",
      "Step. time since epoch: 8.523. Train acc: 0.156. Train Loss: 73.590\n",
      "Step. time since epoch: 9.134. Train acc: 0.094. Train Loss: 73.764\n",
      "Step. time since epoch: 9.775. Train acc: 0.188. Train Loss: 73.308\n",
      "Step. time since epoch: 10.375. Train acc: 0.188. Train Loss: 73.636\n",
      "Step. time since epoch: 10.986. Train acc: 0.094. Train Loss: 74.232\n",
      "Step. time since epoch: 11.586. Train acc: 0.156. Train Loss: 73.495\n",
      "Step. time since epoch: 12.187. Train acc: 0.094. Train Loss: 73.808\n",
      "Step. time since epoch: 12.786. Train acc: 0.125. Train Loss: 73.759\n",
      "Step. time since epoch: 13.400. Train acc: 0.062. Train Loss: 73.884\n",
      "Step. time since epoch: 14.001. Train acc: 0.125. Train Loss: 73.440\n",
      "Step. time since epoch: 14.636. Train acc: 0.062. Train Loss: 74.054\n",
      "Step. time since epoch: 15.319. Train acc: 0.094. Train Loss: 74.449\n",
      "Step. time since epoch: 15.992. Train acc: 0.094. Train Loss: 73.922\n",
      "Step. time since epoch: 16.656. Train acc: 0.062. Train Loss: 73.504\n",
      "Step. time since epoch: 17.345. Train acc: 0.094. Train Loss: 74.267\n",
      "Step. time since epoch: 17.991. Train acc: 0.094. Train Loss: 73.664\n",
      "Step. time since epoch: 18.639. Train acc: 0.219. Train Loss: 73.535\n",
      "Step. time since epoch: 19.275. Train acc: 0.250. Train Loss: 73.188\n",
      "Step. time since epoch: 19.904. Train acc: 0.156. Train Loss: 73.348\n",
      "Step. time since epoch: 20.523. Train acc: 0.219. Train Loss: 72.850\n",
      "Step. time since epoch: 21.154. Train acc: 0.094. Train Loss: 73.532\n",
      "Step. time since epoch: 21.771. Train acc: 0.125. Train Loss: 73.133\n",
      "Step. time since epoch: 22.411. Train acc: 0.094. Train Loss: 73.843\n",
      "Step. time since epoch: 23.060. Train acc: 0.094. Train Loss: 73.954\n",
      "Step. time since epoch: 23.724. Train acc: 0.156. Train Loss: 72.781\n",
      "Step. time since epoch: 24.373. Train acc: 0.125. Train Loss: 73.127\n",
      "Step. time since epoch: 25.033. Train acc: 0.156. Train Loss: 73.297\n",
      "Step. time since epoch: 25.683. Train acc: 0.031. Train Loss: 73.790\n",
      "Step. time since epoch: 26.329. Train acc: 0.094. Train Loss: 74.105\n",
      "Step. time since epoch: 26.959. Train acc: 0.250. Train Loss: 73.382\n",
      "Step. time since epoch: 27.601. Train acc: 0.062. Train Loss: 74.703\n",
      "Step. time since epoch: 28.238. Train acc: 0.125. Train Loss: 73.730\n",
      "Step. time since epoch: 28.879. Train acc: 0.156. Train Loss: 73.362\n",
      "Step. time since epoch: 29.514. Train acc: 0.031. Train Loss: 73.811\n",
      "Step. time since epoch: 30.160. Train acc: 0.094. Train Loss: 73.626\n",
      "Step. time since epoch: 30.795. Train acc: 0.094. Train Loss: 73.484\n",
      "Step. time since epoch: 31.454. Train acc: 0.062. Train Loss: 73.479\n",
      "Step. time since epoch: 32.097. Train acc: 0.062. Train Loss: 73.748\n",
      "Step. time since epoch: 32.730. Train acc: 0.156. Train Loss: 72.924\n",
      "Step. time since epoch: 33.365. Train acc: 0.188. Train Loss: 72.364\n",
      "Step. time since epoch: 34.003. Train acc: 0.156. Train Loss: 72.484\n",
      "Step. time since epoch: 34.652. Train acc: 0.062. Train Loss: 72.411\n",
      "Step. time since epoch: 35.324. Train acc: 0.219. Train Loss: 74.258\n",
      "Step. time since epoch: 35.990. Train acc: 0.000. Train Loss: 74.079\n",
      "Step. time since epoch: 36.674. Train acc: 0.250. Train Loss: 72.987\n",
      "Step. time since epoch: 37.334. Train acc: 0.188. Train Loss: 71.928\n",
      "Step. time since epoch: 38.024. Train acc: 0.156. Train Loss: 72.389\n",
      "Step. time since epoch: 38.670. Train acc: 0.250. Train Loss: 69.072\n",
      "Step. time since epoch: 39.307. Train acc: 0.062. Train Loss: 89.245\n",
      "Step. time since epoch: 39.963. Train acc: 0.031. Train Loss: 73.971\n",
      "Step. time since epoch: 40.603. Train acc: 0.062. Train Loss: 75.134\n",
      "Step. time since epoch: 41.236. Train acc: 0.125. Train Loss: 74.036\n",
      "Step. time since epoch: 41.876. Train acc: 0.000. Train Loss: 74.656\n",
      "Step. time since epoch: 42.516. Train acc: 0.094. Train Loss: 73.395\n",
      "Step. time since epoch: 43.167. Train acc: 0.156. Train Loss: 73.913\n",
      "Step. time since epoch: 43.826. Train acc: 0.250. Train Loss: 74.859\n",
      "Step. time since epoch: 44.499. Train acc: 0.156. Train Loss: 73.094\n",
      "Step. time since epoch: 45.164. Train acc: 0.062. Train Loss: 73.492\n",
      "Step. time since epoch: 45.833. Train acc: 0.125. Train Loss: 74.086\n",
      "Step. time since epoch: 46.492. Train acc: 0.156. Train Loss: 74.275\n",
      "Step. time since epoch: 47.154. Train acc: 0.094. Train Loss: 73.801\n",
      "Step. time since epoch: 47.872. Train acc: 0.125. Train Loss: 73.063\n",
      "Step. time since epoch: 48.505. Train acc: 0.062. Train Loss: 74.474\n",
      "Step. time since epoch: 49.132. Train acc: 0.156. Train Loss: 72.877\n",
      "Step. time since epoch: 49.767. Train acc: 0.156. Train Loss: 72.722\n",
      "Step. time since epoch: 50.392. Train acc: 0.156. Train Loss: 74.449\n",
      "Step. time since epoch: 51.057. Train acc: 0.031. Train Loss: 74.813\n",
      "Step. time since epoch: 51.724. Train acc: 0.094. Train Loss: 73.825\n",
      "Step. time since epoch: 52.399. Train acc: 0.125. Train Loss: 73.384\n",
      "Step. time since epoch: 53.058. Train acc: 0.062. Train Loss: 74.000\n",
      "Step. time since epoch: 53.729. Train acc: 0.031. Train Loss: 74.170\n",
      "Step. time since epoch: 54.385. Train acc: 0.125. Train Loss: 74.403\n",
      "Step. time since epoch: 55.062. Train acc: 0.094. Train Loss: 73.563\n",
      "Step. time since epoch: 55.724. Train acc: 0.125. Train Loss: 73.390\n",
      "Step. time since epoch: 56.370. Train acc: 0.094. Train Loss: 73.650\n",
      "Step. time since epoch: 57.017. Train acc: 0.156. Train Loss: 73.691\n",
      "Step. time since epoch: 57.671. Train acc: 0.125. Train Loss: 73.315\n",
      "Step. time since epoch: 58.330. Train acc: 0.188. Train Loss: 72.965\n",
      "Step. time since epoch: 58.985. Train acc: 0.125. Train Loss: 73.065\n",
      "Step. time since epoch: 59.648. Train acc: 0.156. Train Loss: 73.848\n",
      "Step. time since epoch: 60.307. Train acc: 0.062. Train Loss: 74.764\n",
      "Step. time since epoch: 60.995. Train acc: 0.156. Train Loss: 73.616\n",
      "Step. time since epoch: 61.678. Train acc: 0.031. Train Loss: 74.704\n",
      "Step. time since epoch: 62.357. Train acc: 0.125. Train Loss: 73.646\n",
      "Step. time since epoch: 63.077. Train acc: 0.062. Train Loss: 74.014\n",
      "Step. time since epoch: 63.776. Train acc: 0.156. Train Loss: 73.396\n",
      "Step. time since epoch: 64.507. Train acc: 0.094. Train Loss: 73.973\n",
      "Step. time since epoch: 65.218. Train acc: 0.062. Train Loss: 74.204\n",
      "Step. time since epoch: 65.931. Train acc: 0.094. Train Loss: 73.691\n",
      "Step. time since epoch: 66.645. Train acc: 0.094. Train Loss: 73.486\n",
      "Step. time since epoch: 67.413. Train acc: 0.094. Train Loss: 74.094\n",
      "Step. time since epoch: 68.156. Train acc: 0.062. Train Loss: 73.975\n",
      "Step. time since epoch: 68.900. Train acc: 0.156. Train Loss: 73.225\n",
      "Step. time since epoch: 69.668. Train acc: 0.094. Train Loss: 74.018\n",
      "Step. time since epoch: 70.440. Train acc: 0.062. Train Loss: 73.740\n",
      "Step. time since epoch: 71.195. Train acc: 0.188. Train Loss: 73.641\n",
      "Step. time since epoch: 71.959. Train acc: 0.125. Train Loss: 72.719\n",
      "Step. time since epoch: 72.721. Train acc: 0.094. Train Loss: 74.036\n",
      "Step. time since epoch: 73.477. Train acc: 0.062. Train Loss: 74.013\n",
      "Step. time since epoch: 74.230. Train acc: 0.094. Train Loss: 74.604\n",
      "Step. time since epoch: 75.011. Train acc: 0.188. Train Loss: 73.382\n",
      "Step. time since epoch: 75.767. Train acc: 0.062. Train Loss: 74.093\n",
      "Step. time since epoch: 76.527. Train acc: 0.031. Train Loss: 74.207\n",
      "Step. time since epoch: 77.283. Train acc: 0.031. Train Loss: 73.957\n",
      "Step. time since epoch: 78.059. Train acc: 0.156. Train Loss: 73.200\n",
      "Step. time since epoch: 78.835. Train acc: 0.094. Train Loss: 74.080\n",
      "Step. time since epoch: 79.593. Train acc: 0.094. Train Loss: 73.726\n",
      "Step. time since epoch: 80.356. Train acc: 0.094. Train Loss: 73.947\n",
      "Step. time since epoch: 81.117. Train acc: 0.062. Train Loss: 74.005\n",
      "Step. time since epoch: 81.866. Train acc: 0.250. Train Loss: 73.469\n",
      "Step. time since epoch: 82.634. Train acc: 0.031. Train Loss: 73.836\n",
      "Step. time since epoch: 83.387. Train acc: 0.094. Train Loss: 73.179\n",
      "Step. time since epoch: 84.147. Train acc: 0.031. Train Loss: 73.984\n",
      "Step. time since epoch: 84.922. Train acc: 0.188. Train Loss: 73.918\n",
      "Step. time since epoch: 85.692. Train acc: 0.031. Train Loss: 73.849\n",
      "Step. time since epoch: 86.453. Train acc: 0.156. Train Loss: 73.213\n",
      "Step. time since epoch: 87.218. Train acc: 0.094. Train Loss: 74.059\n",
      "Step. time since epoch: 87.972. Train acc: 0.031. Train Loss: 73.352\n",
      "Step. time since epoch: 88.766. Train acc: 0.188. Train Loss: 73.813\n",
      "Step. time since epoch: 89.556. Train acc: 0.062. Train Loss: 73.932\n",
      "Step. time since epoch: 90.342. Train acc: 0.094. Train Loss: 73.527\n",
      "Step. time since epoch: 91.106. Train acc: 0.062. Train Loss: 73.792\n",
      "Step. time since epoch: 91.876. Train acc: 0.156. Train Loss: 74.036\n",
      "Step. time since epoch: 92.636. Train acc: 0.062. Train Loss: 73.251\n",
      "Step. time since epoch: 93.397. Train acc: 0.188. Train Loss: 73.514\n",
      "Step. time since epoch: 94.185. Train acc: 0.156. Train Loss: 73.572\n",
      "Step. time since epoch: 94.957. Train acc: 0.094. Train Loss: 73.818\n",
      "Step. time since epoch: 95.780. Train acc: 0.156. Train Loss: 73.596\n",
      "Step. time since epoch: 96.546. Train acc: 0.031. Train Loss: 73.752\n",
      "Step. time since epoch: 97.316. Train acc: 0.125. Train Loss: 73.852\n",
      "Step. time since epoch: 98.106. Train acc: 0.188. Train Loss: 72.970\n",
      "Step. time since epoch: 98.891. Train acc: 0.094. Train Loss: 74.265\n",
      "Step. time since epoch: 99.678. Train acc: 0.031. Train Loss: 74.244\n",
      "Step. time since epoch: 100.445. Train acc: 0.125. Train Loss: 74.245\n",
      "Step. time since epoch: 101.217. Train acc: 0.062. Train Loss: 73.919\n",
      "Step. time since epoch: 101.971. Train acc: 0.094. Train Loss: 73.945\n",
      "Step. time since epoch: 102.735. Train acc: 0.125. Train Loss: 73.736\n",
      "Step. time since epoch: 103.486. Train acc: 0.094. Train Loss: 73.130\n",
      "Step. time since epoch: 104.252. Train acc: 0.219. Train Loss: 73.572\n",
      "Step. time since epoch: 105.013. Train acc: 0.031. Train Loss: 74.026\n",
      "Step. time since epoch: 105.788. Train acc: 0.094. Train Loss: 73.345\n",
      "Step. time since epoch: 106.548. Train acc: 0.156. Train Loss: 73.257\n",
      "Step. time since epoch: 107.320. Train acc: 0.094. Train Loss: 73.362\n",
      "Step. time since epoch: 108.101. Train acc: 0.219. Train Loss: 73.217\n",
      "Step. time since epoch: 108.899. Train acc: 0.219. Train Loss: 72.558\n",
      "Step. time since epoch: 109.667. Train acc: 0.062. Train Loss: 73.957\n",
      "Step. time since epoch: 110.445. Train acc: 0.094. Train Loss: 74.322\n",
      "Step. time since epoch: 111.234. Train acc: 0.094. Train Loss: 73.830\n",
      "Step. time since epoch: 111.990. Train acc: 0.125. Train Loss: 73.156\n",
      "Step. time since epoch: 112.726. Train acc: 0.125. Train Loss: 73.907\n",
      "Step. time since epoch: 113.460. Train acc: 0.062. Train Loss: 73.915\n",
      "Step. time since epoch: 114.192. Train acc: 0.219. Train Loss: 73.493\n",
      "Step. time since epoch: 114.932. Train acc: 0.094. Train Loss: 73.541\n",
      "Step. time since epoch: 115.664. Train acc: 0.094. Train Loss: 73.539\n",
      "Step. time since epoch: 116.407. Train acc: 0.031. Train Loss: 73.275\n",
      "Step. time since epoch: 117.140. Train acc: 0.156. Train Loss: 73.126\n",
      "Step. time since epoch: 117.906. Train acc: 0.062. Train Loss: 74.737\n",
      "Step. time since epoch: 118.646. Train acc: 0.000. Train Loss: 74.315\n",
      "Step. time since epoch: 119.407. Train acc: 0.125. Train Loss: 73.500\n",
      "Step. time since epoch: 120.144. Train acc: 0.156. Train Loss: 73.046\n",
      "Step. time since epoch: 120.895. Train acc: 0.156. Train Loss: 73.384\n",
      "Step. time since epoch: 121.635. Train acc: 0.094. Train Loss: 73.582\n",
      "Step. time since epoch: 122.374. Train acc: 0.188. Train Loss: 73.345\n",
      "Step. time since epoch: 123.110. Train acc: 0.031. Train Loss: 74.035\n",
      "Step. time since epoch: 123.857. Train acc: 0.094. Train Loss: 72.789\n",
      "Step. time since epoch: 124.587. Train acc: 0.094. Train Loss: 73.843\n",
      "Step. time since epoch: 125.345. Train acc: 0.156. Train Loss: 73.839\n",
      "Step. time since epoch: 126.080. Train acc: 0.094. Train Loss: 73.707\n",
      "Step. time since epoch: 126.824. Train acc: 0.188. Train Loss: 73.552\n",
      "Step. time since epoch: 127.577. Train acc: 0.062. Train Loss: 73.459\n",
      "Step. time since epoch: 128.331. Train acc: 0.094. Train Loss: 73.371\n",
      "Step. time since epoch: 129.089. Train acc: 0.156. Train Loss: 73.729\n",
      "Step. time since epoch: 129.859. Train acc: 0.094. Train Loss: 74.072\n",
      "Step. time since epoch: 130.594. Train acc: 0.188. Train Loss: 73.082\n",
      "Step. time since epoch: 131.340. Train acc: 0.250. Train Loss: 72.767\n",
      "Step. time since epoch: 132.070. Train acc: 0.062. Train Loss: 73.795\n",
      "Step. time since epoch: 132.825. Train acc: 0.219. Train Loss: 73.754\n",
      "Step. time since epoch: 133.558. Train acc: 0.188. Train Loss: 73.125\n",
      "Step. time since epoch: 134.304. Train acc: 0.250. Train Loss: 72.509\n",
      "Step. time since epoch: 135.044. Train acc: 0.062. Train Loss: 72.911\n",
      "Step. time since epoch: 135.795. Train acc: 0.062. Train Loss: 73.547\n",
      "Step. time since epoch: 136.527. Train acc: 0.094. Train Loss: 74.288\n",
      "Step. time since epoch: 137.294. Train acc: 0.188. Train Loss: 72.908\n",
      "Step. time since epoch: 138.036. Train acc: 0.125. Train Loss: 73.611\n",
      "Step. time since epoch: 138.797. Train acc: 0.062. Train Loss: 73.484\n",
      "Step. time since epoch: 139.584. Train acc: 0.156. Train Loss: 72.070\n",
      "Step. time since epoch: 140.347. Train acc: 0.062. Train Loss: 76.267\n",
      "Step. time since epoch: 141.090. Train acc: 0.156. Train Loss: 73.251\n",
      "Step. time since epoch: 141.852. Train acc: 0.156. Train Loss: 73.617\n",
      "Step. time since epoch: 142.593. Train acc: 0.125. Train Loss: 73.334\n",
      "Step. time since epoch: 143.361. Train acc: 0.031. Train Loss: 74.546\n",
      "Step. time since epoch: 144.113. Train acc: 0.094. Train Loss: 73.873\n",
      "Step. time since epoch: 144.873. Train acc: 0.156. Train Loss: 73.276\n",
      "Step. time since epoch: 145.601. Train acc: 0.156. Train Loss: 72.325\n",
      "Step. time since epoch: 146.342. Train acc: 0.094. Train Loss: 73.938\n",
      "Step. time since epoch: 147.072. Train acc: 0.094. Train Loss: 74.291\n",
      "Step. time since epoch: 147.833. Train acc: 0.125. Train Loss: 73.277\n",
      "Step. time since epoch: 148.568. Train acc: 0.188. Train Loss: 72.459\n",
      "Step. time since epoch: 149.322. Train acc: 0.156. Train Loss: 72.766\n",
      "Step. time since epoch: 150.050. Train acc: 0.062. Train Loss: 73.871\n",
      "Step. time since epoch: 150.868. Train acc: 0.125. Train Loss: 73.775\n",
      "Step. time since epoch: 151.669. Train acc: 0.188. Train Loss: 73.005\n",
      "Step. time since epoch: 152.453. Train acc: 0.156. Train Loss: 72.280\n",
      "Step. time since epoch: 153.215. Train acc: 0.156. Train Loss: 71.899\n",
      "Step. time since epoch: 153.967. Train acc: 0.281. Train Loss: 70.275\n",
      "Step. time since epoch: 154.741. Train acc: 0.406. Train Loss: 67.716\n",
      "Step. time since epoch: 155.511. Train acc: 0.156. Train Loss: 79.860\n",
      "Step. time since epoch: 156.243. Train acc: 0.125. Train Loss: 74.424\n",
      "Step. time since epoch: 156.988. Train acc: 0.094. Train Loss: 74.530\n",
      "Step. time since epoch: 157.729. Train acc: 0.156. Train Loss: 73.115\n",
      "Step. time since epoch: 158.497. Train acc: 0.125. Train Loss: 73.832\n",
      "Step. time since epoch: 159.230. Train acc: 0.094. Train Loss: 74.762\n",
      "Step. time since epoch: 160.012. Train acc: 0.156. Train Loss: 73.324\n",
      "Step. time since epoch: 160.748. Train acc: 0.094. Train Loss: 73.166\n",
      "Step. time since epoch: 161.525. Train acc: 0.125. Train Loss: 74.290\n",
      "Step. time since epoch: 162.259. Train acc: 0.031. Train Loss: 74.722\n",
      "Step. time since epoch: 162.999. Train acc: 0.125. Train Loss: 74.190\n",
      "Step. time since epoch: 163.738. Train acc: 0.094. Train Loss: 74.228\n",
      "Step. time since epoch: 164.494. Train acc: 0.031. Train Loss: 74.570\n",
      "Step. time since epoch: 165.268. Train acc: 0.250. Train Loss: 73.391\n",
      "Step. time since epoch: 166.046. Train acc: 0.156. Train Loss: 73.118\n",
      "Step. time since epoch: 166.785. Train acc: 0.062. Train Loss: 74.741\n",
      "Step. time since epoch: 167.531. Train acc: 0.094. Train Loss: 73.893\n",
      "Step. time since epoch: 168.283. Train acc: 0.156. Train Loss: 73.126\n",
      "Step. time since epoch: 169.055. Train acc: 0.188. Train Loss: 73.418\n",
      "Step. time since epoch: 169.835. Train acc: 0.125. Train Loss: 73.467\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m lr, num_epochs  \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;241m5\u001B[39m\n\u001B[1;32m      2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 8\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(net, train_iter, test_iter, trainer, num_epochs)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m X, y \u001B[38;5;129;01min\u001B[39;00m train_iter:\n\u001B[1;32m      7\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m----> 8\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     l \u001B[38;5;241m=\u001B[39m loss(y_hat, y)\n\u001B[1;32m     10\u001B[0m     l\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "lr, num_epochs  = 0.01, 5\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "train(net, train_iter, test_iter, trainer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:15:50.021352Z",
     "start_time": "2023-08-02T18:15:50.004835Z"
    }
   },
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, input_channels, num_channels):\n",
    "\n",
    "    block = nn.Sequential(\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    for i in range(num_convs - 1):\n",
    "        block.add_module(\"conv{}\".format(i),\n",
    "                         nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
    "                         )\n",
    "        block.add_module(\"relu{}\".format(i),\n",
    "                         nn.ReLU()\n",
    "                         )\n",
    "\n",
    "    block.add_module(\"pool\", nn.MaxPool2d(2, stride=2))\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:15:50.267739Z",
     "start_time": "2023-08-02T18:15:50.250980Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_arch = ((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:15:50.694756Z",
     "start_time": "2023-08-02T18:15:50.483848Z"
    }
   },
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    net = nn.Sequential()\n",
    "\n",
    "    for i, (num_convs, input_ch, num_channels) in enumerate(conv_arch):\n",
    "        net.add_module(\"block{}\".format(i), vgg_block(num_convs, input_ch, num_channels))\n",
    "\n",
    "    \n",
    "    classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(6272, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 10))\n",
    "\n",
    "    net.add_module('classifier', classifier)\n",
    "    return net\n",
    "\n",
    "net = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:15:52.112970Z",
     "start_time": "2023-08-02T18:15:51.887026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ") \t\t torch.Size([1, 64, 112, 112])\n",
      "Sequential(\n",
      "  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ") \t\t torch.Size([1, 128, 56, 56])\n",
      "Sequential(\n",
      "  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (conv0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu0): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ") \t\t torch.Size([1, 256, 28, 28])\n",
      "Sequential(\n",
      "  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (conv0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu0): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ") \t\t torch.Size([1, 512, 14, 14])\n",
      "Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (conv0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu0): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ") \t\t torch.Size([1, 512, 7, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x25088 and 6272x4096)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m x \u001B[38;5;241m=\u001B[39m train_dataset[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m224\u001B[39m,\u001B[38;5;241m224\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m net:\n\u001B[0;32m----> 4\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43ml\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(l, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, x\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1x25088 and 6272x4096)"
     ]
    }
   ],
   "source": [
    "x = train_dataset[0][0].reshape(1,1,224,224)\n",
    "\n",
    "for l in net:\n",
    "    x = l(x)\n",
    "    print(l, \"\\t\\t\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:16:13.767837Z",
     "start_time": "2023-08-02T18:16:13.526534Z"
    }
   },
   "outputs": [],
   "source": [
    "ratio = 4\n",
    "small_conv_arch = [(v[0], max(v[1] // ratio, 1), v[2] // ratio) for v in conv_arch]\n",
    "net = vgg(small_conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:16:15.030068Z",
     "start_time": "2023-08-02T18:16:15.013835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(1, 1, 16), (1, 16, 32), (2, 32, 64), (2, 64, 128), (2, 128, 128)]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_conv_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:16:15.661815Z",
     "start_time": "2023-08-02T18:16:15.612801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ") \t\t torch.Size([1, 16, 112, 112])\n",
      "Sequential(\n",
      "  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ") \t\t torch.Size([1, 32, 56, 56])\n",
      "Sequential(\n",
      "  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu0): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ") \t\t torch.Size([1, 64, 28, 28])\n",
      "Sequential(\n",
      "  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (conv0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu0): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ") \t\t torch.Size([1, 128, 14, 14])\n",
      "Sequential(\n",
      "  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (conv0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu0): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ") \t\t torch.Size([1, 128, 7, 7])\n",
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=6272, out_features=4096, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Dropout(p=0.5, inplace=False)\n",
      "  (7): Linear(in_features=4096, out_features=10, bias=True)\n",
      ") \t\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "x = train_dataset[0][0].reshape(1,1,224,224)\n",
    "for l in net:\n",
    "    x = l(x)\n",
    "    print(l, \"\\t\\t\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:16:28.931447Z",
     "start_time": "2023-08-02T18:16:18.305676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step. time since epoch: 1.482. Train acc: 0.125. Train Loss: 73.779\n",
      "Step. time since epoch: 2.429. Train acc: 0.062. Train Loss: 74.387\n",
      "Step. time since epoch: 3.302. Train acc: 0.062. Train Loss: 73.436\n",
      "Step. time since epoch: 4.193. Train acc: 0.156. Train Loss: 72.676\n",
      "Step. time since epoch: 5.073. Train acc: 0.094. Train Loss: 75.069\n",
      "Step. time since epoch: 5.961. Train acc: 0.094. Train Loss: 75.430\n",
      "Step. time since epoch: 6.867. Train acc: 0.156. Train Loss: 73.494\n",
      "Step. time since epoch: 7.780. Train acc: 0.125. Train Loss: 73.690\n",
      "Step. time since epoch: 8.707. Train acc: 0.125. Train Loss: 75.226\n",
      "Step. time since epoch: 9.614. Train acc: 0.125. Train Loss: 74.022\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m lr, num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.05\u001B[39m, \u001B[38;5;241m5\u001B[39m\n\u001B[1;32m      2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 10\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(net, train_iter, test_iter, trainer, num_epochs)\u001B[0m\n\u001B[1;32m      8\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m net(X)\n\u001B[1;32m      9\u001B[0m l \u001B[38;5;241m=\u001B[39m loss(y_hat, y)\n\u001B[0;32m---> 10\u001B[0m \u001B[43ml\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m trainer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     12\u001B[0m train_l_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m l\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.05, 5\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "train(net, train_iter, test_iter, trainer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## NiN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:31:00.677381Z",
     "start_time": "2023-08-02T18:31:00.662065Z"
    }
   },
   "outputs": [],
   "source": [
    "def nin_block(input_channels, num_channels, kernel_size, strides, padding):\n",
    "    blk = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, num_channels, kernel_size, strides, padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_channels, num_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_channels, num_channels, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "    )\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:31:01.132760Z",
     "start_time": "2023-08-02T18:31:01.106319Z"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(nin_block(1, 96, kernel_size=11, strides=4, padding=0),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nin_block(96, 256, kernel_size=5, strides=1, padding=2),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nin_block(256, 384, kernel_size=3, strides=1, padding=1),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.Dropout(0.5),\n",
    "        nin_block(384, 10, kernel_size=3, strides=1, padding=1),\n",
    "        nn.AvgPool2d(5),\n",
    "        nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:31:01.813224Z",
     "start_time": "2023-08-02T18:31:01.780522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      ") torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) torch.Size([1, 96, 26, 26])\n",
      "Sequential(\n",
      "  (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      ") torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) torch.Size([1, 256, 12, 12])\n",
      "Sequential(\n",
      "  (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      ") torch.Size([1, 384, 12, 12])\n",
      "MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) torch.Size([1, 384, 5, 5])\n",
      "Dropout(p=0.5, inplace=False) torch.Size([1, 384, 5, 5])\n",
      "Sequential(\n",
      "  (0): Conv2d(384, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      ") torch.Size([1, 10, 5, 5])\n",
      "AvgPool2d(kernel_size=5, stride=5, padding=0) torch.Size([1, 10, 1, 1])\n",
      "Flatten(start_dim=1, end_dim=-1) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = train_dataset[0][0].reshape(1, 1, 224, 224)\n",
    "for l in net:\n",
    "    X = l(X)\n",
    "    print(l , X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:31:48.848599Z",
     "start_time": "2023-08-02T18:31:02.912725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step. time since epoch: 1.204. Train acc: 0.062. Train Loss: 73.798\n",
      "Step. time since epoch: 2.015. Train acc: 0.125. Train Loss: 73.577\n",
      "Step. time since epoch: 3.160. Train acc: 0.094. Train Loss: 73.352\n",
      "Step. time since epoch: 4.008. Train acc: 0.156. Train Loss: 73.401\n",
      "Step. time since epoch: 4.819. Train acc: 0.031. Train Loss: 74.732\n",
      "Step. time since epoch: 5.634. Train acc: 0.031. Train Loss: 74.059\n",
      "Step. time since epoch: 6.439. Train acc: 0.094. Train Loss: 73.702\n",
      "Step. time since epoch: 7.247. Train acc: 0.031. Train Loss: 73.769\n",
      "Step. time since epoch: 8.065. Train acc: 0.062. Train Loss: 73.683\n",
      "Step. time since epoch: 8.842. Train acc: 0.094. Train Loss: 73.683\n",
      "Step. time since epoch: 9.620. Train acc: 0.062. Train Loss: 73.683\n",
      "Step. time since epoch: 10.419. Train acc: 0.031. Train Loss: 73.683\n",
      "Step. time since epoch: 11.246. Train acc: 0.031. Train Loss: 73.683\n",
      "Step. time since epoch: 12.051. Train acc: 0.125. Train Loss: 73.683\n",
      "Step. time since epoch: 12.856. Train acc: 0.188. Train Loss: 73.683\n",
      "Step. time since epoch: 13.739. Train acc: 0.031. Train Loss: 73.683\n",
      "Step. time since epoch: 15.105. Train acc: 0.156. Train Loss: 73.683\n",
      "Step. time since epoch: 16.179. Train acc: 0.000. Train Loss: 73.683\n",
      "Step. time since epoch: 17.267. Train acc: 0.125. Train Loss: 73.683\n",
      "Step. time since epoch: 18.208. Train acc: 0.094. Train Loss: 73.683\n",
      "Step. time since epoch: 19.105. Train acc: 0.188. Train Loss: 73.683\n",
      "Step. time since epoch: 19.978. Train acc: 0.062. Train Loss: 73.683\n",
      "Step. time since epoch: 20.865. Train acc: 0.094. Train Loss: 73.683\n",
      "Step. time since epoch: 21.763. Train acc: 0.062. Train Loss: 73.683\n",
      "Step. time since epoch: 22.680. Train acc: 0.125. Train Loss: 73.683\n",
      "Step. time since epoch: 23.535. Train acc: 0.062. Train Loss: 73.683\n",
      "Step. time since epoch: 24.384. Train acc: 0.094. Train Loss: 73.683\n",
      "Step. time since epoch: 25.283. Train acc: 0.094. Train Loss: 73.683\n",
      "Step. time since epoch: 26.159. Train acc: 0.062. Train Loss: 73.683\n",
      "Step. time since epoch: 27.048. Train acc: 0.125. Train Loss: 73.683\n",
      "Step. time since epoch: 27.932. Train acc: 0.094. Train Loss: 73.683\n",
      "Step. time since epoch: 28.801. Train acc: 0.125. Train Loss: 73.683\n",
      "Step. time since epoch: 29.652. Train acc: 0.156. Train Loss: 73.683\n",
      "Step. time since epoch: 30.538. Train acc: 0.062. Train Loss: 73.683\n",
      "Step. time since epoch: 31.395. Train acc: 0.125. Train Loss: 73.683\n",
      "Step. time since epoch: 32.249. Train acc: 0.062. Train Loss: 73.683\n",
      "Step. time since epoch: 33.134. Train acc: 0.094. Train Loss: 73.683\n",
      "Step. time since epoch: 33.942. Train acc: 0.062. Train Loss: 73.683\n",
      "Step. time since epoch: 34.756. Train acc: 0.000. Train Loss: 73.683\n",
      "Step. time since epoch: 35.546. Train acc: 0.031. Train Loss: 73.683\n",
      "Step. time since epoch: 36.351. Train acc: 0.062. Train Loss: 73.683\n",
      "Step. time since epoch: 37.209. Train acc: 0.000. Train Loss: 73.683\n",
      "Step. time since epoch: 38.000. Train acc: 0.250. Train Loss: 73.683\n",
      "Step. time since epoch: 38.803. Train acc: 0.125. Train Loss: 73.683\n",
      "Step. time since epoch: 39.711. Train acc: 0.031. Train Loss: 73.683\n",
      "Step. time since epoch: 41.087. Train acc: 0.094. Train Loss: 73.683\n",
      "Step. time since epoch: 42.072. Train acc: 0.156. Train Loss: 73.683\n",
      "Step. time since epoch: 43.366. Train acc: 0.125. Train Loss: 73.683\n",
      "Step. time since epoch: 44.232. Train acc: 0.000. Train Loss: 73.683\n",
      "Step. time since epoch: 45.060. Train acc: 0.125. Train Loss: 73.683\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m lr, num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.05\u001B[39m, \u001B[38;5;241m5\u001B[39m\n\u001B[1;32m      2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 10\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(net, train_iter, test_iter, trainer, num_epochs)\u001B[0m\n\u001B[1;32m      8\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m net(X)\n\u001B[1;32m      9\u001B[0m l \u001B[38;5;241m=\u001B[39m loss(y_hat, y)\n\u001B[0;32m---> 10\u001B[0m \u001B[43ml\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m trainer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     12\u001B[0m train_l_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m l\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.05, 5\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "train(net, train_iter, test_iter, trainer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\r\n",
      "  Obtaining dependency information for torchinfo from https://files.pythonhosted.org/packages/72/25/973bd6128381951b23cdcd8a9870c6dcfc5606cb864df8eabd82e529f9c1/torchinfo-1.8.0-py3-none-any.whl.metadata\r\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\r\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\r\n",
      "Installing collected packages: torchinfo\r\n",
      "Successfully installed torchinfo-1.8.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Install torchinfo if it's not available, import it if it is\n",
    "try: \n",
    "    import torchinfo\n",
    "except:\n",
    "    !pip install torchinfo\n",
    "    import torchinfo"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T18:32:39.376659Z",
     "start_time": "2023-08-02T18:32:32.692004Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nSequential                               [1, 10]                   --\n├─Sequential: 1-1                        [1, 96, 54, 54]           --\n│    └─Conv2d: 2-1                       [1, 96, 54, 54]           11,712\n│    └─ReLU: 2-2                         [1, 96, 54, 54]           --\n│    └─Conv2d: 2-3                       [1, 96, 54, 54]           9,312\n│    └─ReLU: 2-4                         [1, 96, 54, 54]           --\n│    └─Conv2d: 2-5                       [1, 96, 54, 54]           9,312\n│    └─ReLU: 2-6                         [1, 96, 54, 54]           --\n├─MaxPool2d: 1-2                         [1, 96, 26, 26]           --\n├─Sequential: 1-3                        [1, 256, 26, 26]          --\n│    └─Conv2d: 2-7                       [1, 256, 26, 26]          614,656\n│    └─ReLU: 2-8                         [1, 256, 26, 26]          --\n│    └─Conv2d: 2-9                       [1, 256, 26, 26]          65,792\n│    └─ReLU: 2-10                        [1, 256, 26, 26]          --\n│    └─Conv2d: 2-11                      [1, 256, 26, 26]          65,792\n│    └─ReLU: 2-12                        [1, 256, 26, 26]          --\n├─MaxPool2d: 1-4                         [1, 256, 12, 12]          --\n├─Sequential: 1-5                        [1, 384, 12, 12]          --\n│    └─Conv2d: 2-13                      [1, 384, 12, 12]          885,120\n│    └─ReLU: 2-14                        [1, 384, 12, 12]          --\n│    └─Conv2d: 2-15                      [1, 384, 12, 12]          147,840\n│    └─ReLU: 2-16                        [1, 384, 12, 12]          --\n│    └─Conv2d: 2-17                      [1, 384, 12, 12]          147,840\n│    └─ReLU: 2-18                        [1, 384, 12, 12]          --\n├─MaxPool2d: 1-6                         [1, 384, 5, 5]            --\n├─Dropout: 1-7                           [1, 384, 5, 5]            --\n├─Sequential: 1-8                        [1, 10, 5, 5]             --\n│    └─Conv2d: 2-19                      [1, 10, 5, 5]             34,570\n│    └─ReLU: 2-20                        [1, 10, 5, 5]             --\n│    └─Conv2d: 2-21                      [1, 10, 5, 5]             110\n│    └─ReLU: 2-22                        [1, 10, 5, 5]             --\n│    └─Conv2d: 2-23                      [1, 10, 5, 5]             110\n│    └─ReLU: 2-24                        [1, 10, 5, 5]             --\n├─AvgPool2d: 1-9                         [1, 10, 1, 1]             --\n├─Flatten: 1-10                          [1, 10]                   --\n==========================================================================================\nTotal params: 1,992,166\nTrainable params: 1,992,166\nNon-trainable params: 0\nTotal mult-adds (M): 763.82\n==========================================================================================\nInput size (MB): 0.20\nForward/backward pass size (MB): 12.20\nParams size (MB): 7.97\nEstimated Total Size (MB): 20.37\n=========================================================================================="
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(net, input_size=[1, 1, 224, 224]) # "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T18:33:03.222597Z",
     "start_time": "2023-08-02T18:33:03.164461Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## GoogleLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T17:08:12.092643Z",
     "start_time": "2019-11-18T17:08:12.087776Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.module.Module"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T13:29:01.329069Z",
     "start_time": "2019-11-17T13:29:01.317911Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, ic, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        self.p1_1 = nn.Sequential(nn.Conv2d(ic, c1, kernel_size=1), nn.ReLU())\n",
    "        self.p2_1 = nn.Sequential(nn.Conv2d(ic, c2[0], kernel_size=1), nn.ReLU())\n",
    "        self.p2_2 = nn.Sequential(nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1), nn.ReLU())\n",
    "        self.p3_1 = nn.Sequential(nn.Conv2d(ic, c3[0], kernel_size=1), nn.ReLU())\n",
    "        self.p3_2 = nn.Sequential(nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2), nn.ReLU())\n",
    "        self.p4_1 = nn.Sequential(nn.MaxPool2d(3, stride=1, padding=1))\n",
    "        self.p4_2 = nn.Sequential(nn.Conv2d(ic, c4, kernel_size=1), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1_1(x)\n",
    "        p2 = self.p2_2(self.p2_1(x))\n",
    "        p3 = self.p3_2(self.p3_1(x))\n",
    "        p4 = self.p4_2(self.p4_1(x))\n",
    "        # Concatenate the outputs on the channel dimension.\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T13:29:01.480962Z",
     "start_time": "2019-11-17T13:29:01.476713Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(),\n",
    "       nn.MaxPool2d(3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T13:29:01.626101Z",
     "start_time": "2019-11-17T13:29:01.620836Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(\n",
    "       nn.Conv2d(64, 64, kernel_size=1),\n",
    "       nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "       nn.MaxPool2d(3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T13:30:56.752212Z",
     "start_time": "2019-11-17T13:30:56.741605Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b3 = nn.Sequential(\n",
    "       Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "       Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "       nn.MaxPool2d(3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T13:34:33.986895Z",
     "start_time": "2019-11-17T13:34:33.949995Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b4 = nn.Sequential(\n",
    "       Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "       Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "       Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "       Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "       Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "       nn.MaxPool2d(3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T13:34:49.151063Z",
     "start_time": "2019-11-17T13:34:49.120596Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b5 = nn.Sequential(\n",
    "       Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "       Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "       nn.AvgPool2d(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T13:35:35.796920Z",
     "start_time": "2019-11-17T13:35:35.792707Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, b2, b3, b4, b5, nn.Flatten(), nn.Linear(1024, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T17:11:01.447938Z",
     "start_time": "2019-11-18T17:11:01.372789Z"
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X = train_dataset[0][0].reshape(1, 1, 224, 224)\n",
    "for l in net:\n",
    "    X = l(X)\n",
    "    print(l , X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T17:21:23.003367Z",
     "start_time": "2019-11-18T17:11:07.846611Z"
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.05, 5\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "train(net, train_iter, test_iter, trainer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:39:32.914016Z",
     "start_time": "2023-08-02T18:39:32.835112Z"
    }
   },
   "outputs": [],
   "source": [
    "transoforms = tv.transforms.Compose([\n",
    "    tv.transforms.Grayscale(3),\n",
    "    tv.transforms.Resize((224,224)),\n",
    "    tv.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = tv.datasets.MNIST('.', train=True, transform=transoforms, download=True)\n",
    "test_dataset = tv.datasets.MNIST('.', train=False, transform=transoforms, download=True)\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:40:43.724433Z",
     "start_time": "2023-08-02T18:40:43.519219Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maratmovlamov/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/maratmovlamov/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = tv.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:41:05.900569Z",
     "start_time": "2023-08-02T18:41:05.874874Z"
    }
   },
   "outputs": [],
   "source": [
    "## Убираем требование градиента:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:41:15.806547Z",
     "start_time": "2023-08-02T18:41:15.788255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Linear(in_features=512, out_features=1000, bias=True)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:42:40.363890Z",
     "start_time": "2023-08-02T18:42:40.345153Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(in_features=512, out_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:42:55.584599Z",
     "start_time": "2023-08-02T18:42:55.568262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T17:25:11.558131Z",
     "start_time": "2019-11-18T17:25:11.554358Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-02T18:46:19.896417Z",
     "start_time": "2023-08-02T18:43:15.390450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step. time since epoch: 1.039. Train acc: 0.062. Train Loss: 76.296\n",
      "Step. time since epoch: 1.774. Train acc: 0.062. Train Loss: 79.770\n",
      "Step. time since epoch: 2.455. Train acc: 0.062. Train Loss: 77.880\n",
      "Step. time since epoch: 3.146. Train acc: 0.156. Train Loss: 73.103\n",
      "Step. time since epoch: 3.836. Train acc: 0.062. Train Loss: 81.671\n",
      "Step. time since epoch: 4.526. Train acc: 0.031. Train Loss: 80.685\n",
      "Step. time since epoch: 5.218. Train acc: 0.250. Train Loss: 72.240\n",
      "Step. time since epoch: 5.905. Train acc: 0.250. Train Loss: 75.459\n",
      "Step. time since epoch: 6.594. Train acc: 0.062. Train Loss: 83.678\n",
      "Step. time since epoch: 7.289. Train acc: 0.125. Train Loss: 78.632\n",
      "Step. time since epoch: 8.010. Train acc: 0.000. Train Loss: 78.080\n",
      "Step. time since epoch: 8.691. Train acc: 0.062. Train Loss: 78.287\n",
      "Step. time since epoch: 9.371. Train acc: 0.062. Train Loss: 82.328\n",
      "Step. time since epoch: 10.036. Train acc: 0.156. Train Loss: 77.613\n",
      "Step. time since epoch: 10.702. Train acc: 0.125. Train Loss: 75.179\n",
      "Step. time since epoch: 11.370. Train acc: 0.156. Train Loss: 78.387\n",
      "Step. time since epoch: 12.034. Train acc: 0.094. Train Loss: 82.953\n",
      "Step. time since epoch: 12.742. Train acc: 0.125. Train Loss: 76.291\n",
      "Step. time since epoch: 13.462. Train acc: 0.156. Train Loss: 78.614\n",
      "Step. time since epoch: 14.153. Train acc: 0.000. Train Loss: 80.149\n",
      "Step. time since epoch: 14.828. Train acc: 0.188. Train Loss: 72.972\n",
      "Step. time since epoch: 15.504. Train acc: 0.125. Train Loss: 80.653\n",
      "Step. time since epoch: 16.176. Train acc: 0.094. Train Loss: 80.831\n",
      "Step. time since epoch: 16.856. Train acc: 0.094. Train Loss: 83.887\n",
      "Step. time since epoch: 17.538. Train acc: 0.062. Train Loss: 83.248\n",
      "Step. time since epoch: 18.209. Train acc: 0.094. Train Loss: 83.491\n",
      "Step. time since epoch: 18.878. Train acc: 0.219. Train Loss: 76.341\n",
      "Step. time since epoch: 19.563. Train acc: 0.156. Train Loss: 76.141\n",
      "Step. time since epoch: 20.231. Train acc: 0.031. Train Loss: 85.188\n",
      "Step. time since epoch: 20.905. Train acc: 0.062. Train Loss: 80.822\n",
      "Step. time since epoch: 21.566. Train acc: 0.094. Train Loss: 82.167\n",
      "Step. time since epoch: 22.236. Train acc: 0.094. Train Loss: 76.027\n",
      "Step. time since epoch: 22.908. Train acc: 0.094. Train Loss: 77.187\n",
      "Step. time since epoch: 23.577. Train acc: 0.062. Train Loss: 83.153\n",
      "Step. time since epoch: 24.255. Train acc: 0.094. Train Loss: 81.674\n",
      "Step. time since epoch: 24.923. Train acc: 0.000. Train Loss: 78.466\n",
      "Step. time since epoch: 25.588. Train acc: 0.000. Train Loss: 81.831\n",
      "Step. time since epoch: 26.252. Train acc: 0.094. Train Loss: 79.960\n",
      "Step. time since epoch: 26.925. Train acc: 0.031. Train Loss: 79.901\n",
      "Step. time since epoch: 27.593. Train acc: 0.062. Train Loss: 85.692\n",
      "Step. time since epoch: 28.262. Train acc: 0.062. Train Loss: 82.835\n",
      "Step. time since epoch: 28.923. Train acc: 0.094. Train Loss: 77.792\n",
      "Step. time since epoch: 29.607. Train acc: 0.156. Train Loss: 74.055\n",
      "Step. time since epoch: 30.279. Train acc: 0.062. Train Loss: 80.672\n",
      "Step. time since epoch: 30.971. Train acc: 0.062. Train Loss: 82.530\n",
      "Step. time since epoch: 31.655. Train acc: 0.125. Train Loss: 82.073\n",
      "Step. time since epoch: 32.324. Train acc: 0.031. Train Loss: 78.780\n",
      "Step. time since epoch: 32.990. Train acc: 0.094. Train Loss: 83.123\n",
      "Step. time since epoch: 33.661. Train acc: 0.000. Train Loss: 84.386\n",
      "Step. time since epoch: 34.338. Train acc: 0.094. Train Loss: 78.282\n",
      "Step. time since epoch: 34.999. Train acc: 0.125. Train Loss: 73.369\n",
      "Step. time since epoch: 35.665. Train acc: 0.062. Train Loss: 78.939\n",
      "Step. time since epoch: 36.328. Train acc: 0.062. Train Loss: 77.317\n",
      "Step. time since epoch: 37.056. Train acc: 0.156. Train Loss: 75.775\n",
      "Step. time since epoch: 37.903. Train acc: 0.062. Train Loss: 83.260\n",
      "Step. time since epoch: 39.053. Train acc: 0.094. Train Loss: 87.214\n",
      "Step. time since epoch: 39.793. Train acc: 0.094. Train Loss: 81.826\n",
      "Step. time since epoch: 40.476. Train acc: 0.062. Train Loss: 80.034\n",
      "Step. time since epoch: 41.154. Train acc: 0.031. Train Loss: 76.606\n",
      "Step. time since epoch: 41.857. Train acc: 0.156. Train Loss: 80.769\n",
      "Step. time since epoch: 42.549. Train acc: 0.031. Train Loss: 80.778\n",
      "Step. time since epoch: 43.249. Train acc: 0.125. Train Loss: 80.341\n",
      "Step. time since epoch: 43.937. Train acc: 0.125. Train Loss: 82.619\n",
      "Step. time since epoch: 44.617. Train acc: 0.062. Train Loss: 80.395\n",
      "Step. time since epoch: 45.280. Train acc: 0.062. Train Loss: 81.282\n",
      "Step. time since epoch: 45.938. Train acc: 0.188. Train Loss: 76.264\n",
      "Step. time since epoch: 46.605. Train acc: 0.062. Train Loss: 83.344\n",
      "Step. time since epoch: 47.261. Train acc: 0.188. Train Loss: 80.407\n",
      "Step. time since epoch: 47.933. Train acc: 0.156. Train Loss: 77.610\n",
      "Step. time since epoch: 48.595. Train acc: 0.031. Train Loss: 83.087\n",
      "Step. time since epoch: 49.249. Train acc: 0.094. Train Loss: 81.227\n",
      "Step. time since epoch: 49.908. Train acc: 0.062. Train Loss: 83.330\n",
      "Step. time since epoch: 50.578. Train acc: 0.062. Train Loss: 79.855\n",
      "Step. time since epoch: 51.236. Train acc: 0.062. Train Loss: 77.919\n",
      "Step. time since epoch: 51.893. Train acc: 0.062. Train Loss: 83.676\n",
      "Step. time since epoch: 52.550. Train acc: 0.031. Train Loss: 77.501\n",
      "Step. time since epoch: 53.221. Train acc: 0.062. Train Loss: 80.814\n",
      "Step. time since epoch: 53.899. Train acc: 0.156. Train Loss: 79.955\n",
      "Step. time since epoch: 54.576. Train acc: 0.031. Train Loss: 78.143\n",
      "Step. time since epoch: 55.239. Train acc: 0.188. Train Loss: 74.015\n",
      "Step. time since epoch: 55.899. Train acc: 0.188. Train Loss: 76.224\n",
      "Step. time since epoch: 56.559. Train acc: 0.125. Train Loss: 80.782\n",
      "Step. time since epoch: 57.220. Train acc: 0.094. Train Loss: 80.155\n",
      "Step. time since epoch: 57.881. Train acc: 0.062. Train Loss: 81.913\n",
      "Step. time since epoch: 58.543. Train acc: 0.062. Train Loss: 80.687\n",
      "Step. time since epoch: 59.202. Train acc: 0.219. Train Loss: 74.193\n",
      "Step. time since epoch: 59.876. Train acc: 0.094. Train Loss: 80.742\n",
      "Step. time since epoch: 60.545. Train acc: 0.125. Train Loss: 78.423\n",
      "Step. time since epoch: 61.205. Train acc: 0.125. Train Loss: 72.978\n",
      "Step. time since epoch: 61.869. Train acc: 0.031. Train Loss: 76.030\n",
      "Step. time since epoch: 62.525. Train acc: 0.125. Train Loss: 78.784\n",
      "Step. time since epoch: 63.203. Train acc: 0.062. Train Loss: 80.660\n",
      "Step. time since epoch: 63.868. Train acc: 0.125. Train Loss: 85.723\n",
      "Step. time since epoch: 64.538. Train acc: 0.125. Train Loss: 79.831\n",
      "Step. time since epoch: 65.201. Train acc: 0.188. Train Loss: 79.527\n",
      "Step. time since epoch: 65.877. Train acc: 0.094. Train Loss: 80.624\n",
      "Step. time since epoch: 66.549. Train acc: 0.031. Train Loss: 86.086\n",
      "Step. time since epoch: 67.219. Train acc: 0.125. Train Loss: 81.851\n",
      "Step. time since epoch: 67.890. Train acc: 0.062. Train Loss: 81.035\n",
      "Step. time since epoch: 68.615. Train acc: 0.031. Train Loss: 80.983\n",
      "Step. time since epoch: 69.918. Train acc: 0.062. Train Loss: 78.645\n",
      "Step. time since epoch: 70.713. Train acc: 0.188. Train Loss: 76.143\n",
      "Step. time since epoch: 71.425. Train acc: 0.031. Train Loss: 80.795\n",
      "Step. time since epoch: 72.129. Train acc: 0.062. Train Loss: 80.252\n",
      "Step. time since epoch: 72.834. Train acc: 0.062. Train Loss: 78.250\n",
      "Step. time since epoch: 73.575. Train acc: 0.125. Train Loss: 79.917\n",
      "Step. time since epoch: 74.368. Train acc: 0.094. Train Loss: 75.861\n",
      "Step. time since epoch: 75.093. Train acc: 0.062. Train Loss: 82.023\n",
      "Step. time since epoch: 75.776. Train acc: 0.094. Train Loss: 76.267\n",
      "Step. time since epoch: 76.494. Train acc: 0.062. Train Loss: 79.530\n",
      "Step. time since epoch: 77.189. Train acc: 0.031. Train Loss: 79.549\n",
      "Step. time since epoch: 77.862. Train acc: 0.125. Train Loss: 81.215\n",
      "Step. time since epoch: 78.523. Train acc: 0.125. Train Loss: 73.431\n",
      "Step. time since epoch: 79.185. Train acc: 0.031. Train Loss: 79.903\n",
      "Step. time since epoch: 79.849. Train acc: 0.156. Train Loss: 75.940\n",
      "Step. time since epoch: 80.521. Train acc: 0.125. Train Loss: 80.213\n",
      "Step. time since epoch: 81.176. Train acc: 0.094. Train Loss: 76.591\n",
      "Step. time since epoch: 81.833. Train acc: 0.031. Train Loss: 84.349\n",
      "Step. time since epoch: 82.486. Train acc: 0.094. Train Loss: 78.819\n",
      "Step. time since epoch: 83.141. Train acc: 0.156. Train Loss: 80.180\n",
      "Step. time since epoch: 83.798. Train acc: 0.031. Train Loss: 85.046\n",
      "Step. time since epoch: 84.462. Train acc: 0.094. Train Loss: 75.630\n",
      "Step. time since epoch: 85.210. Train acc: 0.031. Train Loss: 83.094\n",
      "Step. time since epoch: 85.877. Train acc: 0.125. Train Loss: 75.559\n",
      "Step. time since epoch: 86.540. Train acc: 0.062. Train Loss: 86.096\n",
      "Step. time since epoch: 87.216. Train acc: 0.156. Train Loss: 77.210\n",
      "Step. time since epoch: 87.887. Train acc: 0.031. Train Loss: 84.807\n",
      "Step. time since epoch: 88.563. Train acc: 0.062. Train Loss: 83.842\n",
      "Step. time since epoch: 89.229. Train acc: 0.094. Train Loss: 80.939\n",
      "Step. time since epoch: 89.911. Train acc: 0.156. Train Loss: 79.931\n",
      "Step. time since epoch: 90.618. Train acc: 0.094. Train Loss: 80.092\n",
      "Step. time since epoch: 91.332. Train acc: 0.125. Train Loss: 78.529\n",
      "Step. time since epoch: 92.044. Train acc: 0.094. Train Loss: 75.115\n",
      "Step. time since epoch: 92.738. Train acc: 0.062. Train Loss: 77.940\n",
      "Step. time since epoch: 93.453. Train acc: 0.156. Train Loss: 82.159\n",
      "Step. time since epoch: 94.174. Train acc: 0.156. Train Loss: 72.883\n",
      "Step. time since epoch: 94.886. Train acc: 0.125. Train Loss: 80.963\n",
      "Step. time since epoch: 95.621. Train acc: 0.031. Train Loss: 81.207\n",
      "Step. time since epoch: 96.348. Train acc: 0.062. Train Loss: 82.937\n",
      "Step. time since epoch: 97.089. Train acc: 0.031. Train Loss: 80.470\n",
      "Step. time since epoch: 97.852. Train acc: 0.125. Train Loss: 76.159\n",
      "Step. time since epoch: 98.590. Train acc: 0.062. Train Loss: 82.044\n",
      "Step. time since epoch: 99.323. Train acc: 0.125. Train Loss: 75.875\n",
      "Step. time since epoch: 100.058. Train acc: 0.094. Train Loss: 78.964\n",
      "Step. time since epoch: 100.802. Train acc: 0.062. Train Loss: 84.773\n",
      "Step. time since epoch: 101.565. Train acc: 0.062. Train Loss: 82.434\n",
      "Step. time since epoch: 102.353. Train acc: 0.031. Train Loss: 79.454\n",
      "Step. time since epoch: 103.114. Train acc: 0.062. Train Loss: 78.500\n",
      "Step. time since epoch: 103.869. Train acc: 0.094. Train Loss: 77.059\n",
      "Step. time since epoch: 104.662. Train acc: 0.094. Train Loss: 81.034\n",
      "Step. time since epoch: 105.437. Train acc: 0.031. Train Loss: 81.945\n",
      "Step. time since epoch: 106.189. Train acc: 0.125. Train Loss: 82.398\n",
      "Step. time since epoch: 106.952. Train acc: 0.156. Train Loss: 73.712\n",
      "Step. time since epoch: 107.735. Train acc: 0.094. Train Loss: 80.688\n",
      "Step. time since epoch: 108.494. Train acc: 0.062. Train Loss: 78.681\n",
      "Step. time since epoch: 109.245. Train acc: 0.094. Train Loss: 78.357\n",
      "Step. time since epoch: 110.004. Train acc: 0.250. Train Loss: 72.505\n",
      "Step. time since epoch: 110.773. Train acc: 0.062. Train Loss: 83.348\n",
      "Step. time since epoch: 111.560. Train acc: 0.156. Train Loss: 79.470\n",
      "Step. time since epoch: 112.348. Train acc: 0.062. Train Loss: 79.150\n",
      "Step. time since epoch: 113.139. Train acc: 0.156. Train Loss: 77.716\n",
      "Step. time since epoch: 113.949. Train acc: 0.094. Train Loss: 78.778\n",
      "Step. time since epoch: 114.729. Train acc: 0.062. Train Loss: 78.576\n",
      "Step. time since epoch: 115.521. Train acc: 0.000. Train Loss: 80.079\n",
      "Step. time since epoch: 116.295. Train acc: 0.125. Train Loss: 77.294\n",
      "Step. time since epoch: 117.084. Train acc: 0.031. Train Loss: 80.906\n",
      "Step. time since epoch: 117.867. Train acc: 0.188. Train Loss: 73.850\n",
      "Step. time since epoch: 118.650. Train acc: 0.031. Train Loss: 80.616\n",
      "Step. time since epoch: 119.436. Train acc: 0.125. Train Loss: 79.515\n",
      "Step. time since epoch: 120.225. Train acc: 0.125. Train Loss: 79.035\n",
      "Step. time since epoch: 121.021. Train acc: 0.156. Train Loss: 75.980\n",
      "Step. time since epoch: 121.803. Train acc: 0.156. Train Loss: 75.060\n",
      "Step. time since epoch: 122.593. Train acc: 0.156. Train Loss: 78.646\n",
      "Step. time since epoch: 123.389. Train acc: 0.188. Train Loss: 78.545\n",
      "Step. time since epoch: 124.186. Train acc: 0.094. Train Loss: 81.049\n",
      "Step. time since epoch: 124.967. Train acc: 0.062. Train Loss: 79.197\n",
      "Step. time since epoch: 125.750. Train acc: 0.125. Train Loss: 75.412\n",
      "Step. time since epoch: 126.535. Train acc: 0.125. Train Loss: 84.561\n",
      "Step. time since epoch: 127.321. Train acc: 0.125. Train Loss: 78.097\n",
      "Step. time since epoch: 128.117. Train acc: 0.094. Train Loss: 78.585\n",
      "Step. time since epoch: 128.915. Train acc: 0.125. Train Loss: 79.785\n",
      "Step. time since epoch: 129.706. Train acc: 0.062. Train Loss: 78.714\n",
      "Step. time since epoch: 130.476. Train acc: 0.031. Train Loss: 81.822\n",
      "Step. time since epoch: 131.253. Train acc: 0.094. Train Loss: 81.896\n",
      "Step. time since epoch: 132.025. Train acc: 0.031. Train Loss: 83.435\n",
      "Step. time since epoch: 132.801. Train acc: 0.125. Train Loss: 78.904\n",
      "Step. time since epoch: 133.592. Train acc: 0.125. Train Loss: 75.060\n",
      "Step. time since epoch: 134.356. Train acc: 0.000. Train Loss: 80.901\n",
      "Step. time since epoch: 135.123. Train acc: 0.125. Train Loss: 83.088\n",
      "Step. time since epoch: 135.918. Train acc: 0.188. Train Loss: 73.436\n",
      "Step. time since epoch: 136.696. Train acc: 0.094. Train Loss: 74.640\n",
      "Step. time since epoch: 137.491. Train acc: 0.125. Train Loss: 74.734\n",
      "Step. time since epoch: 138.309. Train acc: 0.031. Train Loss: 82.297\n",
      "Step. time since epoch: 139.069. Train acc: 0.031. Train Loss: 83.478\n",
      "Step. time since epoch: 139.887. Train acc: 0.031. Train Loss: 79.375\n",
      "Step. time since epoch: 140.648. Train acc: 0.094. Train Loss: 81.432\n",
      "Step. time since epoch: 141.428. Train acc: 0.062. Train Loss: 76.642\n",
      "Step. time since epoch: 142.168. Train acc: 0.156. Train Loss: 76.463\n",
      "Step. time since epoch: 142.903. Train acc: 0.125. Train Loss: 76.870\n",
      "Step. time since epoch: 143.645. Train acc: 0.062. Train Loss: 83.212\n",
      "Step. time since epoch: 144.392. Train acc: 0.062. Train Loss: 77.507\n",
      "Step. time since epoch: 145.125. Train acc: 0.125. Train Loss: 80.669\n",
      "Step. time since epoch: 145.874. Train acc: 0.062. Train Loss: 85.500\n",
      "Step. time since epoch: 146.611. Train acc: 0.062. Train Loss: 83.319\n",
      "Step. time since epoch: 147.352. Train acc: 0.062. Train Loss: 81.105\n",
      "Step. time since epoch: 148.090. Train acc: 0.000. Train Loss: 81.132\n",
      "Step. time since epoch: 148.811. Train acc: 0.188. Train Loss: 73.839\n",
      "Step. time since epoch: 149.563. Train acc: 0.031. Train Loss: 79.485\n",
      "Step. time since epoch: 150.296. Train acc: 0.031. Train Loss: 82.504\n",
      "Step. time since epoch: 151.027. Train acc: 0.094. Train Loss: 77.764\n",
      "Step. time since epoch: 151.756. Train acc: 0.062. Train Loss: 84.106\n",
      "Step. time since epoch: 152.494. Train acc: 0.094. Train Loss: 77.242\n",
      "Step. time since epoch: 153.237. Train acc: 0.031. Train Loss: 88.760\n",
      "Step. time since epoch: 153.964. Train acc: 0.188. Train Loss: 82.064\n",
      "Step. time since epoch: 154.709. Train acc: 0.094. Train Loss: 81.870\n",
      "Step. time since epoch: 155.442. Train acc: 0.156. Train Loss: 77.676\n",
      "Step. time since epoch: 156.189. Train acc: 0.094. Train Loss: 78.317\n",
      "Step. time since epoch: 156.919. Train acc: 0.125. Train Loss: 75.527\n",
      "Step. time since epoch: 157.651. Train acc: 0.094. Train Loss: 79.567\n",
      "Step. time since epoch: 158.389. Train acc: 0.062. Train Loss: 80.570\n",
      "Step. time since epoch: 159.126. Train acc: 0.094. Train Loss: 80.324\n",
      "Step. time since epoch: 159.852. Train acc: 0.031. Train Loss: 81.055\n",
      "Step. time since epoch: 160.604. Train acc: 0.156. Train Loss: 78.600\n",
      "Step. time since epoch: 161.338. Train acc: 0.062. Train Loss: 82.941\n",
      "Step. time since epoch: 162.077. Train acc: 0.062. Train Loss: 82.453\n",
      "Step. time since epoch: 162.811. Train acc: 0.031. Train Loss: 80.188\n",
      "Step. time since epoch: 163.552. Train acc: 0.094. Train Loss: 77.584\n",
      "Step. time since epoch: 164.307. Train acc: 0.188. Train Loss: 81.982\n",
      "Step. time since epoch: 165.053. Train acc: 0.031. Train Loss: 82.825\n",
      "Step. time since epoch: 165.795. Train acc: 0.062. Train Loss: 80.310\n",
      "Step. time since epoch: 166.538. Train acc: 0.062. Train Loss: 79.364\n",
      "Step. time since epoch: 167.276. Train acc: 0.062. Train Loss: 75.637\n",
      "Step. time since epoch: 168.020. Train acc: 0.031. Train Loss: 81.795\n",
      "Step. time since epoch: 168.757. Train acc: 0.094. Train Loss: 83.141\n",
      "Step. time since epoch: 169.492. Train acc: 0.094. Train Loss: 79.633\n",
      "Step. time since epoch: 170.236. Train acc: 0.062. Train Loss: 81.213\n",
      "Step. time since epoch: 170.969. Train acc: 0.062. Train Loss: 80.964\n",
      "Step. time since epoch: 171.711. Train acc: 0.062. Train Loss: 77.697\n",
      "Step. time since epoch: 172.438. Train acc: 0.094. Train Loss: 76.635\n",
      "Step. time since epoch: 173.181. Train acc: 0.094. Train Loss: 72.837\n",
      "Step. time since epoch: 173.920. Train acc: 0.094. Train Loss: 76.748\n",
      "Step. time since epoch: 174.662. Train acc: 0.000. Train Loss: 83.012\n",
      "Step. time since epoch: 175.412. Train acc: 0.094. Train Loss: 77.456\n",
      "Step. time since epoch: 176.225. Train acc: 0.062. Train Loss: 77.702\n",
      "Step. time since epoch: 177.458. Train acc: 0.094. Train Loss: 81.215\n",
      "Step. time since epoch: 178.239. Train acc: 0.156. Train Loss: 78.620\n",
      "Step. time since epoch: 179.048. Train acc: 0.094. Train Loss: 80.526\n",
      "Step. time since epoch: 179.814. Train acc: 0.156. Train Loss: 76.422\n",
      "Step. time since epoch: 180.582. Train acc: 0.062. Train Loss: 78.600\n",
      "Step. time since epoch: 181.339. Train acc: 0.125. Train Loss: 76.543\n",
      "Step. time since epoch: 182.142. Train acc: 0.156. Train Loss: 77.810\n",
      "Step. time since epoch: 182.945. Train acc: 0.125. Train Loss: 76.925\n",
      "Step. time since epoch: 183.730. Train acc: 0.094. Train Loss: 74.111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 8\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(net, train_iter, test_iter, trainer, num_epochs)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m X, y \u001B[38;5;129;01min\u001B[39;00m train_iter:\n\u001B[1;32m      7\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m----> 8\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     l \u001B[38;5;241m=\u001B[39m loss(y_hat, y)\n\u001B[1;32m     10\u001B[0m     l\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001B[0m, in \u001B[0;36mResNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torchvision/models/resnet.py:273\u001B[0m, in \u001B[0;36mResNet._forward_impl\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    270\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[1;32m    271\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaxpool(x)\n\u001B[0;32m--> 273\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    274\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x)\n\u001B[1;32m    275\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(x)\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torchvision/models/resnet.py:103\u001B[0m, in \u001B[0;36mBasicBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    100\u001B[0m     identity \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownsample(x)\n\u001B[1;32m    102\u001B[0m out \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m identity\n\u001B[0;32m--> 103\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/modules/activation.py:103\u001B[0m, in \u001B[0;36mReLU.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Projects/DS_Foundations/venv/lib/python3.10/site-packages/torch/nn/functional.py:1455\u001B[0m, in \u001B[0;36mrelu\u001B[0;34m(input, inplace)\u001B[0m\n\u001B[1;32m   1453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(relu, (\u001B[38;5;28minput\u001B[39m,), \u001B[38;5;28minput\u001B[39m, inplace\u001B[38;5;241m=\u001B[39minplace)\n\u001B[1;32m   1454\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[0;32m-> 1455\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1456\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1457\u001B[0m     result \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28minput\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_iter, test_iter, trainer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
