{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Использование Pre-Trained Трансформера\n",
    "----\n",
    "\n",
    "В подавляющем большинстве случаев трансформер обучен на невообразимо большом количестве неразмеченных данных и так как у обычного разработчика нет возможности и ресурсов тренировать такие мегабольшие модели, у нас остается только один выбор - брать веса трансформера (Open-source) и дообучивать уже на нашей конкретной задаче. Поэтому рассмотрим работу и принцип работы нескольких разнообразных моделей трансформера и попробуем что-нибудь применить из них на практике. Для того, чтобы проиллюстрировать работу трансформера мною были выбраны несколько популярных архитектур:\n",
    "\n",
    "\n",
    "1. General Pre-Trained Transformer - GPT (**[GPT-1](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)** , **[GPT-2](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)**, **[GPT-3](https://arxiv.org/pdf/2005.14165.pdf)** - доступна только платная версия)\n",
    "2. Bidirectional Encoder Representation Transformer - **BERT**\n",
    "3. Bidirectional Auto-Regressive Transformer - **BART**\n",
    "\n",
    "\n",
    "\n",
    "Важный момент, так как трансформеры способны самостоятельно строить скрытые связи в структуре данных, то мы можем использовать подход **self-supervised learning (unsupervised pre-training)**, то есть мы используем большое количество немаркированного (неразмеченного) контента (данных) для того чтобы научить модель делать правильные прогнозы.\n",
    "\n",
    "\n",
    "В задаче генерации текста либо машинного перевода, подход self-supervised learning может быть представлен следующим образом:\n",
    "1. Предположим что у нас имеется последовательность из n слов (элементов), этап pre-training может быть разбит на несколько частей:\n",
    "2. **Первая часть:** На шаге t мы обучаем трансформер на истинной последовательности (реальный y1, ..., yi)\n",
    "3. **Вторая часть:** Просим модель сделать предсказание на шаге t и сравниваем его с истинным значением yi.\n",
    "4. **Третья часть:** Обновляем модель и шаг, т.е. i := i+1 и возвращаемся на шаг 1 и повторяем до тех пор пока не обработаем всю последовательность.\n",
    "\n",
    "Вышеуказанный подход очень похож на Teacher Forcing для рекуррентных нейронных сетей.\n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "Основная идея - использовать \"сырой\" текст (без разметок) на этапе Pre-training, а затем будем дообучивать модельку на выполнение конкретной задачи для которой будет отложен качественно размеченный датасет меньшего размера. Это один из видов дотренировки модели.\n",
    "Наример задача машинного перевода или генерации текста в данном контексте может быть представлена как однонаправленный подход (где формируем только представляения) на этапе pre-training.\n",
    "\n",
    "\n",
    "### Итого полный этап тренировки Open-Sourced трансформера состоит из 2-х частей:\n",
    "1. Pre-Training на большом, неразмеченном датасете (варианты feature-based approach (ELMo, ELM, Word2Vec, Glove etc) и fine-tuning approach)\n",
    "2. Fine-Tuning трансформера для решения специальной задачи используя маленький размеченный датасет (свою выборку)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import transformers # pip install transformers чтобы установить библиотеку для работы с моделями-трансформерами\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Немного про GPT-подобные модели:\n",
    "Тренировка GPT-1 состоит из 2 частей:\n",
    "1. Pre-Training на огромном количестве неразмеченных данных (текст)\n",
    "2. Fine-tuning (Supervised-fine-tuning).\n",
    "\n",
    "Пока рассмотрим Decoder, так как Encoder плюс-минус везде и во всех архитектурах одинаков, в исследовательской работе и в большинстве источников архитектура выглядит следующим образом:\n",
    "\n",
    "![Image](./../../src/img/gpt_one.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPT подобные модели, очень хорошо показывают себя на задачах Zero-Shot (Zero-Shot Classification, Zero-Shot Captioning, Zero-Shot Generation etc). Что означает подход Zero-Shot - это такой подход который позволяет модели использоваться везде. Суть заключается в следующем - на этапе претренировки или тренировки мы обучаем модель на части данных, а на этапе инференса и тестирования модели подаем в модель невиданные ранее данные и смотрим насколько хорошо GPT справилась с задачей. Если метрики по Zero-Shot задаче показывают себя выше чем на обычной тренировке, то модель научилась контекстному распознованию данных и сформировала определенные зависимости которые позволяют самостоятельно \"думать\" (искать взаимосвязи) модели.\n",
    "\n",
    "----\n",
    "##### Реализации следующих версий GPT не сильно отличаются от своего предшественника, за исключением, что у модели нет нужды в дотренировке, так как добавлен механизм формирования связей, плюс сохранен контекст с ее предшественника.\n",
    "----\n",
    "GPT-3 добавлен механизм [Few-Shot Learning](https://arxiv.org/abs/1904.10509) для изучения контекста и понимания структуры данных. Few-Shot Learning - подход позаимствованный у человека, мы показываем пару раз нейронной сети как необходимо посутпать, а затем она пытается повторить то что увидела (то есть подаем на этапе тренировки несколько раз правильные (истинные) данные, а затем выкидываем их, чтобы на этапе инференса модель уже сама искал контекстуальную зависимость)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Посмотрим на задачу генерации текста с использованием модели GPT-2:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aea7a593508f4fb5b00566a56bbd91fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/523M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef9fa5a9f94c4a2188af553281cadf02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04b75aa0a823467d85da61f3f0c446c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fe35ecdeb454ec0912587856621e207"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "daf4a135ba5c45a4929ec9dcbb9d5a07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline # https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'generated_text': 'Humanity today is a ills. There has been plenty of innovation, but the world we live in is still so bad, so full of unthinking, ignorant people. I still think that there is something wrong with us. The United Kingdom doesn'},\n {'generated_text': 'Humanity today is a vernacular term – a word which some interpreters attribute to a different species, the white kyllo in the Bible. To avoid confusion, a word known as quakkakuk is pronounced quakk.'},\n {'generated_text': 'Humanity today is a \\ue000uice state, a state that has two \\ue000um \\ue000a of natural \\ue000um \\ue000um \\ue000um in relation to Nature and to the \\ue002ou'},\n {'generated_text': 'Humanity today is a vernacular of language used for everyday thought, action, perception, and action. It is a language that allows a person to think and act. It is called a person, since the person is an object of the mind'},\n {'generated_text': \"Humanity today is a vernacular for what it calls social, ecological, political, and economic order based on human humanists as the dominant social unit – its leader and its leaders.\\n\\nAn 'enlightened state' has to deal\"}]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Humanity today is a \", max_length=50, num_return_sequences=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[1212,  318, 1194, 2239, 3371, 2081, 9552]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Как было сказано выше мы также можем использовать модели трансформеров для генерации новых признаков для подачи в другие модели:\n",
    "import transformers\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"This is another step towards true AI\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = transformers.GPT2Model.from_pretrained(\"gpt2\") # https://huggingface.co/gpt2  # https://huggingface.co/docs/transformers/model_doc/gpt2\n",
    "output = model(*encoded_input)\n",
    "print(output[\"last_hidden_state\"].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BERT - Bidirectional Encoder Representation Transformer (Google, 2018, [научная статья](https://arxiv.org/abs/1810.04805))\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "~ 345 млн параметров. BERT тоже трансформер, отличие - двунаправленный механизм для расчета эмбеддингов, обработки входной последовательности, обработки скрытых состояний. Энкодинг зависит как от предшественников последовательности так и от будущих элементов. Двунаправленность не позволяет BERT заниматься задачей генерации текста (так как генерации происходит пословесно), вывод модели сразу весь батч обработанной последовательности. Подходит для задач классификации, формирования эмбеддингов данных. Главное отличие BERT Другой подход к fine-tuning модели, который состоит также из 2 этапов, но другой подход:\n",
    "\n",
    "1. Pre-Training:\n",
    "    1.1 Masked Language Model (MLM) токены последовательности случайным образом отфильтровываются при помощи так называемой маски и модель учится предсказывать убранные слова через данную маску (маска просто разряженная матрица с различными распределениями).\n",
    "    1.2 Предсказание следующего предложения, но с форматом [CLS]A[SEP]B[SEP] где А и В - некие предложения, CLS токен классификации который просто указывает на начало предложения, SEP  токен конца предложения. На данном эатпе модель учится предсказывать является ли предложение В действительно продолжением предложения А.\n",
    "2. Fine-Tuning\n",
    "\n",
    "Модель BERT очень хорошо подходи для классификации последовательностей, диалоговых систем, теггирование контекста, расшифровки контекста. Широко применяется в биоинформатике (кодирование-декодирование химических соединений, ДНК, формирование связей между различными реакциями в экспериментах).\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "### Посмотрим как можно применять BERT для классификационной задачки:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import requests\n",
    "import torchtext\n",
    "import transformers\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "torch.backends.cudnn.deterministic = True\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              review  sentiment\n0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n1  OK... so... I really like Kris Kristofferson a...          0\n2  ***SPOILER*** Do not read this, if you think a...          0\n3  hi for all the people who have seen this wonde...          1\n4  I recently bought the DVD, forgetting just how...          0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>OK... so... I really like Kris Kristofferson a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>***SPOILER*** Do not read this, if you think a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>hi for all the people who have seen this wonde...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I recently bought the DVD, forgetting just how...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/movie_data.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 50000\n"
     ]
    }
   ],
   "source": [
    "print(\"Observations: {}\".format(len(df)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "train_texts = df.iloc[:35000]['review'].values\n",
    "train_labels = df.iloc[:35000]['sentiment'].values\n",
    "\n",
    "valid_texts = df.iloc[35000:40000]['review'].values\n",
    "valid_labels = df.iloc[35000:40000]['sentiment'].values\n",
    "\n",
    "test_texts = df.iloc[40000:]['review'].values\n",
    "test_labels = df.iloc[40000:]['sentiment'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') # https://huggingface.co/docs/tokenizers/python/latest/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "valid_dataset = IMDbDataset(valid_encodings, valid_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = transformers.DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "        ### Prepare data\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "            num_examples += labels.size(0)\n",
    "            correct_pred += (predicted_labels == labels).sum()\n",
    "\n",
    "        return correct_pred.float()/num_examples * 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(20):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "\n",
    "        ### Prepare data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        ### Forward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "\n",
    "        ### Backward\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        ### Logging\n",
    "        if not batch_idx % 250:\n",
    "            print (f'Epoch: {epoch+1:04d}/{20:04d} | '\n",
    "                   f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'Training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nValid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "\n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "\n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fine-Tuning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = transformers.DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(DEVICE)\n",
    "model.train();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# install dataset via pip install datasets\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred # logits are a numpy array, not pytorch tensor\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(\n",
    "               predictions=predictions, references=labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    compute_metrics=compute_metrics,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(optim, None) # optimizer and learning rate scheduler\n",
    ")\n",
    "\n",
    "# force model to only use 1 GPU (even if multiple are availabe)\n",
    "# to compare more fairly to previous code\n",
    "\n",
    "trainer.args._n_gpu = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "trainer.train()\n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}