{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## ARCH\n",
    "----\n",
    "\n",
    "AutoRegressive Conditional Heteroscedasticit - пытаемся объяснить дисперсию в ряде через предыдущие значения (применяя к ним AR)\n",
    "\n",
    "\n",
    "Пусть временной ряд представляется в таком виде:\n",
    "\n",
    "$\\epsilon_t = z_t * \\sqrt{\\omega + \\sum_{k=1}^p \\alpha_i \\epsilon_{k-i}^2}$ при условии, что $\\omega > 0, 1 \\geq \\alpha_i \\geq 0$\n",
    "\n",
    "где на этот раз:\n",
    "\n",
    "$\\epsilon_t$ - модель временного ряда, который мы пытаемся смоделировать\n",
    "$z_t$ - некая константа представляющая непрогнозируемый шум и по условию $E[z_t] = 0, E[z^2_t] = 1$\n",
    "$\\omega$ - случайная ошибка (bias)\n",
    "$\\alpha_i$ - коэффициенты авторегрессии\n",
    "\n",
    "\n",
    "Тогда условная дисперсия ряда будет равна\n",
    "\n",
    "$\\sigma_t^2 = E[\\epsilon^2_t | \\epsilon_{t-1}, ..., \\epsilon_{t-q}) = \\sigma^2_t = \\omega + \\sum_{k=1}^p \\alpha_k \\epsilon_{t-k}^2$\n",
    "\n",
    "Получили модель ARCH(q) условной дисперсии. Требуем, чтобы все коэффициенты были больше 0 (иначе может получится отрицательная дисперсия)\n",
    "\n",
    "\n",
    "Как помним из курса статистики:\n",
    "1. Гетероскедастичность - дисперсия изменяется с течением времени.\n",
    "2. Гомоскедастичность - дисперсия постоянна (выражается в виде шума, случаного блуждания).\n",
    "\n",
    "\n",
    "Почему Conditional: вводят предположение, что дисперсия взаимозависима от ближайших значений ряда.\n",
    "\n",
    "Тогда, имеет смысл попробовать построить модель, которая зависит от своих предыдущих значений во временном ряду.\n",
    "\n",
    "Недостатки модели ARCH/GARCH:\n",
    "* Не подходит для долгосрочного прогноза (как и впрочем почти все модели для временных рядов) :)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hvplot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import pacf, plot_pacf, acf, plot_acf\n",
    "from statsmodels.tsa.stattools import adfuller"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(20, 7), style='bmh'):\n",
    "    test_stationarity(y)\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    with plt.style.context(style):\n",
    "        plt.figure(figsize=figsize)\n",
    "        layout = (4, 1)\n",
    "        ts_ax = plt.subplot2grid(layout, (0, 0), rowspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (2, 0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (3, 0))\n",
    "\n",
    "        y.plot(ax=ts_ax, color='blue', label='Or')\n",
    "        ts_ax.set_title('Original')\n",
    "        plot_acf(y, lags=lags, ax=acf_ax, alpha=0.05)\n",
    "        plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def test_stationarity(timeseries):\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "    for [key, value] in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' % key] = value\n",
    "    print(dfoutput)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Имитация ARCH(1) для временного ряда:_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# уравнение вида: Var(yt) = a_0 + a_1*y{t-1}**2\n",
    "# если a_1 между 0 and 1 тогда yt случайное блуждание\n",
    "np.random.seed(13)\n",
    "\n",
    "\n",
    "a0 = 2\n",
    "a1 = .5\n",
    "\n",
    "y = w = np.random.normal(size=100)\n",
    "Y = np.empty_like(y)\n",
    "\n",
    "for t in range(len(y)):\n",
    "    Y[t] = w[t] * np.sqrt((a0 + a1*y[t-1]**2))\n",
    "\n",
    "# simulated ARCH(1) series, looks like white noise\n",
    "tsplot(Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GARCH\n",
    "-----\n",
    "\n",
    "Как было сказано выше GARCH - дисперсия (волатильность) полностью зависит от случайного шума:\n",
    "\n",
    "$\\sigma_t^2 = E[\\epsilon^2_t | \\epsilon_{t-1}, ..., \\epsilon_{t-q}) = \\sigma^2_t = \\omega + \\sum_{k=1}^p \\alpha_k \\epsilon_{t-k}^2$\n",
    "\n",
    "Мы хотим сделать дисперсию более или менее постоянной, для улучшения прогнозной способности модели. Для этого и необходим немного улучшенный подход, который получил название GARCH. Как выглядит модель GARCH:\n",
    "\n",
    "$\\sigma_t^2 =E[\\epsilon^2_t | \\epsilon_{t-1}, ..., \\epsilon_{t-q}) = \\omega + \\sum_{k=1}^p \\alpha_k \\epsilon_{t-k}^2 + \\sum_{k=1}^q \\beta_k \\sigma_{t-k}^2$\n",
    "\n",
    "\n",
    "Таким образом мы просто добавляем зависимость от прошлых значений для условной дисперсиии. Получаем модель GARCH(p,q), то есть это по сутии таже самая ARMA примененная к дисперсии временного ряда.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# pip install arch\n",
    "from arch import arch_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как было показано на предыдущих занятиях, моделирование и прогнозирование временного ряда состоит обычно из 3 шагов:\n",
    "1. Создаем экземпляр модели - передаем данные и гиперпараметры.\n",
    "2. Обучаем модель\n",
    "3. Прогнозируем\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "```\n",
    "1 from arch import arch_model\n",
    "2\n",
    "3 model = arch_model(\n",
    "4               y=training_data,\n",
    "5               x=None,          # внешние данные (exogenous data)\n",
    "6               mean='Constant', # среднее значение модели, e.g. 'Zero', 'ARX'\n",
    "7               vol='GARCH',     # тип модели (ARCH, GARCH, EGARCH и т.д.)\n",
    "8               p=1,             # параметр p\n",
    "9               q=1,             # игнорируется если модель типа ARCH\n",
    "10              dist='Normal'    # тип распределения (StudentsT, Exp, Beta)\n",
    "11 )\n",
    "12 result = model.fit()\n",
    "13 result.ssummary()\n",
    "```\n",
    "\n",
    "Прогнозирование при помощи GARCH реализовано иначе, нам необходимо спрогнозировать статистику $\\epsilon_t$ (среднее и дисперсию).\n",
    "Нас волнует теперь только дисперсия (или стандартное отклонение - так называемая волатильность). Таким образом мы хотим спрогнозировать параметр $\\sigma_t$\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "```\n",
    "1 forecast_result = result.forecast(\n",
    "2                           horizon=h,\n",
    "3                           reindex=False,\n",
    "4                           start='2022-10-10')\n",
    "5 result.conditional_volatility # обученные предиктор, то что нам необходимо\n",
    "6 np.ssqrt(forecast_result.variance) # условная дисперсия нашего прогноза\n",
    "```\n",
    "\n",
    "__Аргумент reindex:__ Реиндексирует прогноз таким образом, чтобы размерности прогноза совпадали с размерностью временного ряда.\n",
    "__Зачем:__ При помощи модели GARCH мы можешь реализовывать прогноз с любой точки временного ряда (например если данные с Jan 2021 - Jan 2022, мы можем выбрать точку в качестве прогноза Mar 2021 и тогда с нее начнется прогноз). По умолчанию прогноз начинается с последней точки данных временного ряда. Кроме того, мы можем также реализовывать прогноз не только начиная с какой-либо точки данных временного ряда, но и организовать прогноз для каждой точки временного ряда одновременно, например, если горизонт временного ряда 2021-2022, то у нас есть возможность организовать прогноз начиная с Jan-21, Feb-21, ..., Dec-22.\n",
    "<br>\n",
    "</br>\n",
    "__reindex=True__: выводит датафрейм такой же размерности как и тренировочный временной ряд.\n",
    "__reindex=False__: выводит датафрейм, который содержит только тот горизонт, для которого мы бы хотели сделать прогноз."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Предположим, что горизонт временного ряда March-1 - March-10:\n",
    "```\n",
    "forecast(horizon=5, reindex=True, start='March 6')\n",
    "```\n",
    "![](./../src/imgs/garch_fcast.png)\n",
    "\n",
    "\n",
    "Расшифровка столбцов: Каждый столбец - это прогноз на N+1 горизонт, например в нашем случуае h.1 = March-7, h.2 = March-8, ..., h.5=March-10."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arch import arch_model\n",
    "import hvplot\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "from math import sqrt\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/SPY.csv', parse_dates=True, index_col='Date')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "df['LogReturns'] = np.log(df['Close']).diff()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "#df2 = df.loc['2010-01-05':'2015-01-01'].copy()\n",
    "df2 = df.iloc[1:].copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hvplot.plot(df2['LogReturns'], kind='line', width=800, height=400, label='Logarithmic Returns')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "df2['Ysq'] = df['LogReturns'] ** 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hvplot.plot(df2['Ysq'], kind='line', width=800, height=400, label='Scaled Logarithmic Returns')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Ислледуем PACF/ACF:_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_acf(df2['LogReturns'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_pacf(df2['LogReturns'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_acf(df2['Ysq'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_pacf(df2['Ysq'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noise_sq = np.random.randn(500)**2\n",
    "plot_acf(noise_sq)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_pacf(noise_sq)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Train/Test Split_:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "Ntest = 500\n",
    "train = df2.iloc[:-Ntest][['LogReturns']].copy()\n",
    "test = df2.iloc[-Ntest:][['LogReturns']].copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Зачем производить шкалирование_:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "model = arch_model(train['LogReturns'], vol='GARCH', p=1, q=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = model.fit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_ARCH(1)_:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "m = train['LogReturns'].mean()\n",
    "s = test['LogReturns'].std()\n",
    "\n",
    "train['Scaled'] = (train['LogReturns'] - m) / s\n",
    "test['Scaled'] = (test['LogReturns'] - m) / s\n",
    "df2['Scaled'] = (df2['LogReturns'] - m) / s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "train['Scaled'] = train.Scaled.astype(np.float)\n",
    "test['Scaled'] = train.Scaled.astype(np.float)\n",
    "df2['Scaled'] = train.Scaled.astype(np.float)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "arch_one = arch_model(train['Scaled'], vol='GARCH', p=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_arch_one = arch_one.fit(update_freq=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_arch_one.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2['ARCH_1'] = res_arch_one.conditional_volatility\n",
    "hvplot.plot(df2[['Scaled', 'ARCH_1']], kind='line', width=800, height=400, label='Scaled Log Returns and Conditional Volatility')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "<arch.univariate.base.ARCHModelForecast at 0x169c60baf20>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_arch_one.forecast(horizon=Ntest)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "forecast_arch_one = res_arch_one.forecast(horizon=Ntest, reindex=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "<arch.univariate.base.ARCHModelForecast at 0x169c60bb9d0>"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_arch_one"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Разница между reindex=False и reindex=True_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forecast_arch_one.mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forecast_arch_one.variance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forecast_arch_one.residual_variance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "forecast_arch_one = res_arch_one.forecast(horizon=Ntest, reindex=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forecast_arch_one.mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forecast_arch_one.variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forecast_arch_one.residual_variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Прогнозирование"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2.loc['2011-08-09':]['ARCH_1'].plot(figsize=(15,5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "fcast_arch1 = res_arch_one.forecast(horizon=Ntest, reindex=True, start='2011-08-01')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fcast_arch1.variance['2011-07-28':'2011-08-05']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fcast_arch1.variance.loc['2011-08-09'].to_numpy().shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2.index.get_loc('2011-08-09')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "Timestamp('2013-08-06 00:00:00')"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.index[402+500]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "df2.loc['2011-08-10':'2013-08-06', 'ARCH_1 Forecast'] = np.sqrt(\n",
    "    fcast_arch1.variance.loc['2011-08-09'].to_numpy()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2.loc['2011-08-01':'2012-02-01'][['ARCH_1','ARCH_1 Forecast']].plot(figsize=(15,5))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выберем другую дату"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fcast_arch1.variance.index.get_loc('2011-08-17')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2.index[408+500]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "df2.loc['2011-08-18':'2013-08-14', 'ARCH_1 Forecast Low'] = np.sqrt(\n",
    "    fcast_arch1.variance.loc['2011-08-17'].to_numpy()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_cols = ['ARCH_1 Forecast', 'ARCH_1 Forecast Low']\n",
    "df2.loc['2011-08-01':'2012-02-01'][plot_cols].plot(figsize=(15,5))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GARCH(1, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "garch11 = arch_model(train['Scaled'], vol='GARCH', p=1, q=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_garch11 = garch11.fit(update_freq=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_garch11.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2['GARCH(1,1)'] = res_garch11.conditional_volatility\n",
    "df2[['Scaled', 'ARCH_1', 'GARCH(1,1)']].plot(figsize=(15,5))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "fcast_garch11 = res_garch11.forecast(\n",
    "    horizon=Ntest, reindex=False, start='2011-08-09'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "df2.loc['2011-08-10':'2013-08-06', 'GARCH(1,1) Forecast'] = np.sqrt(\n",
    "    fcast_garch11.variance.loc['2011-08-09'].to_numpy()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "df2['AbsScaled'] = df2['Scaled'].abs()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_cols = ['AbsScaled', 'ARCH_1 Forecast', 'GARCH(1,1) Forecast']\n",
    "df2.loc['2011-08-01':'2012-02-01'][plot_cols].plot(figsize=(15,5))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Практика: создайте модель GARCH (p, q) с параметрами на выбор. Распределение выбрать StudentsT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "garchpq = arch_model(train['Scaled'], p=8, q=5, dist='StudentsT')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:     10,   Func. Count:    185,   Neg. LLF: 2678.256488785693\n",
      "Iteration:     20,   Func. Count:    368,   Neg. LLF: 2525.536748176307\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 2525.2882244412085\n",
      "            Iterations: 29\n",
      "            Function evaluations: 520\n",
      "            Gradient evaluations: 29\n"
     ]
    }
   ],
   "source": [
    "res_garchpq = garchpq.fit(update_freq=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_garchpq.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_arch_one.aic, res_garchpq.aic, res_garch11.aic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2['GARCH(p,q)']=res_garchpq.conditional_volatility\n",
    "df2[['Scaled', 'GARCH(p,q)']].plot(figsize=(15,5))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "fcast_garchpq = res_garchpq.forecast(horizon=Ntest, reindex=False, start='2011-08-09')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "df2.loc['2011-08-10':'2013-08-06', 'GARCH(p,q) Forecast'] = np.sqrt(fcast_garchpq.variance.loc['2011-08-09'].to_numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_cols = ['AbsScaled', 'GARCH(p,q) Forecast']\n",
    "df2.loc['2011-08-01':'2012-02-01'][plot_cols].plot(figsize=(15,5))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как еще можно использовать GARCH в связке с ARIMA:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series = pd.read_csv('./../data/air_passengers.csv')['passengers']\n",
    "series = boxcox(series, 0)\n",
    "series = series[12:] - series[:-12]\n",
    "series = series[1:] - series[:-1]\n",
    "tsplot(series)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "mdl = ARIMA(series, order=(4,0,4)).fit()\n",
    "tsplot(mdl.resid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "with plt.style.context('bmh'):\n",
    "    plt.figure(figsize=(14,8))\n",
    "    # ax = plt.axes()\n",
    "    plot_predict(mdl)\n",
    "    plt.plot(series, color='red', label='Series')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _get_best_model(TS):\n",
    "    best_aic = np.inf\n",
    "    best_order = None\n",
    "    best_mdl = None\n",
    "\n",
    "    for i in range(5):\n",
    "        for d in range(5):\n",
    "            for j in range(5):\n",
    "                try:\n",
    "                    tmp_mdl = ARIMA(TS, order=(i,d,j)).fit(method='statespace')\n",
    "                    tmp_aic = tmp_mdl.aic\n",
    "                    if tmp_aic < best_aic:\n",
    "                        best_aic = tmp_aic\n",
    "                        best_order = (i, d, j)\n",
    "                        best_mdl = tmp_mdl\n",
    "                except: continue\n",
    "    print('aic: {:6.5f} | order: {}'.format(best_aic, best_order))\n",
    "    return best_aic, best_order, best_mdl\n",
    "\n",
    "aic, order, mdl = _get_best_model(series)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tsplot(mdl.resid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Применим GARCH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we can fit the arch model using the best fit arima model parameters\n",
    "p_ = order[0]\n",
    "o_ = order[1]\n",
    "q_ = order[2]\n",
    "\n",
    "# Using student T distribution usually provides better fit\n",
    "am = arch_model(series, p=p_, o=o_, q=q_, dist='StudentsT')\n",
    "res = am.fit(update_freq=5, disp='off')\n",
    "print(res.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tsplot(res.resid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Полезные материалы:\n",
    "\n",
    "1. [Статистическое обоснование модели GARCH](https://online.stat.psu.edu/stat510/lesson/11/11.1)\n",
    "2. [Use Case ARCH/GARCH](https://medium.com/@ranjithkumar.rocking/time-series-model-s-arch-and-garch-2781a982b448)\n",
    "3. [Vector Autoregressive Model](https://kevinkotze.github.io/ts-7-var/)\n",
    "4. [VARIMA](https://analyticsindiamag.com/a-guide-to-varma-with-auto-arima-in-time-series-modelling/)\n",
    "5. [VARIMA/VARMA](https://faculty.washington.edu/ezivot/econ584/notes/varModels.pdf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
