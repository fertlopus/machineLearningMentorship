{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План\n",
    "1. Введение \n",
    "  * Основные определения\n",
    "  * Автокорреляция, Стационарность/Нестационарность, Тесть Дьюка-Фуллера\n",
    "  * Метрики качества прогнозов для временных рядов\n",
    "2. Сглаживание временного ряда\n",
    "  * Rolling window estimations\n",
    "  * Экспоненциальное сглаживание, простое, двойное и Holt-Winters\n",
    "  * Кросс-валидация на временных рядах\n",
    "3. Эконометрический подход \n",
    "  * Стационарность и единичные корни - еще раз на всякий случай\n",
    "  * Избавление от нестационарности, дифференцирование, сезонное дифференцирование, выделение трендов\n",
    "  * ARIMA - выбор начальных параметров модели по ACF, PACF, выбор параметров перебором, построение SARIMAX с подобранными параметрами\n",
    "  * Ограничения, недостатки эконометрического подхода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Введение\n",
    "\n",
    "При работе с временными рядами чаще всего возникает вопрос — а что у нас будет происходить с нашими показателями в ближайший день/неделю/месяц/пр. — сколько будет онлайна, как много действий совершат пользователи, и так далее. К задаче прогнозирования можно подходить по-разному, в зависимости от того, какого качества должен быть проноз, на какой период мы хотим его строить, и, конечно, как долго нужно подбирать и настраивать параметры модели для его получения.\n",
    "\n",
    "Небольшое [определение](https://ru.wikipedia.org/wiki/Временной_ряд) временного ряда:\n",
    "> Временной ряд – это последовательность значений $y_{1}$, $y_{2}$, ..., $y_{t}$, где $y_{t} \\in R$, то есть значения признаков, которые описывают во времени некий случайный процесс, который измеряется в последовательные постоянные моменты времени (через равные промежутки, постоянные временные интервалы). Очень важно, чтобы временные интервалы между признаками (элементами измерений) были постоянными, иначе задача не сходится к решению временного ряда.\n",
    "\n",
    "Таким образом, данные оказываются упорядочены относительно неслучайных моментов времени, и, значит, в отличие от случайных выборок, могут содержать в себе дополнительную информацию, которую мы постараемся извлечь. \n",
    "----\n",
    "В целом, задачи, связанные с временными рядами можно разбить на несколько групп\n",
    "- _**Анализ**_ - где мы сейчас и почему? Куда идем? Скорость движения, волатильность (размах) движения и т.д.\n",
    "- _**Прогнозирование**_ - когда хотим узнать, что будет дальше и куда мы идем, с какой вероятностью достигнем цели\n",
    "- _**Поиск аномалий**_ - когда хотим понять, где были проблемы в прошлом или где происходило что-то необычное\n",
    "- _**Кластеризация и классификация**_ - когда временные ряды сами являются признаками объектов\n",
    "----\n",
    "\n",
    "### Пример визуализации временного ряда:\n",
    "![](./../src/imgs/example_ts.png)\n",
    "\n",
    "----\n",
    "### Где применяется анализ и моделирование временных рядов:\n",
    "- _**Финансовый Сектор (FinTech)**_ - прогнозирование и моделирование движений различных финансовых инструментов, моделирование спроса и предложения и т.д.\n",
    "- _**Показания датчиков**_ - прогнозирование активности пользователя по данным со смарт-часов, анализ нагрузки на датчики измерения давления и т.д.\n",
    "- _**Объемы продаж/производства**_\n",
    "- _**Телеметрия IT-систем**_ - RPS, нагрузки на серверное оборудование, объем запросов и т.д.\n",
    "- _**Маркетинг и Реклама**_ - анализ поведения рекламных кампаний, окупаемость рекламных кампаний, тренд, сезонность кампаний, ROI и т.д.\n",
    "- _**Поведенческий Анализ**_ - предсказания социальных активностей, моделирование поведения пользователей в приложении, предсказания оттока, измерение тренда настроений и т.д.\n",
    "...\n",
    "----\n",
    "### Способы визуализации временных рядов:\n",
    "- Простой линейный график\n",
    "- Box-Plot\n",
    "- Candlesticks\n",
    "- Bar Chart\n",
    "----\n",
    "\n",
    "### Составляющие временного ряда:\n",
    "----\n",
    "- _**Тренд**_ (T) - плавное долгосрочное изменение уровня временного ряда\n",
    "\n",
    "![](./../src/imgs/trend_ts.png)\n",
    "----\n",
    "- _**Сезонность**_ (S) - циклические изменения уровня временного ряда с постоянным периодом\n",
    "\n",
    "![](./../src/imgs/trend_seasonality_ts.png)\n",
    "\n",
    "- _**Цикл**_ (C) - изменения уровня временного ряда с переменным периодом (например, экономические циклы, политические циклы, периоды солнечной активности и т.д.)\n",
    "\n",
    "![](./../src/imgs/cycles_ts.png)\n",
    "\n",
    "- _**Случайная (Ненаблюдаемая) Ошибка**_ $\\epsilon$ - непрогнозируемая случайная компонента временного ряда\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Практика: чтение и базовый анализ временных рядов."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "from mplfinance.original_flavor import candlestick_ohlc # pip install --upgrade mplfinance\n",
    "import yfinance as yf  # pip install yfinance\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from dateutil.relativedelta import relativedelta # working with dates with style\n",
    "from scipy.optimize import minimize              # for function minimization\n",
    "import statsmodels.formula.api as smf            # statistics and econometrics\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "from itertools import product\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def get_price_data(ticker='TSLA'):\n",
    "    tickerData = yf.Ticker(ticker)\n",
    "    tickerDf = tickerData.history(period='1d', start='2010-1-1', end='2022-9-25')\n",
    "    return tickerDf.iloc[:, :5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def plot_candlestick(df, ax=None, fmt=\"%Y-%m-%d\"):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    idx_name = df.index.name if df.index.name else 'index'\n",
    "    dat = df.reset_index()[[idx_name, \"Open\", \"High\", \"Low\", \"Close\"]]\n",
    "    dat[idx_name] = dat[idx_name].map(mdates.date2num)\n",
    "    ax.xaxis_date()\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(fmt))\n",
    "    plt.xticks(rotation=45)\n",
    "    _ = candlestick_ohlc(ax, dat.values, width=.6, colorup='g', alpha =1)\n",
    "    ax.set_xlabel(idx_name)\n",
    "    ax.set_ylabel(\"OHLC\")\n",
    "    return ax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# get price data (return pandas dataframe)\n",
    "df = get_price_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Немного про работу с time series данными:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Когда мы читаем time series данные через Pandas, то по умолчанию колонке с датой присваивается кастомный тип данных, его необходимо перевести соответственно в формат time delta, TimeObj, TimeStamp и т.д.:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "air_quality = pd.read_csv('./../data/air_quality.csv').rename(columns={\"date.utc\": \"datetime\"})\n",
    "air_quality.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "air_quality.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Перевод в datetime object для работы с датой:\n",
    "air_quality['datetime'] = pd.to_datetime(air_quality['datetime'])\n",
    "air_quality['datetime'].head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Если вы заранее знаете название колонки с датой, то при загрузке вы автоматом можете указать что данная колонка является датой:\n",
    "air_quality = pd.read_csv('./../data/air_quality.csv', parse_dates=['datetime'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Теперь когда у нас колонка с типом данных TimesTamp мы можем реализовывать различные операции\n",
    "air_quality['datetime'].min(), air_quality['datetime'].max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Составим новый признак - месяц наблюдения:\n",
    "air_quality['month'] = air_quality['datetime'].dt.month\n",
    "air_quality.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Группировка и аггрегация данных time series:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Например нам необходимо посчитать среднее какого-либо признака для каждого для недели по каждой локации измерений (страна, канал рекламы, город и тд):\n",
    "air_quality.groupby(\n",
    "    [air_quality['datetime'].dt.weekday, \"location\"])['value'].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Также можно навесить на это визуализации:\n",
    "fig, axs = plt.subplots(figsize=(12, 4))\n",
    "air_quality.groupby(air_quality[\"datetime\"].dt.hour)[\"value\"].mean().plot(\n",
    "    kind='bar', rot=0, ax=axs)\n",
    "plt.xlabel(\"Hour of the day\");\n",
    "plt.ylabel(\"$NO_2 (µg/m^3)$\");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Очень часто для того чтобы эффективно работать с time series данными нам необходимо чтобы колонка с данными даты выступала в качестве индекса, тогда нам будут доступны более продвинутые статистические операции и анализ:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "air_quality_indexed = air_quality.pivot_table(index=\"datetime\", columns=\"location\", values=\"value\", aggfunc=\"mean\")\n",
    "air_quality_indexed.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "air_quality_indexed.index.year, air_quality_indexed.index.weekday"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Когда у нас дата в качестве индекса задача визуализации по нескольким измерениям (признакам) становится проще:\n",
    "air_quality_indexed[\"2019-05-20\":\"2019-05-21\"].plot();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Resampling и Reshaping:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "air_quality_indexed_by_month_max = air_quality_indexed.resample('M').max()\n",
    "air_quality_indexed_by_month_max"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Посмотрим по какому периоду проходит семплирование:\n",
    "air_quality_indexed_by_month_max.index.freq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "air_quality_indexed.resample(\"D\").mean().plot(style=\"-o\", figsize=(10,5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Полезные материалы по работе (чтнием, аггрегацией, сэмплированием) с time series датафреймами:\n",
    "\n",
    "------\n",
    "\n",
    "* [Working With Times Series Data Tutorial](https://towardsdatascience.com/working-with-time-series-data-a8872ebcac3)\n",
    "* [TS Data Manipulation](https://www.datacamp.com/tutorial/time-series-analysis-tutorial)\n",
    "* [Выдержка из книги Data Science Handbook по работе с TS](https://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Способы визуализации:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_**Обычный линейный график**_, который вы будете использовать почти всегда:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = df['2018-6-15':'2022-6-30']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data['Close'], linewidth=5.0)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Кто работал (или будет работать) с финансовыми данными, то очень хорошо подойдет _**Candle Sticks**_:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "plot_candlestick(data['2022-3-30':'2022-4-30'], fig.subplots())\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_**Box-Plot**_ вариант:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(df['Close'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Пройдемся немного по базовым вещам используя датасет _monthly-car-sales-in-quebec-1960.csv_:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/monthly-car-sales-in-quebec-1960.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df['Count'], linewidth=3)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# Выделяем месяц\n",
    "df['Month'] = df.apply(lambda s: int(s['Month'][-2:]), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Определим тупо перебором максимальные продажи:\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df['Count'], linewidth=3)\n",
    "plt.plot(df[df['Month']==5]['Count'], 'ro')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Тоже самое только с минимальными продажами:\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df['Count'], linewidth=3)\n",
    "plt.plot(df[df['Month']==9]['Count'], 'ro')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "При помощи графика рассеяния (scatterplot) посмотрите как связаны продажи в текущем месяце и в следующем:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Текущий месяц к следующему_:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = df['Count'][:-1]\n",
    "y = df['Count'][1:]\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Текущий месяц к $t+2$_:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = df['Count'][:-2]\n",
    "y = df['Count'][2:]\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Текущий месяц к $t+5$_:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = df['Count'][:-5]\n",
    "y = df['Count'][5:]\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Текущий месяц к $t+12$_:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = df['Count'][:-12]\n",
    "y = df['Count'][12:]\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Поговорим немного о такой штуке как автокорреляция:\n",
    "\n",
    "> Автокорреляция - это статистическая взаимосвязь (коэффициент корреляции) между последовательностями величин (данных, признаков) одного ряда, взятыми с некоторым сдвигом  $ \\tau $. Пришло к нам из эконометрики.\n",
    "\n",
    "Формула:\n",
    "<br>\n",
    "$R (\\tau) = \\frac{E((y_t - E_y)(y_t_+_\\tau - E_y))}{D_y}$\n",
    "\n",
    "<br>\n",
    "$\\rho (\\tau) = \\frac{\\sum_{t=1}^{T-\\tau}(y_t - \\hat{y})(y_t_+_\\tau - \\hat{y})}{\\sum_{i=1}^{T}(y_t - \\hat{y})^2}$\n",
    "\n",
    "\n",
    "Если у нас сильный коэффициент корреляции, то скорее всего во временном ряду присутствует сезонность.\n",
    "\n",
    "Как понять какая величина автокорреляции хорошая? А плохая?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Считаем автокорреляцию при помощи метода _pandas - autocorr():_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    print(i, df['Count'].autocorr(i + 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Коррелолаграмма - график автокорреляций для разных лагов:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(df['Count'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как понять какой коэффициент автокорреляции значимый:\n",
    "1. Просто посмотреть на график автокорреляций и решить (тупо но иногда рабочая схема).\n",
    "2. Проверить гипотвезу при помощи статистики:\n",
    "```\n",
    "Дано: Временной ряд: у(t)\n",
    "Нулевая гипотеза: H_0 : ro_tau = 0\n",
    "Альтернативная гипотеза: H_1 : ro_tao != 0\n",
    "```\n",
    "Считаем статистику:\n",
    "<br>\n",
    "$T(y_t) = \\frac{\\rho_\\tau \\sqrt{T - \\tau - 2}}{\\sqrt{1 - \\rho^2_t}}$\n",
    "\n",
    "Получаем распределение: $T(y_t) \\approx S_t (T - \\tau - 2)$\n",
    "\n",
    "Проверяем на P-value: 0.01, 0.05, 0.1 ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Познакомимся с временными рядами более конкретно:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = pd.read_csv('./../data/ads.csv', index_col=['Time'], parse_dates=['Time'])\n",
    "currency = pd.read_csv('./../data/currency.csv', index_col=['Time'], parse_dates=['Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Реальные часовые данные по запуску рекламной кампании (х - время, у - количество просмотров в час):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ads.Ads)\n",
    "plt.title('Ads watched (hourly data)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Откопал старенькие данные с позапрошлой работы, это внутриигровые покупки у пользователей в приложении за один день:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(currency.GEMS_GEMS_SPENT)\n",
    "plt.title('In-game currency spendings (daily data)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики качества прогноза\n",
    "\n",
    "Рассмотрим основные и самые распространенные метрики качества прогнозов, которые, по большому счету, являются метриками для задачи регрессии и используются далеко не только во временных рядах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [R squared](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination), коэффициент детерминации (в эконометрике - доля объясненной моделью дисперсии), $(-\\infty, 1]$\n",
    "\n",
    "$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$\n",
    "\n",
    "Очень отсталая и непригодная метрика, которую очень любит менеджмент, который прочитал пару статей про статистику :)\n",
    "\n",
    "Почему плохая метрика:\n",
    "1. Неинтерпретируемость - находится в рейндже не от 0 до 1. Поэтому ее иногда трудно объяснить. Почему? Как можно построить модель, которая дает качество хуже чем 0?\n",
    "2. Метрика не убывает при добавлении новых признаков в признаковое пространство данных. Если мы добавляем любой признак (даже если случайный шум) то метрика не увеличится, а может даже уменьшиться. Поэтому, например, еще и для отбора признаков метрика крайне не пригода.\n",
    "\n",
    "```python\n",
    "sklearn.metrics.r2_score\n",
    "```\n",
    "---\n",
    "- [Mean Absolute Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error), интерпретируемая метрика, измеряется в тех же единицах, что и исходный ряд, $[0, +\\infty)$\n",
    "\n",
    "$MAE = \\frac{\\sum\\limits_{i=1}^{n} |y_i - \\hat{y}_i|}{n}$\n",
    "\n",
    "Хорошая метрика, понятная, измеряется в тех же единицах, что и изначальный ряд, то есть с масштабом все более или менее понятно. 0 - отлично попали в прогноз и до бесконечности, когда у нас существуют какие-то расхождения.\n",
    "\n",
    "```python\n",
    "sklearn.metrics.mean_absolute_error\n",
    "```\n",
    "---\n",
    "- [Median Absolute Error](http://scikit-learn.org/stable/modules/model_evaluation.html#median-absolute-error), также интерпретируемая метрика, однако её преимущество - нечувствительность (робастность) к выбросам в данных, $[0, +\\infty)$\n",
    "\n",
    "$MedAE = median(|y_1 - \\hat{y}_1|, ... , |y_n - \\hat{y}_n|)$\n",
    "\n",
    "Тоже интерпретируемая метрика, которая хорошо работает в сильно зашумленных данных, где много выбросов.\n",
    "\n",
    "```python\n",
    "sklearn.metrics.median_absolute_error\n",
    "```\n",
    "---\n",
    "- [Mean Squared Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error), используется в большинстве случаев, сильнее наказывает модель за большие ошибки и меньше - за маленькие (парабола), $[0, +\\infty)$\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "Ее и так вы знаете.\n",
    "\n",
    "```python\n",
    "sklearn.metrics.mean_squared_error\n",
    "```\n",
    "---\n",
    "- [Mean Squared Logarithmic Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-logarithmic-error), практически тоже самое, но значения предварительно логарифмируются, таким образом маленьким ошибкам также уделяется значительное внимание, обычно используется, если данным присущ экспоненциальный рост, $[0, +\\infty)$\n",
    "\n",
    "$MSLE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (log(1+y_i) - log(1+\\hat{y}_i))^2$\n",
    "\n",
    "Почти тоже самое что и MSE, только мы добавляем еще логарифм. Важно если используете, убедитесь что ряд не околонулевой. Зачем она нужна? Логарифмирование временного ряда очень важная операция (как увидим впоследующем) в том случае, когда у нас есть какой-либо сильный экспоненциальный рост. Позволяет избавиться от некоторых допущений.\n",
    "\n",
    "```python\n",
    "sklearn.metrics.mean_squared_log_error\n",
    "```\n",
    "---\n",
    "- Mean Absolute Percentage Error, как MAE, только в процентах, - удобно для объяснения заказчику качества прогноза, $[0, +\\infty)$\n",
    "\n",
    "$MAPE = \\frac{100}{n}\\sum\\limits_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}$\n",
    "\n",
    "Очень хорошая метрика, супер интерпретируемая. Измеряется в %. Если MAPE = 10 то вы в среднем ошибаетесь на 10%. Стоит помнить про нее, но ее не удобно оптимизировать, особенно если вы используете ее в нейронной сети.\n",
    "\n",
    "```python\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем всё, о чем говорили выше\n",
    "\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, теперь мы знаем, как можно измерить качество нашего прогноза, какие метрики стоит применять и как объяснить всё заказчику, дело осталось за малым - нужно построить прогноз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Сглаживаем, оцениваем и прогнозируем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем моделирование с наивного предположения - \"завтра будет, как вчера\", но вместо модели вида $\\hat{y}_{t} = y_{t-1}$ будем считать, что будущее значение переменной зависит от среднего $n$ её предыдущих значений, а значит, воспользуемся скользящей средней. \n",
    "\n",
    "$$\\hat{y}_{t} = \\frac{1}{k} \\displaystyle\\sum^{k-1}_{n=0} y_{t-n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def moving_average(series, n):\n",
    "    \"\"\"\n",
    "        Calculate average of last n observations\n",
    "    \"\"\"\n",
    "    return np.average(series[-n:])\n",
    "\n",
    "moving_average(ads, 24) # prediction for the last observed day (past 24 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, такой прогноз долгосрочным сделать не удастся, для получения предсказания на шаг вперед предыдущее значение должно быть фактически наблюдаемой величиной.\n",
    "\n",
    "Зато у скользящей средней есть другое применение - сглаживание исходного ряда для выявления трендов, в pandas есть готовая реализация - [`DataFrame.rolling(window).mean()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html). Чем больше зададим ширину интервала - тем более сглаженным окажется тренд. В случае, если данные сильно зашумлены, что особенно часто встречается, например, в финансовых показателях, такая процедура может помочь увидеть общие паттерны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plotMovingAverage(series, window, plot_intervals=False, scale=1.95, plot_anomalies=False):\n",
    "\n",
    "    \"\"\"\n",
    "        series - dataframe with timeseries\n",
    "        window - rolling window size \n",
    "        plot_intervals - show confidence intervals\n",
    "        plot_anomalies - show anomalies \n",
    "\n",
    "    \"\"\"\n",
    "    # Считаем скользящую среднюю по нашему временному ряду используя тот метод Pandas про который было сказано выше\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"Moving average\\n window size = {}\".format(window))\n",
    "    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n",
    "\n",
    "    # Теперь необходимо посчитать доверительные интервалы:\n",
    "    if plot_intervals:\n",
    "        # Считаем метрику ошибки например среднюю абсолютную ашибку MAE так как те же единицы измерения что и исходный ряд\n",
    "        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
    "        # Возьмем посчитаем скользящее отклонение, то есть фактически доверительный интервал\n",
    "        deviation = np.std(series[window:] - rolling_mean[window:])\n",
    "        # Для нижней границы возьмем наш сглаженный ряд вычтем из него ошибку + стандартное отклонение\n",
    "        lower_bond = rolling_mean - (mae + scale * deviation)\n",
    "        # Аналогично для верхней границы только сложим все что посчитали\n",
    "        upper_bond = rolling_mean + (mae + scale * deviation)\n",
    "        plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n",
    "        plt.plot(lower_bond, \"r--\")\n",
    "        \n",
    "        # Having the intervals, find abnormal values\n",
    "        if plot_anomalies:\n",
    "            # Просто сравнение на значения верхней и нижней границы чтобы определить является ли наблюдение аномальным:\n",
    "            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n",
    "            anomalies[series<lower_bond] = series[series<lower_bond]\n",
    "            anomalies[series>upper_bond] = series[series>upper_bond]\n",
    "            plt.plot(anomalies, \"ro\", markersize=10)\n",
    "        \n",
    "    plt.plot(series[window:], label=\"Actual values\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим такое сглаживание к нашим рядам:\n",
    "\n",
    "Сглаживание по последним 4 часам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(ads, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По 12 часам (полдня)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(ads, 12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сглаживание по 24 часам (одни сутки) - выделяется дневной тренд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(ads, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно отобразить доверительные интервалы для наших средних значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(ads, 4, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь попробуем при помощи этих доверительных интервалов поймать аномалию во временном ряде. К сожалению или к счастью, конкретно в этом ряду всё более-менее хорошо и гладко, поэтому аномальное значение мы зададим самостоятельно в датафрейме `ads_anomaly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_anomaly = ads.copy()\n",
    "ads_anomaly.iloc[-20] = ads_anomaly.iloc[-20] * 0.2 # допустим что у нас появится дроп в рекламе который упал на 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим, удастся ли таким простым образом её поймать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(ads_anomaly, 4, plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоже самое можно проделать и со вторым рядом, попробуем сгладить его с окном в 7 дней, то есть по неделям, и поймать аномалии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(currency, 7, plot_intervals=True, plot_anomalies=True) # недельное сглаживание"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Вопрос на понимание_: __Когда такая система даст сбой? :)__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модификацией простой скользящей средней является взвешенная средняя, внутри которой наблюдениям придаются различные веса, в суме дающие единицу, при этом обычно последним наблюдениям присваивается больший вес. \n",
    "\n",
    "\n",
    "$$\\hat{y}_{t} = \\displaystyle\\sum^{k}_{n=1} \\omega_n y_{t+1-n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def weighted_average(series, weights):\n",
    "    \"\"\"\n",
    "        Calculate weighter average on series\n",
    "    \"\"\"\n",
    "    result = 0.0\n",
    "    weights.reverse()\n",
    "    for n in range(len(weights)):\n",
    "        result += series.iloc[-n-1] * weights[n]\n",
    "    return float(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "weighted_average(ads, [0.6, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Экспоненциальное сглаживание\n",
    "\n",
    "А теперь посмотрим, что произойдёт, если вместо взвешивания последних $n$ значений ряда мы начнем взвешивать все доступные наблюдения, при этом экспоненциально уменьшая веса по мере углубления в исторические данные. В этом нам поможет формула простого [экспоненциального сглаживания](http://www.machinelearning.ru/wiki/index.php?title=Экспоненциальное_сглаживание):\n",
    "\n",
    "$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1} $$\n",
    "\n",
    "Здесь модельное значение представляет собой средневзвешенную между текущим истинным и предыдущим модельным значениями. Вес $\\alpha$ называется сглаживающим фактором. Он определяет, как быстро мы будем \"забывать\" последнее доступное истинное наблюдение. Чем меньше $\\alpha$, тем больше влияния оказывают предыдущие модельные значения, и тем сильнее сглаживается ряд. \n",
    "\n",
    "Экспоненциальность скрывается в рекурсивности функции - каждый раз мы умножаем $(1-\\alpha)$ на предыдущее модельное значение, которое, в свою очередь, также содержало в себе $(1-\\alpha)$, и так до самого начала."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "    \"\"\"\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plotExponentialSmoothing(series, alphas):\n",
    "    \"\"\"\n",
    "        Plots exponential smoothing with different alphas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters\n",
    "        \n",
    "    \"\"\"\n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(15, 7))\n",
    "        for alpha in alphas:\n",
    "            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "        plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Exponential Smoothing\")\n",
    "        plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotExponentialSmoothing(ads.Ads, [0.3, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotExponentialSmoothing(currency.GEMS_GEMS_SPENT, [0.3, 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Двойное экспоненциальное сглаживание\n",
    "\n",
    "До сих пор мы могли получить от наших методов в лучшем случае прогноз лишь на одну точку вперёд (и ещё красиво сгладить ряд), это здорово, но недостаточно, поэтому переходим к расширению экспоненциального сглаживания, которое позволит строить прогноз сразу на две точки вперед (и тоже красиво сглаживать ряд).\n",
    "\n",
    "В этом нам поможет разбиение ряда на две составляющие - уровень (level, intercept) $\\ell$ и тренд $b$ (trend, slope). Уровень, или ожидаемое значение ряда, мы предсказывали при помощи предыдущих методов, а теперь такое же экспоненциальное сглаживание применим к тренду, наивно или не очень полагая, что будущее направление изменения ряда зависит от взвешенных предыдущих изменений.\n",
    "\n",
    "$$\\ell_x = \\alpha y_x + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n",
    "\n",
    "$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n",
    "\n",
    "$$\\hat{y}_{x+1} = \\ell_x + b_x$$\n",
    "\n",
    "В результате получаем набор функций. Первая описывает уровень - он, как и прежде, зависит от текущего значения ряда, а второе слагаемое теперь разбивается на предыдущее значение уровня и тренда. Вторая отвечает за тренд - он зависит от изменения уровня на текущем шаге, и от предыдущего значения тренда. Здесь в роли веса в экспоненциальном сглаживании выступает коэффициент $\\beta$. Наконец, итоговое предсказание представляет собой сумму модельных значений уровня и тренда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "code_folding": [
     0,
     20
    ]
   },
   "outputs": [],
   "source": [
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        alpha - float [0.0, 1.0], smoothing parameter for level\n",
    "        beta - float [0.0, 1.0], smoothing parameter for trend\n",
    "    \"\"\"\n",
    "    # first value is same as series\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
    "        trend = beta*(level-last_level) + (1-beta)*trend\n",
    "        result.append(level+trend)\n",
    "    return result\n",
    "\n",
    "def plotDoubleExponentialSmoothing(series, alphas, betas):\n",
    "    \"\"\"\n",
    "        Plots double exponential smoothing with different alphas and betas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters for level\n",
    "        betas - list of floats, smoothing parameters for trend\n",
    "    \"\"\"\n",
    "    \n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        for alpha in alphas:\n",
    "            for beta in betas:\n",
    "                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "        plt.plot(series.values, label = \"Actual\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Double Exponential Smoothing\")\n",
    "        plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotDoubleExponentialSmoothing(ads.Ads, alphas=[0.9, 0.02], betas=[0.9, 0.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotDoubleExponentialSmoothing(currency.GEMS_GEMS_SPENT, alphas=[0.9, 0.02], betas=[0.9, 0.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас появилась возможность настраивать уже два параметра - $\\alpha$ и $\\beta$. Первый отвечает за сглаживание ряда вокруг тренда, второй - за сглаживание самого тренда. Чем выше значения, тем больший вес будет отдаваться последним наблюдениям и тем менее сглаженным окажется модельный ряд. Комбинации параметров могут выдавать достаточно причудливые результаты, особенно если задавать их руками. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тройное экспоненциальное сглаживание a.k.a. Holt-Winters (1970):\n",
    "\n",
    "Итак, успешно добрались до следующего варианта экспоненциального сглаживания, на сей раз тройного.\n",
    "\n",
    "Идея этого метода заключается в добавлении еще одной, третьей, компоненты - сезонности. Сезонная компонента в модели будет объяснять повторяющиеся колебания вокруг уровня и тренда, а характеризоваться она будет длиной сезона - периодом, после которого начинаются повторения колебаний. Для каждого наблюдения в сезоне формируется своя компонента, например, если длина сезона составляет 7 (например, недельная сезонность), то получим 7 сезонных компонент, по штуке на каждый из дней недели.\n",
    "\n",
    "Получаем новую систему:\n",
    "\n",
    "$$\\ell_x = \\alpha(y_x - s_{x-L}) + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n",
    "\n",
    "$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n",
    "\n",
    "$$s_x = \\gamma(y_x - \\ell_x) + (1-\\gamma)s_{x-L}$$\n",
    "\n",
    "$$\\hat{y}_{x+m} = \\ell_x + mb_x + s_{x-L+1+(m-1)modL}$$\n",
    "\n",
    "Уровень теперь зависит от текущего значения ряда за вычетом соответствующей сезонной компоненты, тренд остаётся без изменений, а сезонная компонента зависит от текущего значения ряда за вычетом уровня и от предыдущего значения компоненты. При этом компоненты сглаживаются через все доступные сезоны, например, если это компонента, отвечающая за понедельник, от и усредняться она будет только с другими понедельниками. Подробнее про работу усреднений и оценку начальных значений тренда и сезонных компонент можно почитать [здесь](http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc435.htm). Теперь, имея сезонную компоненту, мы можем предсказывать уже не на один, и даже не на два, а на произвольные $m$ шагов вперёд, что не может не радовать. \n",
    "\n",
    "Ниже импортируем класс для построения модели тройного экспоненциального сглаживания, также известного по фамилиям её создателей - Чарльза Хольта и его студента Питера Винтерса. \n",
    "Дополнительно в модель включен метод Брутлага для построения доверительных интервалов:\n",
    "\n",
    "$$\\hat y_{max_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}+m⋅d_{t−T}$$\n",
    "\n",
    "$$\\hat y_{min_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}-m⋅d_{t−T}$$\n",
    "\n",
    "$$d_t=\\gamma∣y_t−\\hat y_t∣+(1−\\gamma)d_{t−T}$$\n",
    "\n",
    "где $T$ - длина сезона, $d$ - предсказанное отклонение, а остальные параметры берутся из тройного сглаживани. Подробнее о методе и о его применении к поиску аномалий во временных рядах можно прочесть [здесь](https://fedcsis.org/proceedings/2012/pliks/118.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# вроде есть в statsmodels или sklearn...\n",
    "class HoltWinters:\n",
    "    \"\"\"\n",
    "    Holt-Winters model with the anomalies detection using Brutlag method\n",
    "    # series - initial time series\n",
    "    # slen - length of a season\n",
    "    # alpha, beta, gamma - Holt-Winters model coefficients\n",
    "    # n_preds - predictions horizon\n",
    "    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n",
    "        self.series = series\n",
    "        self.slen = slen\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.n_preds = n_preds\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "    def initial_trend(self):\n",
    "        sum = 0.0\n",
    "        for i in range(self.slen):\n",
    "            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n",
    "        return sum / self.slen  \n",
    "    \n",
    "    def initial_seasonal_components(self):\n",
    "        seasonals = {}\n",
    "        season_averages = []\n",
    "        n_seasons = int(len(self.series)/self.slen)\n",
    "        # let's calculate season averages\n",
    "        for j in range(n_seasons):\n",
    "            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n",
    "        # let's calculate initial values\n",
    "        for i in range(self.slen):\n",
    "            sum_of_vals_over_avg = 0.0\n",
    "            for j in range(n_seasons):\n",
    "                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n",
    "            seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
    "        return seasonals\n",
    "          \n",
    "    def triple_exponential_smoothing(self):\n",
    "        self.result = []\n",
    "        self.Smooth = []\n",
    "        self.Season = []\n",
    "        self.Trend = []\n",
    "        self.PredictedDeviation = []\n",
    "        self.UpperBond = []\n",
    "        self.LowerBond = []\n",
    "        \n",
    "        seasonals = self.initial_seasonal_components()\n",
    "        \n",
    "        for i in range(len(self.series)+self.n_preds):\n",
    "            if i == 0: # components initialization\n",
    "                smooth = self.series[0]\n",
    "                trend = self.initial_trend()\n",
    "                self.result.append(self.series[0])\n",
    "                self.Smooth.append(smooth)\n",
    "                self.Trend.append(trend)\n",
    "                self.Season.append(seasonals[i%self.slen])\n",
    "                \n",
    "                self.PredictedDeviation.append(0)\n",
    "                \n",
    "                self.UpperBond.append(self.result[0] + \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                \n",
    "                self.LowerBond.append(self.result[0] - \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                continue\n",
    "                \n",
    "            if i >= len(self.series): # predicting\n",
    "                m = i - len(self.series) + 1\n",
    "                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n",
    "                \n",
    "                # when predicting we increase uncertainty on each step\n",
    "                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n",
    "                \n",
    "            else:\n",
    "                val = self.series[i]\n",
    "                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n",
    "                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n",
    "                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n",
    "                self.result.append(smooth+trend+seasonals[i%self.slen])\n",
    "                \n",
    "                # Deviation is calculated according to Brutlag algorithm.\n",
    "                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n",
    "                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n",
    "                     \n",
    "            self.UpperBond.append(self.result[-1] + \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.LowerBond.append(self.result[-1] - \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.Smooth.append(smooth)\n",
    "            self.Trend.append(trend)\n",
    "            self.Season.append(seasonals[i%self.slen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-валидация на временных рядах\n",
    "\n",
    "Перед тем, как построить модель, поговорим, наконец, о не ручной оценке параметров для моделей. \n",
    "\n",
    "Ничего необычного здесь нет, по-прежнему сначала необходимо выбрать подходящуюю для данной задачи функцию потерь, которая будет следить за качеством подгонки модели под исходные данные. Затем будем оценивать на кросс-валидации значение функции потерь при данных параметрах модели, искать градиент, менять в соответствии с ним параметры и бодро опускаться в сторону глобального минимума ошибки. \n",
    "\n",
    "Так как временной ряд внезапно имеет временную структуру, случайно перемешивать в фолдах значения всего ряда без сохранения этой структуры нельзя, иначе в процессе потеряются все взаимосвязи наблюдений друг с другом. Поэтому придется использовать чуть более хитрый способ для оптимизации параметров, официального названия которому я так и не нашел, но на сайте [CrossValidated](https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection), предлагают название \"cross-validation on a rolling basis\", что не дословно можно перевести как кросс-валидация на скользящем окне.\n",
    "\n",
    "Суть достаточно проста - начинаем обучать модель на небольшом отрезке временного ряда, от начала до некоторого $t$, делаем прогноз на $t+n$ шагов вперед и считаем ошибку. Далее расширяем обучающую выборку до $t+n$ значения и прогнозируем с $t+n$ до $t+2*n$, так продолжаем двигать тестовый отрезок ряда до тех пор, пока не упрёмся в последнее доступное наблюдение. В итоге получим столько фолдов, сколько $n$ уместится в промежуток между изначальным обучающим отрезком и всей длиной ряда.\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, зная, как настраивать кросс-валидацию, подберём с её помощью параметры для модели Хольта-Винтерса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def timeseriesCVscore(params, series, loss_function=mean_squared_error, slen=24):\n",
    "    \"\"\"\n",
    "        Returns error on CV\n",
    "        params - vector of parameters for optimization\n",
    "        series - dataset with timeseries\n",
    "        slen - season length for Holt-Winters model\n",
    "    \"\"\"\n",
    "    # errors array\n",
    "    errors = []\n",
    "    values = series.values\n",
    "    alpha, beta, gamma = params\n",
    "    # set the number of folds for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    # iterating over folds, train model on each, forecast and calculate error\n",
    "    for train, test in tscv.split(values):\n",
    "        model = HoltWinters(series=values[train], slen=slen, \n",
    "                            alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n",
    "        model.triple_exponential_smoothing()\n",
    "        predictions = model.result[-len(test):]\n",
    "        actual = values[test]\n",
    "        error = loss_function(predictions, actual)\n",
    "        errors.append(error)\n",
    "        \n",
    "    return np.mean(np.array(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модели Хольта-Винтерса, так же, как и в других моделях экспоненциального сглаживания, есть ограничение на величину сглаживающих коэффициентов. Каждый из них должен быть от 0 до 1. Соответственно, чтобы минимизировать нашу функцию потерь, нам нужно использовать метод, который поддерживает ограничения на оцениваемые параметры. В нашем случае это метод truncated Newton conjugate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data = ads.Ads[:-20] # leave some data for testing\n",
    "# initializing model parameters alpha, beta and gamma\n",
    "x = [0, 0, 0]\n",
    "# Minimizing the loss function \n",
    "opt = minimize(timeseriesCVscore, x0=x, \n",
    "               args=(data, mean_squared_error), \n",
    "               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1)))\n",
    "\n",
    "# Take optimal values...\n",
    "alpha_final, beta_final, gamma_final = opt.x\n",
    "print(alpha_final, beta_final, gamma_final)\n",
    "# ...and train the model with them, forecasting for the next 50 hours\n",
    "model = HoltWinters(data, slen = 24, \n",
    "                    alpha = alpha_final, \n",
    "                    beta = beta_final, \n",
    "                    gamma = gamma_final, \n",
    "                    n_preds = 50, scaling_factor = 3)\n",
    "model.triple_exponential_smoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plotHoltWinters(series, plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        plot_intervals - show confidence intervals\n",
    "        plot_anomalies - show anomalies \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(model.result, label = \"Model\")\n",
    "    plt.plot(series.values, label = \"Actual\")\n",
    "    error = mean_absolute_percentage_error(series.values, model.result[:len(series)])\n",
    "    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n",
    "    \n",
    "    if plot_anomalies:\n",
    "        anomalies = np.array([np.NaN]*len(series))\n",
    "        anomalies[series.values<model.LowerBond[:len(series)]] = \\\n",
    "            series.values[series.values<model.LowerBond[:len(series)]]\n",
    "        anomalies[series.values>model.UpperBond[:len(series)]] = \\\n",
    "            series.values[series.values>model.UpperBond[:len(series)]]\n",
    "        plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    if plot_intervals:\n",
    "        plt.plot(model.UpperBond, \"r--\", alpha=0.5, label = \"Up/Low confidence\")\n",
    "        plt.plot(model.LowerBond, \"r--\", alpha=0.5)\n",
    "        plt.fill_between(x=range(0,len(model.result)), y1=model.UpperBond, \n",
    "                         y2=model.LowerBond, alpha=0.2, color = \"grey\")    \n",
    "        \n",
    "    plt.vlines(len(series), ymin=min(model.LowerBond), ymax=max(model.UpperBond), linestyles='dashed')\n",
    "    plt.axvspan(len(series)-20, len(model.result), alpha=0.3, color='lightgrey')\n",
    "    plt.grid(True)\n",
    "    plt.axis('tight')\n",
    "    plt.legend(loc=\"best\", fontsize=13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotHoltWinters(ads.Ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotHoltWinters(ads.Ads, plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если посмотреть на смоделированное отклонение, хорошо видно, что модель достаточно резко регирует на значительные изменения в структуре ряда, но при этом быстро возвращает дисперсию к обычным значениям, \"забывая\" прошлое. Такая особенность позволяет неплохо и без значительных затрат на подготовку-обучение модели настроить систему по детектированию аномалий даже в достаточно шумных рядах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "plt.plot(model.PredictedDeviation)\n",
    "plt.grid(True)\n",
    "plt.axis('tight')\n",
    "plt.title(\"Brutlag's predicted deviation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогоним тот же самый алгоритм для второго временного ряда, в котором, как мы знаем, есть тренд и месячная сезонность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data = currency.GEMS_GEMS_SPENT[:-50] \n",
    "slen = 30 # 30-day seasonality\n",
    "\n",
    "x = [0, 0, 0]\n",
    "opt = minimize(timeseriesCVscore, x0=x, \n",
    "               args=(data, mean_absolute_percentage_error, slen), \n",
    "               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1))\n",
    "              )\n",
    "\n",
    "alpha_final, beta_final, gamma_final = opt.x\n",
    "print(alpha_final, beta_final, gamma_final)\n",
    "\n",
    "model = HoltWinters(data, slen = slen, \n",
    "                    alpha = alpha_final, \n",
    "                    beta = beta_final, \n",
    "                    gamma = gamma_final, \n",
    "                    n_preds = 100, scaling_factor = 3)\n",
    "model.triple_exponential_smoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotHoltWinters(currency.GEMS_GEMS_SPENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит весьма правдоподобно, модель уловила и восходящий тренд, и сезонные пики, и в целом достаточно точно описывает наш временной ряд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotHoltWinters(currency.GEMS_GEMS_SPENT, plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(model.PredictedDeviation)\n",
    "plt.grid(True)\n",
    "plt.axis('tight')\n",
    "plt.title(\"Brutlag's predicted deviation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Эконометрический подход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стационарность\n",
    "\n",
    "Перед тем, как перейти к моделированию, стоит сказать о таком важном свойстве временного ряда, как [**стационарность**](https://ru.wikipedia.org/wiki/Стационарность). \n",
    "Под стационарностью понимают свойство процесса не менять своих статистических характеристик с течением времени, а именно постоянство матожидания, постоянство дисперсии (она же [гомоскедастичность](https://ru.wikipedia.org/wiki/Гомоскедастичность)) и независимость ковариационной функции от времени (должна зависеть только от расстояния между наблюдениями). Наглядно можно посмотреть на эти свойства на картинках, взятых из поста [Sean Abu](http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/):\n",
    "\n",
    "- Временной ряд справа не является стационарным, так как его матожидание со временем растёт\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/20c/9d8/a63/20c9d8a633ec436f91dccd4aedcc6940.png\"/>\n",
    "\n",
    "- Здесь не повезло с дисперсией - разброс значений ряда существенно варьируется в зависимости от периода\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/b88/eec/a67/b88eeca676d642449cab135273fd5a95.png\"/>\n",
    "\n",
    "- Наконец, на последнем графике видно, что значения ряда внезапно становятся ближе друг ко другу, образуя некоторый кластер, а в результате получаем непостоянство ковариаций\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/2f6/1ee/cb2/2f61eecb20714352840748b826e38680.png\"/>\n",
    "\n",
    "Почему стационарность так важна? По стационарному ряду просто строить прогноз, так как мы полагаем, что его будущие статистические характеристики не будут отличаться от наблюдаемых текущих. Большинство моделей временных рядов так или иначе моделируют и предсказывают эти характеристики (например, матожидание или дисперсию), поэтому в случае нестационарности исходного ряда предсказания окажутся неверными. К сожалению, большинство временных рядов, с которыми приходится сталкиваться за пределыми учебных материалов, стационарными не являются, но с этим можно (и нужно) бороться.\n",
    "\n",
    "<br>\n",
    "\n",
    "Есть несколько вариантов стационарности:\n",
    "1. Строгая стационарность - ряд y(t) нзывается строго стационарным, если для любого s совместное распределение верояностей y(t), y(t+1),..., y(y+k) совпадает с совместным распределением вероятностей y(t+s), y(t+s+1),...,y(y+s+k).\n",
    "2. Слабая стационарность ряда y(t) если E(y(t)) = cont и cov(y(t), y(t+s)) зависит только от s\n",
    "\n",
    "Важное правило: Всякий строго стационарный ряд так же и слабо стационарен.\n",
    "\n",
    "Зачем все эти разговоры про стационарность:\n",
    "\n",
    "* Стационарный ряд легче предсказывать. Можно предположить, что статистические свойства распределения не изменяться.\n",
    "* В большинстве моделей временных рядов закладывается гипотеза о стационарности временного ряда. Это означает, что получаемые оценки будут надежными только в случае стационарности временного ряда.\n",
    "\n",
    "\n",
    "Стационарность:\n",
    "* Тренд --> нестационарность\n",
    "* Сезонность --> нестационарность\n",
    "* Цикл --> ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Интуитивное понимание:\n",
    "\n",
    "![](./../src/imgs/stationarity.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как превратить ряд в стационарный:\n",
    "1. Для рядов с монотонно меняющейся дисперсией - логарифмирование\n",
    "2. Дифференцирование – переход к попарным разностям. Позволяет избавиться от тренда\n",
    "3. Сезонное дифференцирование – вычитаем не предыдущее значение, а сдвинутое на сезон\n",
    "4. Дифференцировать можно несколько раз подряд\n",
    "\n",
    "![](./../src/imgs/stat_to_no_stat.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы бороться с нестационарностью, нужно узнать её в лицо, потому посмотрим, как её детектировать. Для этого обратимся к белому шуму и случайному блужданию, чтобы выяснить как попасть из одного в другое.\n",
    "\n",
    "\n",
    "График белого шума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "white_noise = np.random.normal(size=1000)\n",
    "with plt.style.context('bmh'):  \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(white_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, процесс, порожденный стандартным нормальным распределением, стационарен, колеблется вокруг нуля с отклонением в 1. Теперь на основании него сгенерируем новый процесс, в котором каждое последующее значение будет зависеть от предыдущего: $x_t = \\rho x_{t-1} + e_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plotProcess(n_samples=1000, rho=0):\n",
    "    x = w = np.random.normal(size=n_samples)\n",
    "    for t in range(n_samples):\n",
    "        x[t] = rho * x[t-1] + w[t]\n",
    "\n",
    "    with plt.style.context('bmh'):  \n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(x)\n",
    "        plt.title(\"Rho {}\\n Dickey-Fuller p-value: {}\".format(rho, round(sm.tsa.stattools.adfuller(x)[1], 3)))\n",
    "        \n",
    "for rho in [0, 0.6, 0.9, 1]:\n",
    "    plotProcess(rho=rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На первом графике получился точно такой же стационарный белый шум, который строился раньше. На втором значение  увеличилось до $0.6$, в результате чего на графике стали появляться более широкие циклы, но в целом стационарным он быть пока не перестал. Третий график всё сильнее отклоняется от нулевого среднего значения, но всё ещё колеблется вокруг него. Наконец, значение  равное единице дало процесс случайного блуждания — ряд не стационарен.\n",
    "\n",
    "\n",
    "Происходит это из-за того, что при достижении критической единицы, ряд $x_t = \\rho x_{t-1} + e_t$ перестаёт возвращаться к своему среднему значению. Если вычесть из левой и правой части $x_{t-1}$, то получим  $x_t - x_{t-1} = (\\rho - 1) x_{t-1} + e_t$, где выражение слева — первые разности. Если $\\rho=1$, то первые разности дадут стационарный белый шум $e_t$. Этот факт лёг в основу теста Дики-Фуллера на стационарность ряда (наличие единичного корня). Если из нестационарного ряда первыми разностями удаётся получить стационарный, то он называется интегрированным первого порядка. Нулевая гипотеза теста — ряд не стационарен, отвергалась на первых трех графиках, и принялась на последнем. Стоит сказать, что не всегда для получения стационарного ряда хватает первых разностей, так как процесс может быть интегрированным с более высоким порядком (иметь несколько единичных корней), для проверки таких случаев используют расширенный тест Дики-Фуллера, проверяющий сразу несколько лагов.\n",
    "\n",
    "\n",
    "Бороться с нестационарностью можно множеством способов — разностями различного порядка, выделением тренда и сезонности, сглаживаниями и преобразованиями, например, Бокса-Кокса или логарифмированием."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Практика: Избавляемся от нестационарности:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/monthly-sales-of-company-x-jan-6.csv')\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df['Count'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "autocorrelation_plot(df['Count'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нормализуйте дисперсию временного ряда (scipy boxcox):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "modified = boxcox(df['Count'], 0)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(modified)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Продифференцируйте на n сдвиг:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "modified = np.diff(modified, 1)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(modified)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "autocorrelation_plot(modified)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Тест Дики-Фуллера:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries):\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "    for [key, value] in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' % key] = value\n",
    "    print(dfoutput)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_stationarity(df['Count'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_stationarity(modified)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Полноценный Анализ Временного ряда:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries):\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "    for [key, value] in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' % key] = value\n",
    "    print(dfoutput)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(14, 8), style='bmh'):\n",
    "    test_stationarity(y)\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    with plt.style.context(style):\n",
    "        plt.figure(figsize=figsize)\n",
    "        layout = (4, 1)\n",
    "        ts_ax = plt.subplot2grid(layout, (0, 0), rowspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (2, 0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (3, 0))\n",
    "\n",
    "        y.plot(ax=ts_ax, color='blue', label='Or')\n",
    "        ts_ax.set_title('Original')\n",
    "\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.05)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.05)\n",
    "\n",
    "        plt.tight_layout()\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series = pd.read_csv(\"./../data/international-airline-passengers.csv\")['Count']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tsplot(series)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как мы видим, и тест Дики-Фуллера и графики коррелограмм не отвергают гипотезу о нестационарности ряда. Для начала уберем изменение дисперсии при помощи преобразования Бокса-Кокса"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series = boxcox(series, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tsplot(series)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нам удалось убрать размах дисперсии, но тест Дикки-Фуллера все еще не отвергает гипотезу о нестационарности ряда. По графику ряда видно наличие сильного тренда. Уберем его дифференцированием."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series = series[1:] - series[:-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tsplot(series)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Стало еще лучше, но по графику коррелограммы видно сильное влияние сезонности. Уберем ее"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series = series[12:] - series[:-12]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tsplot(series)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь тест Дики-Фуллера и графики коррелограмм отвергают гипотезу о нестационарности ряда!"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
