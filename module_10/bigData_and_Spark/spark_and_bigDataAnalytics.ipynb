{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center> **Модуль 10. Лекция 1: Работа с BigData. Apache Spark.** </center>\n",
    "----\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/intro_img.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "----\n",
    "\n",
    "## __План на сегодня:__\n",
    "\n",
    "----\n",
    "\n",
    "### 1. __Big Data и зачем всё это нужно.__\n",
    "### 2. __Apache Spark для работы с большими данными.__\n",
    "### 3. __MLlib Apache Spark и как обучать модели на основе больших данных (Опционально).__\n",
    "### 4. __Практика и еще раз практика.__\n",
    "### 5. __Q&A__.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Часть 1. Big Data и зачем всё это нужно. Фреймворк Apache Spark для работы с большими данными.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/spark_ecosystem.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__Как анализировать и работать с большими данными?__\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "* В 2004 году выходит статья про __MapReduce__ от Google.\n",
    "* В 2006 году Doug Cutting проектирует экосистему Hadoop вместе с Yahoo!\n",
    "* В 2008 году Hadoop становится top-level проектом под экосистемой __Apache__ и технология MapReduce интегрируется в Hadoop.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Но тогда было пару проблем, связанных с технологией MapReduce:\n",
    "- Медлительность\n",
    "- Сложность\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/speed_mr.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "И уже в 2009 году появляется проект Spark, который взял всё лучшее и оптимизровал работу экосистемы для работы с большими данными и в 2014 году он становится полностью open-source для того, чтобы технология могла развиваться и использоваться повсеместно.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__Немного поговорим о том, что такое Apache Spark и что это такое и где это нужно.__\n",
    "Apache Spark - это быстрый и мощный фреймворк для обработки больших данных. Apache Spark помогает быстро и эффективно обрабатывать большие массивы структурированных и неструктурированных данных и проводить аналитику, искать ответы на сложные вопросы при помощи данных, а также строить модели машинного обучения используя большие массивы данных.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Условно сам Apache Spark написан на языке программирования Scala и сам Spark поддерживает API для нескольких языков, которые очень популярны при работе с анализом данных и в машинном обучении:\n",
    "- Scala\n",
    "- Java\n",
    "- Python\n",
    "- C/C++ (только для подмодуля RDDs)\n",
    "- R\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/spark_api.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Весь Apache Spark состоит из 4-х подмодулей:\n",
    "1. Spark SQL - ETL процедуры, ad-hoc аналитика и процессинг данных.\n",
    "2. Spark Streaming - стриминг потоковых данных (Data Engineering часть), анализа данных в режиме реального времени (real-time)\n",
    "3. MLlib - для обучения моделей машинного обучения на больших данных. Постоянно развивается и дополняется новыми алгоритмами.\n",
    "4. GraphX - для графовых алгоритмов и для работы с графовыми структурами данных. Аналог NetworkX - отдельной библиотеки для экосистемы Python для работы с графами.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Для чего вам может понадобиться такой инструмент как Apache Spark?\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "В качестве ответа на вопрос __Зачем?__ рассмотрим гипотетический пример:\n",
    "\n",
    "Допустим, что у нас имеются логи мобильного приложения (игры) и пользователях этого приложения (какие действия они совершали, что делали и когда и т.д.). Хранится это все локально либо на сервере в виде файлов. К нам пришли и попросили для целей аналитики посчитать среди этого огромного количества данных __количество уникальных визитов за каждый день__. Что будем делать, варианты:\n",
    "\n",
    "- __1GB данных__ - всё просто - pandas в помощь.\n",
    "- __10GB данных__ - уже проблематично, но можно справиться - берем комп помощнее, pandas и вперед.\n",
    "- __100GB данных__  - 0_o?\n",
    "- __1000GB данных__ - 0_о?\n",
    "\n",
    "\n",
    "\n",
    "Если вы будете работать с огромным массивом информации, то в роли Data Scientist/ML Engineer/Analyst вам необходимо будет использовать весь такой огромный массив информации ради того, чтобы выполнить свою задачу. Обычными инструментами это сделать проблематично, поэтому на помощь всегда придет Spark.\n",
    "\n",
    "В задачах Data Science используется для:\n",
    "1. Препроцессинга данных и построения ETL.\n",
    "2. Обучения моделей машинного обучения.\n",
    "3. Анализ данных и поиск ответов на вопросы.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Основа работы Spark - это __распределенные вычисления__:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/distributed_processing.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Окей, что ещё за распределенные вычисления и как они работают?\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__Apache Spark__:\n",
    "- Установлен на каждой ноде кластера\n",
    "- Между нодами есть сетевое соединение\n",
    "- Spark знает о других нодах и держит их на \"связи\"\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Таким образом весь процесс можно выразить при помощи следующей архитектуры:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/spark_nodes.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__Установка и работа со Spark__:\n",
    "\n",
    "Нужен ли Hadoop чтобы запускать Spark (мы ведь работаем в другой экосистемой) - нет (но с ним много интеграций)Ж\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/hadoop_integrations.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Практика 1. Установка и настройка Apache Spark.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Установка может производится локально:\n",
    "1. ```pip install pyspark``` создаем виртуальное окружение и устанавливаем при помощи пакетного менеджера pip.\n",
    "2. Jupyter + Spark в Docker контейнере.\n",
    "3. Google Collab.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__Attention! В ОС Windows чаще всего происходят сбои и неполадки с движком PySpark API поэтому для лучшей воспроизводимости и закрепления материала запускается это всё на UNIX-подобных ОС либо в Google Collab.__\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "1. __Установка PySpark локально при помощи пакетного менеджера pip__:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Что вам нужно:\n",
    "* Python >= 3.6\n",
    "* Установленная Java/OpenJDK версии >= 11.x.x Как проверить установку из терминала для __Linux/macOS__:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "  ```\n",
    "    $ java -version\n",
    "\n",
    "    openjdk version \"x.x.x\" YYYY-MM-DD\n",
    "    OpenJDK Runtime Environment (build xx.x.x)\n",
    "    OpenJDK 64-Bit Server VM (build xx.x.x, mixed mode, sharing)\n",
    "  ```\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "* Создаем виртуальное окружение Python (либо используем автоматизированный процесс в PyCharm):\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "```\n",
    "python3 -m venv venv\n",
    "```\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "* Активируем окружение:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "```\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "* Устанавливаем PySpark при помощи pip:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "```\n",
    "(venv) pip install pyspark\n",
    "```\n",
    "\n",
    "<br>\n",
    "</br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "2. __Установка PySpark при помощи Docker (про Docker чуть позже), чтобы знали__:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "* Необходимо установить [Docker](https://www.docker.com/)\n",
    "* Взять [образ Jupyter Notebook](https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html) c установленным Apache Spark\n",
    "* Запускаем контейнер в интерактивном режиме на 8888 порту\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "  ```\n",
    "docker run -it -p 8888:8888 -p 4040:4040 -v $(pwd):/home/work/jupyter/pyspark-notebook\n",
    "  ```\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Как закончили работу основить контейнер можно при помощи __CMD + C__ (CTRL + C)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. __Запуск Apache Spark внутри Google Collab ноутбука:__\n",
    "\n",
    "\n",
    "* Копируем себе ноутбук ```https://colab.research.google.com/drive/1WAS4NQh2eAILZqhrS5RzilQPgvwH1ZtV?usp=sharing```\n",
    "* Выполняем инструкции которые я описал в ноутбуке для работы (проверьте версию установочного дистрибутива PySpark на всякий случай)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Существует 2 режима работы Spark__:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "1. Интерактивный (REPL)\n",
    "    - spark-shell (Scala)\n",
    "    - pyspark (Python)\n",
    "    - sparkr (R)\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "2. Как отдельная программа (standalone program)\n",
    "    - spark-submit (.jar, .py, .r)\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Ну и какого лешего мне этот PySpark сдался или что можно делать в PySpark:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "Типичная задача на практике выглядит следующим образом:\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<center> <img src=./../src/imgs/typical_task_pyspark.png> </center>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "Рассмотрим основные компоненты работы PySpark приложения:\n",
    "\n",
    "- __SparkSession__ - ваша точка входа в Spark API, инициализация происходит при помощи команды `spark = SparkSession.builder.getOrCreate()`\n",
    "- __Несколько видов абстракции данных__:\n",
    "    * RDD - Resilient Distributed Dataset\n",
    "        * Распределенный датасет\n",
    "        * Устойчив к ошибкам\n",
    "        * Исторически связан с представлением данных в Hadoop\n",
    "        * Труден в понимании и использовании если вы не Data Engineer/Администратор БД\n",
    "    * Dataset\n",
    "        * Строго типизированный RDD\n",
    "        * Не доступен в Python\n",
    "    * DataFrame - то с чем мы будем работать\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__DataFrame__ - это DataSet организованный в виде tuple структуры данных. Очень похож на Pandas DataFrame.\n",
    "- Состовит из строк и колонок\n",
    "- Имеет очень удобный и понятный API особенно для тех кто знаком с SQL\n",
    "- Был придуман как часть модуля [SparkSQL](https://www.databricks.com/glossary/what-is-spark-sql)\n",
    "- Вместо DataFrame API можно писать SQL-подобные запросы и вычисления\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__Процесс чтения данных в DataFrame__:\n",
    "\n",
    "`spark.read`\n",
    "-    .format(\"csv\") - json, parquet, avro, orc, jdbc ...\n",
    "-    .schema(schema_object) - optional, schema discovery by default\n",
    "-    .option(key, value) - (\"header\", \"true\"), optional\n",
    "-    .load(path) - в некоторых случаях \"path\" является optional\n",
    "\n",
    "Пример использования:\n",
    "\n",
    "`spark.read.csv(\"./path/to/file/file.csv\", header=True)`\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "[Документация](https://spark.apache.org/docs/latest/sql-data-sources.html)\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__Трансформируем данные__\n",
    "\n",
    "- `spark.read` возвращает DataFrame\n",
    "- DataFrame API\n",
    "    - очень похож на SQL (select, where, union, join, groupBy, orderBy)\n",
    "    - методы DataFrame можно составлять в цепочки\n",
    "- Есть пакет `pyspark.sql.functions` который содержит большое кол-во дополнительных функций\n",
    "    - помогают расширить функционал DataFrame API\n",
    "    - тоже заимствованы из SQL\n",
    "    - поддерживаются оконные функции\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__Запись данных__\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "`df.write:`\n",
    "- .mode(SaveMode) - \"overwrite\", \"append\", ...\n",
    "- .format(\"csv\")\n",
    "- .save(path)\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "\n",
    "__А как в SQL?__\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "- DataFrame можно зарегистрировать как temporary view `df.createOrReplaceTempView(“table_name”)`\n",
    "- И писать SQL к view: `spark.sql(“select * from table_name where id = 123”)` возвращает DataFrame\n",
    "- Можно использовать SQL выражения в .where() `df.where(“country = ‘US’ and shipping_date > ‘2023-01-01’”)`\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__DataFrame API__:\n",
    "\n",
    "- `df.show(n)`\n",
    "- `df.filter(condition)`\n",
    "- `df.count()`\n",
    "- `df.groupBy(<columns>)`\n",
    "- `df.withColumnRenamed(<current_name>, <new_name>)`\n",
    "- `df.printSchema()`\n",
    "- `df.join(df2, on, how)`\n",
    "- `df.select(<columns>)`\n",
    "- `df.where(condition)`\n",
    "- `df.distinct()`\n",
    "- `df.agg(<aggregation_functions>)`\n",
    "- `df.describe()`\n",
    "- `df.union(df2)`\n",
    "- `df.limit(n)`\n",
    "- `df.orderBy(<columns>)`\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__SQL Functions__:\n",
    "\n",
    "`import pyspark.sql.functions as F`\n",
    "\n",
    "- `F.col(\"name\")`\n",
    "- `F.col().isNull()`\n",
    "- `F.col() ==, >, >=, <, <=, !=`\n",
    "- `F.when(cond, then)...when()...otherwise()`\n",
    "- `F.lit()`\n",
    "- `F.to_date()`\n",
    "- `F.col().alias()`\n",
    "- `F.col().isNotNull()`\n",
    "- `F.col().between(from, to)`\n",
    "- `F.date_format(date, format)`\n",
    "- `df.alias()`\n",
    "- `F.col().isin()`\n",
    "- `F.from_unix_time()`\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "__SQL Aggregation and Window Functions__:\n",
    "\n",
    "- Aggregation Functions:\n",
    "    - `F.count(<col>)`\n",
    "    - `F.countDistinct(<column>)`\n",
    "    - `F.sum() / F.min() / F.max() / F.avg()`\n",
    "    - `F.first() / F.last()`\n",
    "\n",
    "\n",
    "- Window Functions:\n",
    "    - `w = Window.partitionBy(<columns>).orderBy(<columns>)`\n",
    "    - `F.row_number().over(w)`\n",
    "    - `F.lag(<columns>).over(w)`\n",
    "    - `F.lead(<columns>).over(w)`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Практика 2. Потренируемся исследовать данные и строить базовый ETL пайплайн при помощи Apache Spark DataFrame API.\n",
    "\n",
    "---\n",
    "\n",
    "Задачи:\n",
    "1. Загрузить датасет\n",
    "2. Изучить данные (применим функции вывода, агрегации и тд)\n",
    "3. Построить базовый ETL пайплайн для обработки и выгрузки данных с локального источника"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maratmovlamov/Desktop/DS_Foundations/venv/lib/python3.6/site-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x7fb9dda395f8>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://marat:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Practice</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создадите SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()\n",
    "spark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чтение csv файла:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manufacturer_name,model_name,transmission,color,odometer_value,year_produced,engine_fuel,engine_has_gas,engine_type,engine_capacity,body_type,has_warranty,state,drivetrain,price_usd,is_exchangeable,location_region,number_of_photos,up_counter,feature_0,feature_1,feature_2,feature_3,feature_4,feature_5,feature_6,feature_7,feature_8,feature_9,duration_listed\r\n",
      "Subaru,Outback,automatic,silver,190000,2010,gasoline,False,gasoline,2.5,universal,False,owned,all,10900.0,False,Минская обл.,9,13,False,True,True,True,False,True,False,True,True,True,16\r\n",
      "Subaru,Outback,automatic,blue,290000,2002,gasoline,False,gasoline,3.0,universal,False,owned,all,5000.0,True,Минская обл.,12,54,False,True,False,False,True,True,False,False,False,True,83\r\n",
      "Subaru,Forester,automatic,red,402000,2001,gasoline,False,gasoline,2.5,suv,False,owned,all,2800.0,True,Минская обл.,4,72,False,True,False,False,False,False,False,False,True,True,151\r\n",
      "Subaru,Impreza,mechanical,blue,10000,1999,gasoline,False,gasoline,3.0,sedan,False,owned,all,9999.0,True,Минская обл.,9,42,True,False,False,False,False,False,False,False,False,False,86\r\n"
     ]
    }
   ],
   "source": [
    "!head -n5 ./data/cars.csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# прочитайте данные в DataFrame c автоматическим определение схемы:\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/cars.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрите на данные:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------------+------+--------------+-------------+-----------+--------------+-----------+---------------+---------+------------+-----+----------+---------+---------------+---------------+----------------+----------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------------+\n",
      "|manufacturer_name|model_name|transmission| color|odometer_value|year_produced|engine_fuel|engine_has_gas|engine_type|engine_capacity|body_type|has_warranty|state|drivetrain|price_usd|is_exchangeable|location_region|number_of_photos|up_counter|feature_0|feature_1|feature_2|feature_3|feature_4|feature_5|feature_6|feature_7|feature_8|feature_9|duration_listed|\n",
      "+-----------------+----------+------------+------+--------------+-------------+-----------+--------------+-----------+---------------+---------+------------+-----+----------+---------+---------------+---------------+----------------+----------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------------+\n",
      "|           Subaru|   Outback|   automatic|silver|        190000|         2010|   gasoline|         False|   gasoline|            2.5|universal|       False|owned|       all|  10900.0|          False|   Минская обл.|               9|        13|    False|     True|     True|     True|    False|     True|    False|     True|     True|     True|             16|\n",
      "|           Subaru|   Outback|   automatic|  blue|        290000|         2002|   gasoline|         False|   gasoline|            3.0|universal|       False|owned|       all|   5000.0|           True|   Минская обл.|              12|        54|    False|     True|    False|    False|     True|     True|    False|    False|    False|     True|             83|\n",
      "|           Subaru|  Forester|   automatic|   red|        402000|         2001|   gasoline|         False|   gasoline|            2.5|      suv|       False|owned|       all|   2800.0|           True|   Минская обл.|               4|        72|    False|     True|    False|    False|    False|    False|    False|    False|     True|     True|            151|\n",
      "|           Subaru|   Impreza|  mechanical|  blue|         10000|         1999|   gasoline|         False|   gasoline|            3.0|    sedan|       False|owned|       all|   9999.0|           True|   Минская обл.|               9|        42|     True|    False|    False|    False|    False|    False|    False|    False|    False|    False|             86|\n",
      "|           Subaru|    Legacy|   automatic| black|        280000|         2001|   gasoline|         False|   gasoline|            2.5|universal|       False|owned|       all|  2134.11|           True|Гомельская обл.|              14|         7|    False|     True|    False|     True|     True|    False|    False|    False|    False|     True|              7|\n",
      "+-----------------+----------+------------+------+--------------+-------------+-----------+--------------+-----------+---------------+---------+------------+-----+----------+---------+---------------+---------------+----------------+----------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# выведите в консоль 5 строк из датафрейма:\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Дополнительный параметр `vertical=True` выведет каждую строку данных построчно в виде колонка | значение"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " manufacturer_name | Subaru       \n",
      " model_name        | Outback      \n",
      " transmission      | automatic    \n",
      " color             | silver       \n",
      " odometer_value    | 190000       \n",
      " year_produced     | 2010         \n",
      " engine_fuel       | gasoline     \n",
      " engine_has_gas    | False        \n",
      " engine_type       | gasoline     \n",
      " engine_capacity   | 2.5          \n",
      " body_type         | universal    \n",
      " has_warranty      | False        \n",
      " state             | owned        \n",
      " drivetrain        | all          \n",
      " price_usd         | 10900.0      \n",
      " is_exchangeable   | False        \n",
      " location_region   | Минская обл. \n",
      " number_of_photos  | 9            \n",
      " up_counter        | 13           \n",
      " feature_0         | False        \n",
      " feature_1         | True         \n",
      " feature_2         | True         \n",
      " feature_3         | True         \n",
      " feature_4         | False        \n",
      " feature_5         | True         \n",
      " feature_6         | False        \n",
      " feature_7         | True         \n",
      " feature_8         | True         \n",
      " feature_9         | True         \n",
      " duration_listed   | 16           \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Функционал DataFrame API и работа с ним.\n",
    "\n",
    "`.select()`\n",
    "`.select()` - выбирает только необходимые колонки (аналог `SELECT <column_names> FROM <db.name>` в SQL):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------+\n",
      "|manufacturer_name|model_name| color|\n",
      "+-----------------+----------+------+\n",
      "|           Subaru|   Outback|silver|\n",
      "|           Subaru|   Outback|  blue|\n",
      "|           Subaru|  Forester|   red|\n",
      "|           Subaru|   Impreza|  blue|\n",
      "|           Subaru|    Legacy| black|\n",
      "+-----------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# выберите несколько колонок для отображения\n",
    "df.select(\"manufacturer_name\", \"model_name\", \"color\").show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Можно обратиться к колонке через указание датафрейма:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------+\n",
      "|manufacturer_name|model_name| color|\n",
      "+-----------------+----------+------+\n",
      "|           Subaru|   Outback|silver|\n",
      "|           Subaru|   Outback|  blue|\n",
      "|           Subaru|  Forester|   red|\n",
      "|           Subaru|   Impreza|  blue|\n",
      "|           Subaru|    Legacy| black|\n",
      "+-----------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"manufacturer_name\"], df[\"model_name\"], df[\"color\"]).show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Или используя функцию колонки .col() из пакета functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------+\n",
      "|manufacturer_name|model_name| color|\n",
      "+-----------------+----------+------+\n",
      "|           Subaru|   Outback|silver|\n",
      "|           Subaru|   Outback|  blue|\n",
      "|           Subaru|  Forester|   red|\n",
      "|           Subaru|   Impreza|  blue|\n",
      "|           Subaru|    Legacy| black|\n",
      "+-----------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df.select(F.col(\"manufacturer_name\"), F.col(\"model_name\"), F.col(\"color\")).show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`.filter()`\n",
    "Метод `.filter()` принимает условия для фильтрации (аналог `SELECT <column_names> FROM <db.name> WHERE <column_name> <EXPRESSION>` в SQL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----+\n",
      "|manufacturer_name|model_name|color|\n",
      "+-----------------+----------+-----+\n",
      "|           Subaru|  Forester|  red|\n",
      "|           Subaru|     Justy|  red|\n",
      "|           Subaru|    Legacy|  red|\n",
      "|           Subaru|  Forester|  red|\n",
      "|           Subaru|    Legacy|  red|\n",
      "+-----------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# выберите только марки Lexus\n",
    "df.select(\"manufacturer_name\", \"model_name\", \"color\").filter(\"color = 'red'\").show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----+\n",
      "|manufacturer_name|model_name|color|\n",
      "+-----------------+----------+-----+\n",
      "|            Lexus|        ES|black|\n",
      "|            Lexus|        RX|black|\n",
      "|            Lexus|        RX|white|\n",
      "|            Lexus|        ES|black|\n",
      "|            Lexus|        RX|black|\n",
      "+-----------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"manufacturer_name\", \"model_name\", \"color\").filter('manufacturer_name = \"Lexus\"').show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Цепочка условий:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------------+\n",
      "|manufacturer_name|model_name|transmission|\n",
      "+-----------------+----------+------------+\n",
      "|             Audi|       100|  mechanical|\n",
      "|             Audi|A6 Allroad|  mechanical|\n",
      "|             Audi|       100|  mechanical|\n",
      "|             Audi|        A4|  mechanical|\n",
      "|             Audi|        80|  mechanical|\n",
      "+-----------------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .select(\"manufacturer_name\", \"model_name\", \"transmission\")\n",
    "    .filter(\"manufacturer_name = 'Audi'\")\n",
    "    .filter(\"transmission = 'mechanical'\")\n",
    "    .show(5)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Цепочка условий в виде одного SQL выражения:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------------+\n",
      "|manufacturer_name|model_name|transmission|\n",
      "+-----------------+----------+------------+\n",
      "|            Lexus|        ES|   automatic|\n",
      "|            Lexus|        RX|   automatic|\n",
      "|            Lexus|        RX|   automatic|\n",
      "|            Lexus|        ES|   automatic|\n",
      "|            Lexus|        RX|   automatic|\n",
      "+-----------------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .select(\"manufacturer_name\", \"model_name\", \"transmission\")\n",
    "    .filter(\"manufacturer_name = 'Audio' or manufacturer_name = 'Lexus'\")\n",
    "    .show(5)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Удобнее и логичнее использовать `col()` для составления условий фильтрации:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------------+\n",
      "|manufacturer_name|model_name|transmission|\n",
      "+-----------------+----------+------------+\n",
      "|             Audi|        Q7|   automatic|\n",
      "|             Audi|        TT|   automatic|\n",
      "|             Audi|        A6|   automatic|\n",
      "|             Audi|        Q3|   automatic|\n",
      "|             Audi|        Q5|   automatic|\n",
      "+-----------------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .select(\"manufacturer_name\", \"model_name\", \"transmission\")\n",
    "    .filter(F.col(\"manufacturer_name\") == \"Audi\")\n",
    "    .filter(F.col(\"transmission\") != 'mechanical')\n",
    "    .show(5)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`.count()`\n",
    "Подсчет кол-ва строк:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "38531"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Либо уникальных строк:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "55"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"manufacturer_name\").distinct().count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`.groupBy()` и `.orderBy()`\n",
    "Группировка и агрегация (аналог `GROUP BY` и `ORDER BY` в SQL):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|manufacturer_name|count|\n",
      "+-----------------+-----+\n",
      "|       Volkswagen| 4243|\n",
      "|            Lexus|  213|\n",
      "|           Jaguar|   53|\n",
      "|            Rover|  235|\n",
      "|           Lancia|   92|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# сгрупировать по manufacturer_name и посчитать кол-во каждого\n",
    "df.groupby(\"manufacturer_name\").count().show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сортировка по колонке по возрастанию:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|manufacturer_name|count|\n",
      "+-----------------+-----+\n",
      "|          Lincoln|   36|\n",
      "|       Great Wall|   36|\n",
      "|          Pontiac|   42|\n",
      "|              ЗАЗ|   42|\n",
      "|         Cadillac|   43|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"manufacturer_name\").count().orderBy('count').show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сортировка по колонке по убыванию. Важно явно задать колонку через col() иначе не получится:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|manufacturer_name|count|\n",
      "+-----------------+-----+\n",
      "|       Volkswagen| 4243|\n",
      "|             Opel| 2759|\n",
      "|              BMW| 2610|\n",
      "|             Ford| 2566|\n",
      "|          Renault| 2493|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"manufacturer_name\").count().orderBy(F.col(\"count\").desc()).show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`.withColumnRenamed()` и `.withColumn()`\n",
    "Переименовать существующую колонку:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|manufacturer|\n",
      "+------------+\n",
      "|      Subaru|\n",
      "|      Subaru|\n",
      "|      Subaru|\n",
      "|      Subaru|\n",
      "|      Subaru|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"manufacturer_name\", \"manufacturer\").select(\"manufacturer\").show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создать новую колонку. Первый аргумент это название новой колонки, второй агрумент это выражение (обязательно использовать col() если ссылаемся на другую колонку):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|year_produced|next_year|\n",
      "+-------------+---------+\n",
      "|         2010|   2011.0|\n",
      "|         2002|   2003.0|\n",
      "|         2001|   2002.0|\n",
      "|         1999|   2000.0|\n",
      "|         2001|   2002.0|\n",
      "+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"next_year\", F.col(\"year_produced\") + 1).select(\"year_produced\", \"next_year\").show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`.printSchema()` и `.describe()`\n",
    "Вывести схему датафрейма (типы колонок):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- manufacturer_name: string (nullable = true)\n",
      " |-- model_name: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- odometer_value: string (nullable = true)\n",
      " |-- year_produced: string (nullable = true)\n",
      " |-- engine_fuel: string (nullable = true)\n",
      " |-- engine_has_gas: string (nullable = true)\n",
      " |-- engine_type: string (nullable = true)\n",
      " |-- engine_capacity: string (nullable = true)\n",
      " |-- body_type: string (nullable = true)\n",
      " |-- has_warranty: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- drivetrain: string (nullable = true)\n",
      " |-- price_usd: string (nullable = true)\n",
      " |-- is_exchangeable: string (nullable = true)\n",
      " |-- location_region: string (nullable = true)\n",
      " |-- number_of_photos: string (nullable = true)\n",
      " |-- up_counter: string (nullable = true)\n",
      " |-- feature_0: string (nullable = true)\n",
      " |-- feature_1: string (nullable = true)\n",
      " |-- feature_2: string (nullable = true)\n",
      " |-- feature_3: string (nullable = true)\n",
      " |-- feature_4: string (nullable = true)\n",
      " |-- feature_5: string (nullable = true)\n",
      " |-- feature_6: string (nullable = true)\n",
      " |-- feature_7: string (nullable = true)\n",
      " |-- feature_8: string (nullable = true)\n",
      " |-- feature_9: string (nullable = true)\n",
      " |-- duration_listed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "StructType(List(StructField(manufacturer_name,StringType,true),StructField(model_name,StringType,true),StructField(transmission,StringType,true),StructField(color,StringType,true),StructField(odometer_value,StringType,true),StructField(year_produced,StringType,true),StructField(engine_fuel,StringType,true),StructField(engine_has_gas,StringType,true),StructField(engine_type,StringType,true),StructField(engine_capacity,StringType,true),StructField(body_type,StringType,true),StructField(has_warranty,StringType,true),StructField(state,StringType,true),StructField(drivetrain,StringType,true),StructField(price_usd,StringType,true),StructField(is_exchangeable,StringType,true),StructField(location_region,StringType,true),StructField(number_of_photos,StringType,true),StructField(up_counter,StringType,true),StructField(feature_0,StringType,true),StructField(feature_1,StringType,true),StructField(feature_2,StringType,true),StructField(feature_3,StringType,true),StructField(feature_4,StringType,true),StructField(feature_5,StringType,true),StructField(feature_6,StringType,true),StructField(feature_7,StringType,true),StructField(feature_8,StringType,true),StructField(feature_9,StringType,true),StructField(duration_listed,StringType,true)))"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вывести сводную статистику по датафрейму:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------+------------------+-----------------+\n",
      "|summary|manufacturer_name|        model_name|transmission|     year_produced|        price_usd|\n",
      "+-------+-----------------+------------------+------------+------------------+-----------------+\n",
      "|  count|            38531|             38531|       38531|             38531|            38531|\n",
      "|   mean|             null|1168.2918056562726|        null|2002.9437336170874|6639.971021255616|\n",
      "| stddev|             null| 9820.119520829547|        null| 8.065730511309367|6428.152018202914|\n",
      "|    min|            Acura|               100|   automatic|              1942|              1.0|\n",
      "|    max|              УАЗ|            Таврия|  mechanical|              2019|           9999.0|\n",
      "+-------+-----------------+------------------+------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"manufacturer_name\", \"model_name\", \"transmission\", \"year_produced\", \"price_usd\").describe().show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Остановите процесс Spark Session и закончите работу с сессией\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Ваша задача - реализовать пайплайн обработки файла `cars.csv`__\n",
    "\n",
    "Необходимо посчитать по каждому производителю (dimension - `manufacturer_name`):\n",
    "1. Количество объявлений (записей в csv)\n",
    "2. Средний год выпуска автомобилей (нелогично но закрепить можно)\n",
    "3. Минимальную цену автомобиля\n",
    "4. Максимальную цену автомобиля\n",
    "\n",
    "Выгрузить результат вычислений в `output.csv`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as t\n",
    "\n",
    "\n",
    "def extract_data(spark: SparkSession) -> DataFrame:\n",
    "    path = \"data/cars.csv\"\n",
    "    return spark.read.option(\"header\", \"true\").csv(path)\n",
    "\n",
    "\n",
    "def transform_data(df: DataFrame) -> DataFrame:\n",
    "    output = (\n",
    "        df\n",
    "        .groupby(\"manufacturer_name\")\n",
    "        .agg(\n",
    "            F.count(\"manufacturer_name\").alias(\"count_ads\"),\n",
    "            F.round(F.avg(\"year_produced\")).cast(t.IntegerType()).alias(\"avg_year_produced\"),\n",
    "            F.min(\"price_usd\").alias(\"min_price\"),\n",
    "            F.max(\"price_usd\").alias(\"max_price\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"count_ads\").desc())\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "def save_data(df: DataFrame) -> None:\n",
    "    df.coalesce(4).write.mode(\"overwrite\").format(\"json\").save(\"output.json\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"Practice\").getOrCreate()\n",
    "    df = extract_data(spark)\n",
    "    output = transform_data(df)\n",
    "    save_data(output)\n",
    "    spark.stop()\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Резюме и общие принципы Spark приложения:__\n",
    "1. Data is not mutable - исходные данные которые вы загружаете в датафрейм не меняются.\n",
    "2. Ленивые вычисления - что-то считается только тогда, когда запрашивается конечный результат.\n",
    "3. Spark распределенный - будьте аккуратны с чтением БД, API и следите за объемом потребляемых данных особенно если работаете в облаке.\n",
    "4. Всегда придерживайтесь функционального стиля (чистые и понятные функции по принципу in >> out)\n",
    "5. Batch Processing\n",
    "6. Spark устойчив к локальным ошибкам (не надо писать recovery code, достаточно работать с партишенами).\n",
    "7. PySpark может использовать __любой внешний пакет__ Python (только надо установить его на всех нодах либо отправить вместе с кодом - __использование не Spark API может сильно замедлить программу__)\n",
    "8. Используйте __SQL functions__ по максимуму это покажет ваш профессионализм в глазах ваших коллег и стейкхолдеров."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MLlib и LogisticRegression:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"mllib-logreg\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/bank.csv', header = True, inferSchema = True)\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Баланс классов"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby('deposit').count().toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "summary statistics для числовых переменных"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\n",
    "df.select(numeric_features).describe().toPandas().transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Корреляции"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numeric_data = df.select(numeric_features).toPandas()\n",
    "\n",
    "axs = pd.plotting.scatter_matrix(numeric_data, figsize=(8, 8));\n",
    "\n",
    "# Rotate axis labels and remove axis ticks\n",
    "n = len(numeric_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.select('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit')\n",
    "cols = df.columns\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Подготовка данных для модели:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df = pipelineModel.transform(df)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "df = df.select(selectedCols)\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logistic Regression Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "beta = np.sort(lrModel.coefficients)\n",
    "\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f = trainingSummary.fMeasureByThreshold.toPandas()\n",
    "plt.plot(f['threshold'],f['F-Measure'])\n",
    "plt.ylabel('F-Measure')\n",
    "plt.xlabel('Threshold')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluator.getMetricName()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Кросс Валидация"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "predictions = cvModel.transform(test)\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluator.getMetricName()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
