{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Часть 1.\n",
    "----"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### NLP и Трансформеры, знакомство с библиотекой transformers и datasets:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Работа с языковыми моделями и токенизаторами:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 2023, 2003, 2019, 2742, 1997, 2478, 14324, 10938, 2121, 19204, 17629, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "text = \"this is an example of using bert transformer tokenizer!\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer(text=text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "* _input_ids_: айдишник для каждого токена\n",
    "\n",
    "<br>\n",
    "\n",
    "* _token_type_ids_: тип токена который разделяет первую и последующие **N** последовательностей в корпусе данных\n",
    "\n",
    "<br>\n",
    "\n",
    "* _attention_mask_: маска внимания  - это маска (матрица) из 0 и 1, которая используется для того чтобы определить начало и конец последовательности для модели трансформера, для того чтобы избавить нас от ненужных вычислений. Каждый токенизатор имеет свой собственный способ добавления токенов в первоначальную последовательность. В случае BERT токенизатора, то он просто добавляет **[CLS]** токен в начало и **[SEP]** в конец последовательности (101 и 102 соответственно). Эти числа приходят из айдишников токенов уже натренированного токенизатора.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Экземпляры токенизаторов, могут возвращать вывод в виде pt формата для использования в дальнейшем данных вывода в PyTorch'e либо tf для использования в TensorFlow. Для этого используется аргумент _return_tensors = \"pt\" (PyTorch tensor) и tf (TensorFlow tensor):_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  2003,  2019,  2742,  1997,  2478, 14324, 10938,  2121,\n",
      "         19204, 17629,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(text, return_tensors=\"pt\") # tf для TensorFlow\n",
    "print(encoded_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Как загружать и использовать модели при помощи Transformers API для целей решения своих задач:_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# https://huggingface.co/models\n",
    "# https://huggingface.co/docs/transformers/main_classes/model\n",
    "from transformers import BertModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Загрузив себе предобученный трансформер BERT, мы можем его использовать дальше, например подать в него закодированную последовательность, которую получили выше. Такая операция выдаст нам вывод модели в форме **эмбеддингов** и **кросс-аттеншена:**__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "output = model(**encoded_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1012, -0.2307, -0.0572,  ..., -0.2870, -0.0342,  0.9602],\n         [-0.4558, -0.3543, -0.3250,  ..., -0.5265,  1.0234,  0.3501],\n         [-0.4322, -0.4439, -0.0591,  ..., -0.2053,  0.4723,  0.9452],\n         ...,\n         [-0.3190, -0.6603,  0.0800,  ..., -0.3020, -0.4188,  0.1921],\n         [-0.2751, -0.7012, -0.3172,  ...,  0.4856, -0.3296,  0.3901],\n         [ 0.5882,  0.1226, -0.6750,  ...,  0.2499, -0.7004, -0.0285]]],\n       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7024, -0.1973,  0.0364,  0.1771,  0.1898, -0.0700,  0.4510,  0.0870,\n          0.1515, -0.9997,  0.1503, -0.2038,  0.9657, -0.2926,  0.7801, -0.3612,\n          0.3516, -0.3518,  0.2063, -0.0299,  0.3595,  0.9967,  0.6053,  0.1497,\n          0.1918, -0.2560, -0.3666,  0.8353,  0.9022,  0.7594, -0.3826,  0.0130,\n         -0.9807, -0.2227, -0.5447, -0.9671,  0.1754, -0.6301, -0.0027,  0.0153,\n         -0.8641,  0.2278,  0.9990, -0.5291,  0.2945, -0.2405, -0.9983,  0.1768,\n         -0.7851, -0.3782,  0.0585, -0.1515,  0.0538,  0.2010,  0.3308,  0.0687,\n         -0.1971,  0.1352, -0.2175, -0.3982, -0.4906,  0.1553,  0.0117, -0.8205,\n         -0.0366, -0.0922,  0.0419, -0.2172,  0.0143, -0.0137,  0.5207,  0.1947,\n          0.2563, -0.6728, -0.3223,  0.2577, -0.5204,  1.0000,  0.1419, -0.9519,\n          0.1097, -0.2082,  0.4516,  0.5688, -0.1314, -0.9999,  0.2056, -0.1085,\n         -0.9756,  0.0874,  0.3755, -0.0853,  0.2775,  0.3968, -0.1539, -0.1727,\n         -0.0415, -0.3691, -0.0803, -0.1506,  0.0723, -0.1626, -0.0489, -0.2199,\n          0.2399, -0.0599,  0.0423,  0.3717, -0.3983,  0.5334,  0.4131, -0.2331,\n          0.2291, -0.8664,  0.3799, -0.2668, -0.9756, -0.4308, -0.9730,  0.5825,\n         -0.0311, -0.2828,  0.8100,  0.3231,  0.1275,  0.0577,  0.3492, -1.0000,\n          0.1430, -0.5029,  0.3330, -0.0328, -0.9555, -0.9299,  0.5485,  0.9259,\n          0.1307,  0.9975,  0.0772,  0.8904,  0.5214, -0.1352, -0.5043, -0.1667,\n          0.5944, -0.2073, -0.4655,  0.0906,  0.0273, -0.4824, -0.1551, -0.1209,\n         -0.0611, -0.8447, -0.2106,  0.8786,  0.1764,  0.1465,  0.6072,  0.1860,\n         -0.1758,  0.7563,  0.3649,  0.0862,  0.2332,  0.2037, -0.3343,  0.3107,\n         -0.7916,  0.6503,  0.2270, -0.1228, -0.1811, -0.9567, -0.1667,  0.2676,\n          0.9664,  0.5028,  0.2578, -0.2395, -0.1614, -0.2235, -0.8913,  0.9575,\n         -0.0701,  0.0543, -0.5156,  0.2216, -0.8118, -0.1265,  0.4983,  0.2825,\n         -0.8104,  0.1127, -0.4750, -0.2220, -0.3582,  0.4026, -0.2368, -0.2669,\n          0.0898,  0.8936,  0.7761,  0.5087, -0.3403,  0.4174, -0.6089, -0.0581,\n         -0.1097,  0.1924,  0.0814,  0.9750, -0.5618,  0.0048, -0.7876, -0.9466,\n         -0.1357, -0.6825,  0.1304, -0.6530,  0.3198,  0.0870, -0.5096,  0.0730,\n         -0.5013, -0.6413,  0.1505, -0.3337,  0.4014, -0.2057,  0.9097,  0.2533,\n         -0.5470,  0.0275,  0.9542, -0.1842, -0.7738,  0.3357, -0.1066,  0.6945,\n         -0.3358,  0.9641,  0.0953,  0.0080, -0.7591, -0.0728, -0.6942,  0.3881,\n          0.0019, -0.5422, -0.1979,  0.5440,  0.1404,  0.5146, -0.1280,  0.7417,\n         -0.9351, -0.9174, -0.7294,  0.3862, -0.9832,  0.1575,  0.1910,  0.2729,\n         -0.3957, -0.2449, -0.9404,  0.5835, -0.0357,  0.8795, -0.3243, -0.5529,\n         -0.3028, -0.9038, -0.1781, -0.0986,  0.6171, -0.0635, -0.8665,  0.4084,\n          0.5270,  0.1003,  0.1000,  0.8965,  0.9998,  0.9539,  0.7690,  0.3959,\n         -0.9870, -0.7125,  0.9998, -0.6224, -0.9999, -0.8819, -0.2054,  0.2771,\n         -1.0000, -0.1411,  0.0712, -0.8446, -0.2663,  0.9562,  0.6877, -1.0000,\n          0.8324,  0.7914, -0.5164, -0.2083, -0.2707,  0.9388,  0.4945,  0.4269,\n         -0.0726,  0.2761, -0.5108, -0.6568,  0.1691, -0.1841,  0.9353,  0.0126,\n         -0.5988, -0.6882,  0.1967,  0.1001, -0.3196, -0.9065, -0.2545, -0.5567,\n          0.4861,  0.1087,  0.1747, -0.4838,  0.1617, -0.1389,  0.0475,  0.4604,\n         -0.8523, -0.1943,  0.0434, -0.5217,  0.1858, -0.9584,  0.9077, -0.0897,\n         -0.3580,  1.0000,  0.4709, -0.6453,  0.2418,  0.0154, -0.4651,  1.0000,\n          0.3449, -0.9672, -0.4021,  0.1962, -0.2799, -0.2286,  0.9951, -0.0885,\n          0.1338,  0.2327,  0.9705, -0.9817,  0.8781, -0.5284, -0.9156,  0.8773,\n          0.8700, -0.1660, -0.7255, -0.1258, -0.1434,  0.2309, -0.7814,  0.0881,\n          0.2439,  0.0373,  0.7998, -0.2056, -0.4240,  0.1880,  0.0370,  0.4771,\n          0.4304,  0.3021, -0.2694, -0.0865, -0.1487, -0.6138, -0.8944,  0.1333,\n          1.0000,  0.2813,  0.1720,  0.3631, -0.1020, -0.2340,  0.3101,  0.3747,\n         -0.0838, -0.7532,  0.0714, -0.5772, -0.9824,  0.3347,  0.0947, -0.1330,\n          0.9898,  0.2735,  0.0862, -0.0816,  0.2030,  0.1110,  0.2150, -0.2257,\n          0.9657, -0.2318,  0.4068,  0.3999,  0.2121, -0.2007, -0.5056, -0.0402,\n         -0.9349,  0.0460, -0.9052,  0.9377, -0.1213,  0.2543,  0.1354,  0.3202,\n          1.0000, -0.8176,  0.3323,  0.3930,  0.2292, -0.9835, -0.2412, -0.1303,\n         -0.0634,  0.3192, -0.1151,  0.1334, -0.9482, -0.1788, -0.1969, -0.7498,\n         -0.9603,  0.3031,  0.1632, -0.0996, -0.7642, -0.0654, -0.4716, -0.2268,\n         -0.1554, -0.7780,  0.6568, -0.1889,  0.1938, -0.2501,  0.5277,  0.0463,\n          0.8303, -0.5828,  0.2042,  0.0272, -0.6203,  0.4550, -0.6402,  0.2393,\n         -0.1344,  1.0000, -0.2558,  0.4058,  0.5545,  0.3781, -0.1163,  0.1321,\n          0.5498,  0.1222,  0.2464,  0.0755,  0.1842, -0.1582,  0.4288,  0.2008,\n         -0.0575,  0.5820,  0.5536,  0.0145, -0.0232, -0.1716,  0.8523, -0.0500,\n          0.0791, -0.3260, -0.0810, -0.0281,  0.1961,  1.0000,  0.2272,  0.2084,\n         -0.9759,  0.0246, -0.6930,  0.9996,  0.7487, -0.7085,  0.3386,  0.0498,\n         -0.0775,  0.3608, -0.1768, -0.2508, -0.0062,  0.0945,  0.8877, -0.3305,\n         -0.9573, -0.6357,  0.1925, -0.9290,  0.9890, -0.3801, -0.1085, -0.3041,\n          0.3957, -0.5812, -0.1786, -0.9384,  0.0387,  0.1378,  0.9290,  0.2534,\n         -0.3994, -0.8105, -0.0021,  0.0782,  0.2218, -0.8488,  0.9349, -0.8870,\n          0.0804,  0.9998,  0.4552, -0.6174, -0.0330, -0.2663,  0.0806, -0.3508,\n          0.3343, -0.8793, -0.1807, -0.0932,  0.1225, -0.0518, -0.2916,  0.2999,\n          0.1706, -0.3748, -0.2648,  0.0131,  0.2047,  0.4023, -0.0462, -0.0990,\n         -0.0647, -0.0222, -0.5915, -0.1650, -0.1653, -0.9976,  0.4641, -1.0000,\n         -0.0374, -0.5600, -0.1045,  0.7481,  0.5047,  0.0868, -0.5156,  0.2690,\n          0.8176,  0.4770, -0.0987,  0.6783, -0.4957, -0.0075,  0.0844,  0.0032,\n          0.2842,  0.7386, -0.0030,  1.0000, -0.0216, -0.3357, -0.6269,  0.1720,\n         -0.1862,  0.9999, -0.5389, -0.9128,  0.0396, -0.3451, -0.7524,  0.1624,\n          0.0573, -0.3335, -0.2395,  0.7185,  0.5712, -0.3848,  0.1751, -0.1285,\n         -0.1560, -0.0862, -0.0491,  0.9725,  0.4247,  0.7361,  0.0540, -0.1342,\n          0.9345,  0.2663,  0.1036, -0.0598,  0.9999,  0.1646, -0.8857,  0.3423,\n         -0.9299, -0.0943, -0.8722,  0.0173,  0.0309,  0.8418, -0.2437,  0.8641,\n          0.2454, -0.0824,  0.3302,  0.3930,  0.0835, -0.8476, -0.9724, -0.9718,\n          0.2523, -0.3605, -0.0528,  0.1114, -0.0341,  0.1974,  0.0931, -1.0000,\n          0.8897,  0.2971,  0.0191,  0.9010,  0.1514,  0.4102,  0.1813, -0.9652,\n         -0.4677, -0.2030, -0.2177,  0.5354,  0.3885,  0.7621,  0.0659, -0.4215,\n         -0.4019,  0.2834, -0.8207, -0.9798,  0.1645,  0.5144, -0.6042,  0.9437,\n         -0.2945, -0.1339,  0.5704, -0.3323,  0.3933,  0.6252,  0.2399,  0.0290,\n          0.5652,  0.7324,  0.8401,  0.9685,  0.0551,  0.5526,  0.4613,  0.4601,\n          0.8860, -0.9367,  0.1982,  0.1753,  0.0496,  0.0814, -0.1289, -0.5420,\n          0.8082, -0.1901,  0.2587, -0.2927,  0.1762, -0.2963, -0.0660, -0.7310,\n          0.0691,  0.4173, -0.0541,  0.8572,  0.5259,  0.0398, -0.0744, -0.0123,\n          0.3220, -0.9041,  0.4786,  0.0849,  0.3729,  0.4576, -0.2147,  0.9020,\n         -0.2257, -0.2468, -0.2338, -0.6877,  0.4780, -0.3935, -0.4333, -0.3440,\n          0.2975,  0.2377,  0.9977,  0.1269,  0.3070, -0.3405, -0.0772,  0.2734,\n         -0.2148, -1.0000,  0.2030,  0.4157,  0.0386, -0.0642,  0.0213,  0.4370,\n         -0.8584, -0.1225,  0.6366,  0.1969, -0.3130,  0.0666,  0.3503,  0.7980,\n          0.1509,  0.6733,  0.1948,  0.5500,  0.4587, -0.7377, -0.4976,  0.7972]],\n       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Для узконаправленных задач таких как заполнение пустого текста при помощи обученных масок используя языковые модели, придумали пайплайнинг, который можно использовать прямо из коробки. Например задача заполненния маски:_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 19:26:08.831390: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'score': 0.673102855682373,\n  'token': 10639,\n  'token_str': 'communicate',\n  'sequence': 'the natural language processing is a way of using language models in order to communicate.'},\n {'score': 0.06801625341176987,\n  'token': 3305,\n  'token_str': 'understand',\n  'sequence': 'the natural language processing is a way of using language models in order to understand.'},\n {'score': 0.04671889916062355,\n  'token': 4553,\n  'token_str': 'learn',\n  'sequence': 'the natural language processing is a way of using language models in order to learn.'},\n {'score': 0.02921394817531109,\n  'token': 3853,\n  'token_str': 'function',\n  'sequence': 'the natural language processing is a way of using language models in order to function.'},\n {'score': 0.015427297912538052,\n  'token': 11835,\n  'token_str': 'interact',\n  'sequence': 'the natural language processing is a way of using language models in order to interact.'}]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"The Natural Language Processing is a way of using language models in order to [MASK].\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Для целей, например, последующего анализа текста и мы можем весь вывод из пайплайна обернуть в датафрейм:_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "      score  token    token_str  \\\n0  0.673103  10639  communicate   \n1  0.068016   3305   understand   \n2  0.046719   4553        learn   \n3  0.029214   3853     function   \n4  0.015427  11835     interact   \n\n                                            sequence  \n0  the natural language processing is a way of us...  \n1  the natural language processing is a way of us...  \n2  the natural language processing is a way of us...  \n3  the natural language processing is a way of us...  \n4  the natural language processing is a way of us...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n      <th>token</th>\n      <th>token_str</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.673103</td>\n      <td>10639</td>\n      <td>communicate</td>\n      <td>the natural language processing is a way of us...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.068016</td>\n      <td>3305</td>\n      <td>understand</td>\n      <td>the natural language processing is a way of us...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.046719</td>\n      <td>4553</td>\n      <td>learn</td>\n      <td>the natural language processing is a way of us...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.029214</td>\n      <td>3853</td>\n      <td>function</td>\n      <td>the natural language processing is a way of us...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.015427</td>\n      <td>11835</td>\n      <td>interact</td>\n      <td>the natural language processing is a way of us...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(unmasker(\"The Natural Language Processing is a way of using language models in order to [MASK].\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Если существует возможность воспользоваться пайплайном, который решает уже поставленную задачу - то легче использовать его, так как весь пайплайн (от предобработки до подачи данных) осуществляется за счет уже написанных методов, которые лежат в API Transformers. Допустим нам необходим [zero-shot классификатор](https://en.wikipedia.org/wiki/Zero-shot_learning), мы можем достать его прямо из коробки и использовать преобученную модель уже для своей задачи, не обучая ничего с нуля:_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# https://huggingface.co/google/tapas-base-finetuned-wtq\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Попробуем классифицировать предложение на предмет вероятной тематики исходя из контекста:_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{'sequence': 'I am going to Belarus in 2023 ',\n 'labels': ['fight', 'dance', 'war'],\n 'scores': [0.6005637049674988, 0.23785468935966492, 0.1615816056728363]}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"I am going to Belarus in 2023 \"\n",
    "candidate_labels = ['dance', 'fight', 'war']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Поговорим немного про метрики и бенчмаркинг моделей.\n",
    "___\n",
    "Существует несколько контрольных показателей для оценки инференса моделей трансформеров (и не только). Benchmarking очень важен для задач, связанных с дообучением в контексте решения мультизадач или использования мультиязыковых моделей в NLP. До этого мы в основном фокусировались на одной единственной метрике качества, которая показывала насколько эффективно и качественно модель работает на этапе инференса. Благодаря API Transformers мы можем дообучать модели решать определенную задачу в контексте дообучения. Но проверка в этот раз будет идти по одному из представленных ниже бенчмарков, так же на эти бенчмарки стоит обращать внимания при поиске и выборе обученной крупной модели, так как в контексте вашей задачи показатели могут быть ниже ожидаемых."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **_Наиболее важные сравнительные бенчмарки:_**\n",
    "----\n",
    "* ##### GLUE benchmark\n",
    "* ##### SuperGLUE\n",
    "* ##### XTREME\n",
    "* ##### XGLUE\n",
    "* ##### SQuAD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [Glue Benchmark:](https://gluebenchmark.com/)\n",
    "----\n",
    "Тест GLUE, представляет собой набор инструментов и данных для оценки производительности моделей Multi-Task-Learning (MTL) и трансформеров по конкретному списку задач. Существует лидерборд для мониторинга производительности различных архитектур моделей нейронных сетей. Всего 11 задач в данном бенчмарке. Финальная метрика по любой архитектуре усредняется. Этот бенчмарк влключает в себя 11 задач на контекстное понимание последовательностей:\n",
    "\n",
    "* Задача __Single-Sentence task__.\n",
    "* __CoLA:__ (The Corpus of Linguistic Acceptability dataset) - проверка понимания в соответствии с датасетом, составленным из статей по лингвистической теории.\n",
    "* __SST-2:__ The Stanford Sentiment Treebank dataset. Задача pos/neg классификации на датасете по отзывам фильмов.\n",
    "* __Датасет и задача по поиску похожих N-грам__ и задачи перефразирования (paraphrase).\n",
    "* __MRPC (The Microsoft Research Paraphrase Corpus dataset)__ - задача на эквивалентную семантику.\n",
    "* __QQP (The Quora Question Pairs dataset)__ - задача QA по поиску эквивалентной семантики.\n",
    "* __STS-B (The Semantic Textual Similarity Benchmark dataset)__ - коллекция датасетов из пар QA по поиску похожих последовательностей.\n",
    "* __Мини-задачи на проверку инференса моделей.__\n",
    "* __MNLI (The Multi-Genre Natural Language Inference corpus)__ - коллекция последовательностей, цель задачи - предсказать, влечет ли текст гипотезу (entailment), противоречит гипотезе (contradiction) или ни то, ни другое (neutral).\n",
    "* __QNLI (Question Natural Language Inference dataset)__ - QA задача и датасет.\n",
    "* __RTE (The Recognizing Textual Entailment dataset)__ - QA задача и датасет.\n",
    "* __WNLI (The Winograd Natural Language Inference schema challenge).__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### __[SuperGLUE benchmark](https://gluebenchmark.com/)__\n",
    "Как и Glue, SuperGLUE — это новый бенчмарк, созданный с новым набором более сложных задач и данных на проверку понимания языка и предлагающий общедоступный лидерборд по архитектурам моделей. Состоит из 8 языковых задач, метрика также обобщается и приводится к средней.\n",
    "____\n",
    "##### __[XTREME benchmark](https://sites.research.google/xtreme)__\n",
    "General-Purpose mutlilinguistic benchmark. Для мультиязычных и многозадачных моделей.\n",
    "____\n",
    "##### __[XGLUE benchmark](https://microsoft.github.io/XGLUE/)__\n",
    "XGLUE — это еще один межъязыковой эталонный тест для оценки и улучшения производительности кросс-языковых предварительно обученных моделей для понимания естественного языка (NLU) и генерации естественного языка (NLG). Первоначально он состоял из 11 заданий на 19 языках. Основное отличие от XTREME заключается в том, что для каждой задачи тренировочные данные доступны только на английском языке.\n",
    "____\n",
    "\n",
    "##### __[SQuAD benchmark](https://rajpurkar.github.io/SQuAD-explorer/)__\n",
    "SQuAD — это широко используемый набор данных QA."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Прежде чем пойти дальше по основам, нам необходимо рассмотреть такую библиотеку как [**_datasets_**](https://huggingface.co/docs/datasets/index).\n",
    "----\n",
    "```\n",
    "pip install datasets\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Библиотека datasets является частью экосистемы Transformers API. Библиотека поставляет огромный блок полезных утилит для загрузки, обработки и обмена датасетами через Hugging Face hub. Также библиотека поставляет огромное количество метрик для проверки качества моделей. Стоит иметь в виду, если вы используете публичный датасет для целей вашей задачи, стоит проверить его на предмет доступности, так как информация быстро обновляется и старые данные теряют актуальность и удаляются с репозитория."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e272d04703a241c8ba8bbf7d39802a7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'sentence': ['We yelled ourselves hoarse.'], 'label': [1], 'idx': [21]}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "cola = load_dataset('glue', 'cola')\n",
    "cola['train'][21:22]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10256 datasets and 76 metrics exist in the hub\n",
      "\n",
      "['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc',\n",
      " 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue',\n",
      " 'ajgt_twitter_ar', 'allegro_reviews', 'allocine', 'alt', 'amazon_polarity',\n",
      " 'amazon_reviews_multi', 'amazon_us_reviews', 'ambig_qa', 'americas_nli', 'ami',\n",
      " 'amttl', 'anli']\n",
      "['accuracy', 'bertscore', 'bleu', 'bleurt', 'brier_score', 'cer', 'chrf',\n",
      " 'code_eval', 'comet', 'competition_math', 'coval', 'cuad', 'exact_match', 'f1',\n",
      " 'frugalscore', 'glue', 'google_bleu', 'indic_glue', 'mae', 'mahalanobis',\n",
      " 'matthews_correlation', 'mauve', 'mean_iou', 'meteor', 'mse', 'pearsonr',\n",
      " 'perplexity', 'poseval', 'precision', 'recall', 'rl_reliability', 'roc_auc',\n",
      " 'rouge', 'sacrebleu', 'sari', 'seqeval', 'spearmanr', 'squad', 'squad_v2',\n",
      " 'super_glue', 'ter', 'trec_eval', 'wer', 'wiki_split', 'xnli', 'xtreme_s',\n",
      " 'GMFTBY/dailydialog_evaluate', 'GMFTBY/dailydialogevaluate',\n",
      " 'Vertaix/vendiscore', 'Vlasta/pr_auc', 'abdusahmbzuai/aradiawer',\n",
      " 'angelina-wang/directional_bias_amplification', 'cakiki/ndcg',\n",
      " 'codeparrot/apps_metric', 'cpllab/syntaxgym', 'daiyizheng/valid',\n",
      " 'erntkn/dice_coefficient', 'gorkaartola/metric_for_tp_fp_samples',\n",
      " 'hack/test_metric', 'idsedykh/codebleu', 'idsedykh/codebleu2',\n",
      " 'idsedykh/megaglue', 'idsedykh/metric', 'jordyvl/ece',\n",
      " 'jzm-mailchimp/joshs_second_test_metric', 'kaggle/ai4code', 'kaggle/amex',\n",
      " 'kasmith/woodscore', 'loubnabnl/apps_metric2', 'lvwerra/bary_score',\n",
      " 'lvwerra/test', 'mathemakitten/harness_sentiment', 'mathemakitten/sentiment',\n",
      " 'mfumanelli/geometric_mean', 'mgfrantz/roc_auc_macro', 'yzha/ctc_eval']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from datasets import list_datasets, list_metrics\n",
    "\n",
    "all_d = list_datasets()\n",
    "metrics = list_metrics()\n",
    "\n",
    "print(f\"{len(all_d)} datasets and {len(metrics)} metrics exist in the hub\\n\")\n",
    "\n",
    "pprint(all_d[:20], compact=True)\n",
    "pprint(metrics, compact=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 8551\n    })\n    validation: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 1043\n    })\n    test: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 1063\n    })\n})"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "{'sentence': 'Bill rolled out of the room.', 'label': 1, 'idx': 12}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola['train'][12]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1# GLUE, the General Language Understanding Evaluation benchmark\n",
      "(https://gluebenchmark.com/) is a collection of resources for training,\n",
      "evaluating, and analyzing natural language understanding systems.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"1#\",cola[\"train\"].description)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2# @article{warstadt2018neural,\n",
      "  title={Neural Network Acceptability Judgments},\n",
      "  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},\n",
      "  journal={arXiv preprint arXiv:1805.12471},\n",
      "  year={2018}\n",
      "}\n",
      "@inproceedings{wang2019glue,\n",
      "  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
      "  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
      "  note={In the Proceedings of ICLR.},\n",
      "  year={2019}\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"2#\",cola[\"train\"].citation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3# https://nyu-mll.github.io/CoLA/\n"
     ]
    }
   ],
   "source": [
    "print(\"3#\",cola[\"train\"].homepage)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Как было описано выше GLUE бенчмарк - это набор датасетов для проверки решения на 11-ти задачах. Давайте загрузим датасет для MRPC задачи:_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/maratmovlamov/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a00ef98ccdb43bfade12c1f0af72e54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mrpc = load_dataset('glue', 'mrpc')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_XTREME (работа с набором данных на разных языках) — еще один популярный набор данных, который мы уже обсуждали. Давайте возьмем и загрузим MLQA из набора XTREME. MLQA.en.de — набора данных для проверки качества на английском и немецком языках, который можно загрузить следующим образом:_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xtreme (/Users/maratmovlamov/.cache/huggingface/datasets/xtreme/MLQA.en.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c38aa1351cb149bb9ccb5374d8056465"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_de = load_dataset('xtreme', 'MLQA.en.de')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    test: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 4517\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 512\n    })\n})"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_de"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "                                         id            title  \\\n0  037e8929e7e4d2f949ffbabd10f0f860499ff7c9     Cell culture   \n1  4b36724f3cbde7c287bde512ff09194cbba7f932     Cell culture   \n2  13e58403df16d88b0e2c665953e89575704942d4  TRIPS Agreement   \n3  d23b5372af1de9425a4ae313c01eb80764c910d8  TRIPS Agreement   \n\n                                             context  \\\n0  An established or immortalized cell line has a...   \n1  The 19th-century English physiologist Sydney R...   \n2  After the Uruguay round, the GATT became the b...   \n3  Since TRIPS came into force, it has been subje...   \n\n                                            question  \\\n0                          Woraus besteht die Linie?   \n1  Wann hat Roux etwas von seiner Medullarplatte ...   \n2  Was muss ratifiziert werden, wenn ein Land ger...   \n3  Welche Teile der Welt kritisierten das TRIPS a...   \n\n                                             answers  \n0           {'answer_start': [31], 'text': ['cell']}  \n1          {'answer_start': [232], 'text': ['1885']}  \n2         {'answer_start': [131], 'text': ['TRIPS']}  \n3  {'answer_start': [67], 'text': ['developing co...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>context</th>\n      <th>question</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>037e8929e7e4d2f949ffbabd10f0f860499ff7c9</td>\n      <td>Cell culture</td>\n      <td>An established or immortalized cell line has a...</td>\n      <td>Woraus besteht die Linie?</td>\n      <td>{'answer_start': [31], 'text': ['cell']}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4b36724f3cbde7c287bde512ff09194cbba7f932</td>\n      <td>Cell culture</td>\n      <td>The 19th-century English physiologist Sydney R...</td>\n      <td>Wann hat Roux etwas von seiner Medullarplatte ...</td>\n      <td>{'answer_start': [232], 'text': ['1885']}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13e58403df16d88b0e2c665953e89575704942d4</td>\n      <td>TRIPS Agreement</td>\n      <td>After the Uruguay round, the GATT became the b...</td>\n      <td>Was muss ratifiziert werden, wenn ein Land ger...</td>\n      <td>{'answer_start': [131], 'text': ['TRIPS']}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>d23b5372af1de9425a4ae313c01eb80764c910d8</td>\n      <td>TRIPS Agreement</td>\n      <td>Since TRIPS came into force, it has been subje...</td>\n      <td>Welche Teile der Welt kritisierten das TRIPS a...</td>\n      <td>{'answer_start': [67], 'text': ['developing co...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(en_de['test'][0:4])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Manipulation с использованием библиотеки datasets:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Датасеты поставляются со множеством нарезок (splits), аргумент split позволяет определить какую часть датасета нам хочется загрузить для работы. В случае если аргумент отсутствует, то вернется датасет - train, test, validation, inference и их комбинаций, то есть всё:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "cola_train = load_dataset('glue', 'cola', split ='train')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Можно указать на какие части разбить датасет и сколько их должно быть при помощи следующего синтаксиса:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "cola_sel = load_dataset('glue', 'cola', split = 'train[:300]+validation[-30:]')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n 'label': 1,\n 'idx': 0}"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_sel[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Все загруженные датасеты и модели кешируются локально у вас на устройстве, поэтому внимательно следите, чтобы у вас хватало памяти, так как бывают модели и датасеты, которые весият пару десяток гб:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'filename': '/Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue-train.arrow'},\n {'filename': '/Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue-validation.arrow'}]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_sel.cache_files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Slicing:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "split='train[:100]+validation[:100]'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "cola_sel = load_dataset('glue', 'cola', split = split)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sentence', 'label', 'idx'],\n    num_rows: 200\n})"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_sel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "# 50% of train and the last 30% of validation\n",
    "split='train[:50%]+validation[-30%:]'\n",
    "cola_sel = load_dataset('glue', 'cola', split = split)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sorting, indexing, и shuffling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached sorted indices for dataset at /Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-aab7c26ed0bcc7fa.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_sel.sort('label')['label'][:15]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached sorted indices for dataset at /Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-aab7c26ed0bcc7fa.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_sel.sort('label')['label'][-15:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "{'sentence': ['Fred watered the plants flat.',\n  'The professor talked us into a stupor.',\n  'The trolley rumbled through the tunnel.'],\n 'label': [1, 1, 1],\n 'idx': [6, 19, 44]}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_sel[6,19,44]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-35d5d0d68d77c55f.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'sentence': ['We listened to as little speech as possible.',\n  'John not leave.',\n  'Which picture of himself does Mary think that John said that Susan likes?'],\n 'label': [0, 1, 1],\n 'idx': [4169, 3569, 367]}"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_sel.shuffle(seed=42)[2:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Применение filter() и map() функций:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "cola_sel = load_dataset('glue', 'cola', split='train[:100%]+validation[-30%:]')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-fb43a0247d07beca.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Jill kicked the ball from home plate to third base.',\n 'Fred kicked the ball under the porch.',\n 'Fred kicked the ball behind the tree.']"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_sel.filter(lambda s: \"kick\" in s['sentence'])[\"sentence\"][:3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6d56199747a2d80f.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "[\"Our friends won't buy this analysis, let alone the next one we propose.\",\n \"One more pseudo generalization and I'm giving up.\",\n \"One more pseudo generalization or I'm giving up.\"]"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_sel.filter(lambda s: s['label']== 1)[\"sentence\"][:3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-60a687fadcaa5111.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "[\"Our friends won't buy this analysis, let alone the next one we propose.\",\n \"One more pseudo generalization and I'm giving up.\",\n \"One more pseudo generalization or I'm giving up.\"]"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_sel.filter(lambda s: s['label']== cola_sel.features['label'].str2int('acceptable'))[\"sentence\"][:3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/maratmovlamov/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-50cbe75ae3f27a0e.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                            sentence  label  idx  len\n0  Our friends won't buy this analysis, let alone...      1    0   71\n1  One more pseudo generalization and I'm giving up.      1    1   49\n2   One more pseudo generalization or I'm giving up.      1    2   48",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>label</th>\n      <th>idx</th>\n      <th>len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Our friends won't buy this analysis, let alone...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>One more pseudo generalization and I'm giving up.</td>\n      <td>1</td>\n      <td>1</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>One more pseudo generalization or I'm giving up.</td>\n      <td>1</td>\n      <td>2</td>\n      <td>48</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_new=cola_sel.map(lambda e:{'len': len(e['sentence'])})\n",
    "pd.DataFrame(cola_new[0:3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загрузка локальных файлов (csv, tsv, json, txt и т.д.):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-edec09eebd84f90e\n",
      "Found cached dataset csv (/Users/maratmovlamov/.cache/huggingface/datasets/csv/default-edec09eebd84f90e/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b94cc72f3f3426c82d90d358e2b0b42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e06e68d6771ad1b0\n",
      "Found cached dataset csv (/Users/maratmovlamov/.cache/huggingface/datasets/csv/default-e06e68d6771ad1b0/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b0b540d81814e88b4352df7947a8593"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0e1bf83aa8a8fc9e\n",
      "Found cached dataset csv (/Users/maratmovlamov/.cache/huggingface/datasets/csv/default-0e1bf83aa8a8fc9e/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce4db327445c445086f1e279096521a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data1 = load_dataset('csv', data_files='./../data/data_for_transformers/a.csv', delimiter=\"\\t\")\n",
    "\n",
    "data2 = load_dataset('csv', data_files=['./../data/data_for_transformers/a.csv','./../data/data_for_transformers/b.csv', './../data/data_for_transformers/c.csv'], delimiter=\"\\t\")\n",
    "\n",
    "data3 = load_dataset('csv', data_files={'train':['./../data/data_for_transformers/a.csv','./../data/data_for_transformers/b.csv'], 'test':['./../data/data_for_transformers/c.csv']}, delimiter=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_json = load_dataset('json', data_files='a.json')\n",
    "data_text = load_dataset('text', data_files='a.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Подготовка датасета для работы с моделью трансформеров либо любой другой архитектурой (библиотека не ограничивается функциональностью предназначенной только для 1 архитектуры, можете использовать ее везде, где вам покажется это удобным):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Каждая модель имеет свой собственный способ токенизации. Чтобы, например, воспользоваться токенизатором, мы должны загрузить его из предобученой модели, например _distilBERT-base-uncased model_. Затем при помощи вышеизученного функционала мы можем провести токенизацию и разделить наш датасет на выборки:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/maratmovlamov/.cache/huggingface/datasets/csv/default-0e1bf83aa8a8fc9e/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-0095c18911536749.arrow\n",
      "Loading cached processed dataset at /Users/maratmovlamov/.cache/huggingface/datasets/csv/default-0e1bf83aa8a8fc9e/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-da814cdb02354ddc.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "encoded_data3 = data3.map(lambda e: tokenizer( e['sentence'], padding=True, truncation=True, max_length=12), batched=True, batch_size=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label'],\n        num_rows: 199\n    })\n    test: Dataset({\n        features: ['sentence', 'label'],\n        num_rows: 100\n    })\n})"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      " 'input_ids': [101, 2019, 5186, 16010, 2143, 1012, 102, 0, 0, 0, 0, 0],\n",
      " 'label': 0,\n",
      " 'sentence': 'an extremely unpleasant film . '}\n"
     ]
    }
   ],
   "source": [
    "pprint(encoded_data3['test'][12])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Useful links:\n",
    "[Github Hugginface Intro](https://github.com/huggingface/transformers/tree/main/notebooks)\n",
    "[Bechmarking and Resource Monitor](https://huggingface.co/docs/transformers/benchmarks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Часть 2.\n",
    "----\n",
    "### [BERT](https://github.com/google-research/bert)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_BERT (Bidirectional Encoder Representations from Transformers)_  - одна из первых моделей-автокодировщиков, которая использует архитектуру трансформера с небольшими дополнениями (masked language modelling) для целей моделирования языка. Архитектура BERT - многослойный трансформер-энкодер, который мало чем отличается от оригинальной архитектуры трансформера. Сама модель Transformer изначально предназначалась для задач машинного перевода, но основное улучшение, сделанное в BERT, заключается в использовании части архитектуры для языкового моделирования. Такая языковая модель после предварительной подготовки и дообучения способна обеспечить глобальное понимание языка, на котором она обучилась.\n",
    "\n",
    "----\n",
    "\n",
    "BERT обучали для решения 2-х глобальных задач:\n",
    "1. Masked Language Modelling (MLM).\n",
    "2. Next Sentence Prediction (NSP).\n",
    "\n",
    "\n",
    "Поговорим про подход Masked Language Modelling который используется в архитектуре BERT.\n",
    "\n",
    "_Masked Language Modelling_ - это задача тренировки модели на инпуте (последовательность с некоторым количеством токенов на которых была использована маска (какие-то токены в случайном порядке были пропущены)) и получение вывода целого предложения с заполненной маской (пропуском). Почему использование Masked подхода позволяет модели получать наилучшие результаты на инференсе -  если модель может выполнить cloze test (лингвистический тест для оценки понимания языка путем заполнения пропусков), значит, у нее есть общее понимание самого языка. Тогда для других задач нам будет достаточно небольшой дотренировки, чтобы модель показывала результаты не хуже чем на своем бенчмарке.\n",
    "\n",
    "Пример cloze теста:\n",
    "```\n",
    "1. Input:\n",
    "2. \"Deep ____ is the subset of AI.\"\n",
    "3. Output:\n",
    "4. \"Deep Learning is the subset of AI.\"\n",
    "```\n",
    "\n",
    "Вторая задача, для которой BERT создавался - задача генерации последовательностей. То есть необходимо было сделать так, чтобы BERT не только учитывал случайные отношения между токенами в корпусе и предсказывал пропущенные, но и также обладал способностью понимать отношения между 2-мя и более предложениями в последовательности.\n",
    "\n",
    "_Практические задачи, где используется BERT_:\n",
    "* Text Encoding\n",
    "* Text Similarity\n",
    "* Text Summarization\n",
    "* Text Classification\n",
    "* Next Sentence (Token) Prediction\n",
    "* Contextual Understanding\n",
    "* Question Answering (QA)\n",
    "* Response Seleciton\n",
    "* etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [BERT Архитектура:](https://huggingface.co/blog/bert-101)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Существует несколько вариаций архитектур трансформера BERT, но началось все с 2-х главных архитектур, которые затем минимально улучшались:\n",
    "* BERT-BASE  - модель BERT которая была обучена на относительно большом датасете, архитектура практически не изменялась с оригинальной версией трансформера.\n",
    "* BERT-LARGE - модель BERT которая была обучена на массивном качественном датасете, когда-то являлась SOTA решением в языковых моделях.\n",
    "----\n",
    "\n",
    "BERT - это хорошо обученный стек энкодеров (части архитектуры трансформера), который отвечает за кодирование последовательности. Визуально:\n",
    "\n",
    "![](./../src/imgs/bert-encoders.png)\n",
    "\n",
    "Обе базовые версии BERT имеют большое количество слоев энкодера и следующую специфику:\n",
    "\n",
    "![](./../src/imgs/bert_arch.png)\n",
    "\n",
    "* 12 слоев энкодера для базовой версии и 24 для большой (также эти слои известны как Transformer Blocks)\n",
    "* Feed-Forward слои - 768 скрытых нейронов для базовой версии и 1024 для большой\n",
    "* Attention Heads - 12 для базовой и 16 для большой версии\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Вход для модели трансформера BERT:__\n",
    "\n",
    "![](./../src/imgs/bert_input.png)\n",
    "\n",
    "Первым входом (нулевым элементом последовательности) является специальный токен __[CLS]__, который означает начало классификации (CLS - classification). Остальные элементы последовательности - закодированные токены предложения.\n",
    "Как и обычный энкодер у базового трансформера:\n",
    " 1. BERT принимает на вход последовательность слов (токенов) и обрабатывает его через первый Encoder.\n",
    " 2. В Encoder'e последовательность проходит через self-attention блок.\n",
    " 3. Дальше выход из self-attention'a отдается в Feed-Forward сеть для задачи классификации и результат подается в следующий блок Encoder'a.\n",
    " 4. Так повторяется до тех пор, пока мы не дойдем до последнего блока Encoder'a\n",
    "\n",
    "![](./../src/imgs/bert_encoder_blocks.png)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Выход для модели трансформера BERT:__\n",
    "\n",
    "![](./../src/imgs/bert_output.png)\n",
    "\n",
    "Каждый выходной нейрон в модели BERT выдает вектор размерности __hidden_size__ (768 в случае BERT). В случае задач классификации нам инетерес лишь первый токен [CLS], который использует в качестве инпута в классификаторе. При помощи одного единственного классификатора мы можем решать соответственно задачи на классификацию текста, zero-shot классификацию, заполнение маски и т.д. Как это выглядит визуально:\n",
    "\n",
    "![](./../src/imgs/bert_output2.png)\n",
    "\n",
    "Вот так кратко выглядит работа трансформера BERT."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### __Представления последовательностей для модели трансформера. [__<u>Модель ELMo</u>__](https://arxiv.org/pdf/1802.05365.pdf):__\n",
    "----\n",
    "С какими способами представлений текстовых последовательностей мы уже близко знакомы:\n",
    "1. CountVectorizer\n",
    "2. TfIdf\n",
    "3. Word2Vec\n",
    "4. Glove\n",
    "5. Vanilla Embeddings\n",
    "\n",
    "Будут ли способны все выше указанные представления хорошо понимать смысл (семантическую структуру) последовательности текста?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Не всегда, хорошо было бы иметь такое представление, которое бы учитывало еще и контекст токена, так как в зависимости от контекста токен может обозначать различный смысл.\n",
    "\n",
    "Например:\n",
    "1. В 1997 году _**<u>вышла в свет</u>**_ первая книга о мальчике-волшебнике Гарри Поттере.\n",
    "2. Я  _**<u>вышел</u>**_ на улицу и увидел _**<u>свет</u>**_ горящий где-то в далеке.\n",
    "\n",
    "Для того чтобы не \"хардкодить\" такие отношения, придумали новый вариант представления для Embedding'ов, который получил название ELMO - Embeddings for Language Modelling.\n",
    "Вместо того, чтобы использовать конкретный размер для каждого токена, ELMо берет в рассчет всю последовательность прежде чем закодировать ее в представление Embedding'a. Под капотом работает двунаправленная LSTM рекуррентная нейронная сеть, которая обучается на одной конкретной задаче (узконаправленном корпусе) - составить Embedding для данной задачи.\n",
    "\n",
    "![](./../src/imgs/elmo_embd.png)\n",
    "\n",
    "Но мы рассматриваем модель BERT при чем тут ELMo?\n",
    "\n",
    "Логичный вопрос, но надо было сказать пару слов об ELMo, чтобы понять следующий блок объяснения.\n",
    "\n",
    "Окей, ELMo реализует механизм контекстуального Embedding'a на уровне предложений, соответственно мы можем смотреть на закодированную последовательность с разных углов и решать разные задачи. Но как запустить такой механизм в BERT трансформере. Все просто, придумали механизм MASKING'a (помним выше я упоминал для каких задач изначально придумывали BERT). В чем его суть:\n",
    "\n",
    "![](./../src/imgs/bertmasking.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вроде бы разобрались, для тех кто любит посложнее - вот Paper на реализацию трансформера BERT, очень хорошо прокачивает понимание языковых моделей - [PAPER](https://arxiv.org/pdf/1810.04805.pdf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__**Как и какие задачи можно решать при помощи BERT:**__\n",
    "\n",
    "![](./../src/imgs/bert_tasks.png)\n",
    "\n",
    "\n",
    "Независимо от всех этих задач, наиболее важной способностью BERT является его контекстуальное представление текста. Причина, по которой он успешен в различных задачах, заключается в архитектуре кодировщика Transformer, которая представляет ввод в виде неразреженных (плотных) векторов. Эти векторы могут быть легко преобразованы в выходные данные с помощью очень простых классификаторов и использованны дальше в зависимости от цели задачи (например подавать дальше в RNN, CNN и прочие архитектуры)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Рассмотрим один из вариантов дообучения модели BERT. Будем использовать базу данных отзывов на фильмы [IMDB](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "imdb_df = pd.read_csv(\"./../data/imdb/IMDB_Dataset.csv\")\n",
    "reviews = imdb_df.review.to_string(index=None)\n",
    "\n",
    "# Сохраним как корпус наших данных для дальнейших целей\n",
    "with open(\"./../data/imdb/corpus.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(reviews)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь, когда у нас есть собственный корпус данных, нам необходимо дообучить токенизатор (в нашем случае BERT токенизатор, так как для каждого трансформера нужен собственный токенизатор на котором он обучался). BERT изначально обучали на WordPiece токенизации. Как это реализовать:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.train(\"./../data/imdb/corpus.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Всё :)) Теперь у нас есть собственный дообученный токенизатор, который работает по принципу WordPiece Токенизации. Мы можем посмотреть, что лежит внутри объекта преобученого токенизатора:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ultra': 4246,\n 'cure': 5143,\n 'dimw': 13308,\n 'surp': 5276,\n 'strang': 7417,\n 'figh': 15304,\n '1917': 16971,\n 'cases': 7601,\n '##port': 3568,\n 'neigh': 5526,\n '##gic': 2787,\n 'il': 4137,\n 'gooding': 5868,\n 'sandler': 4815,\n 'promised': 7531,\n 'schizoph': 10476,\n 'grem': 10148,\n 'nephew': 18196,\n 'men': 1433,\n 'baby': 2514,\n 'coat': 15295,\n 'whit': 10001,\n 'slipknot': 17943,\n 'polit': 2425,\n 'ago': 697,\n 'animate': 9105,\n 'britis': 12541,\n 'advert': 3136,\n '##ulse': 7973,\n 'match': 5950,\n 'tw': 421,\n 'judges': 12781,\n 'nr': 14038,\n '196': 1721,\n 'christ': 1001,\n 'dogma': 16232,\n '##orrow': 9969,\n '##olare': 14825,\n '##ison': 1901,\n 'gimm': 8661,\n 'capable': 10726,\n 'mediocrity': 16851,\n 'sadly': 2634,\n 'fortun': 3523,\n 'rudd': 13251,\n 'rut': 9830,\n 'duckman': 17141,\n 'cour': 1757,\n 'patri': 5996,\n 'gallery': 17273,\n 'sho': 1684,\n 'retitled': 15832,\n 'exceptions': 16164,\n '##ably': 578,\n 'jurass': 7719,\n 'growing': 3143,\n 'shows': 1095,\n '##ness': 1297,\n 'ner': 4901,\n 'hatred': 8380,\n 'relaxing': 13511,\n 'grint': 12182,\n 'supreme': 9581,\n '##wyn': 11650,\n 'stock': 4498,\n 'evie': 13915,\n '##ak': 392,\n 'zab': 9871,\n 'truffaut': 13796,\n 'pans': 14086,\n 'looney': 14798,\n '¨': 76,\n 'wonderful': 774,\n '##ounding': 3937,\n 'unheralded': 18217,\n '##ational': 1617,\n 'stilted': 6525,\n 'collecting': 13267,\n '##ville': 3253,\n 'attempted': 6873,\n '##utt': 10037,\n 'tol': 5817,\n 'triumph': 5975,\n 'alph': 11914,\n 'hank': 13969,\n '##ars': 8810,\n '##aum': 10188,\n '##ctic': 14626,\n 'gaspar': 17514,\n 'excel': 6321,\n '##atrice': 14425,\n 'scott': 1967,\n '##gu': 1256,\n '##ccia': 14332,\n 'frederick': 16299,\n 'correction': 8426,\n 'gem': 1592,\n '##gr': 1227,\n 'intercut': 15765,\n 'disk': 10282,\n 'lond': 2395,\n 'chili': 14759,\n 'clear': 1562,\n 'steven': 1927,\n 'rewrites': 17890,\n '##ta': 1299,\n 'exag': 10102,\n 'gid': 13947,\n '15': 1893,\n 'brazilian': 13427,\n 'chaplin': 3175,\n 'english': 1931,\n 'displeasure': 13692,\n 'hoped': 4761,\n 'subjec': 17574,\n 'retur': 6866,\n 'kirst': 13482,\n 'reitman': 11767,\n 'naruto': 13309,\n 'goldie': 6901,\n 'venge': 13001,\n 'euro': 6444,\n 'considere': 13552,\n '##entro': 14619,\n 'fri': 5511,\n 'tas': 6627,\n 'ad': 360,\n 'giv': 5293,\n '##isch': 14398,\n 'businessman': 13089,\n 'tugg': 14132,\n 'und': 526,\n 'proper': 5086,\n 'deliver': 3846,\n 'th': 133,\n '##kid': 14318,\n 'originality': 15480,\n 'bunuel': 5998,\n 'tutor': 13487,\n '##riede': 17720,\n '##ober': 5795,\n 'dialogu': 16435,\n 'gung': 6159,\n 'books': 2503,\n 'van': 1635,\n 'nim': 11466,\n 'unworthy': 17023,\n 'stairway': 13781,\n 'entry': 2548,\n 'gran': 12181,\n '##us': 433,\n 'excelle': 11115,\n 'sundan': 12733,\n 'henry': 3429,\n 'suppo': 10544,\n 'novel': 1159,\n 'unfortunate': 4090,\n 'unde': 5574,\n 'mard': 11456,\n 'strat': 12289,\n 'witchy': 16848,\n 'bashing': 10347,\n 'takas': 16290,\n 'harvard': 13701,\n 'alar': 10127,\n 'somerset': 7299,\n '##isem': 14400,\n '6': 26,\n 'worth': 1236,\n 'comedies': 2053,\n '39': 11338,\n 'num': 1534,\n 'pyun': 8500,\n 'acr': 15327,\n 'documentar': 3318,\n 'err': 3162,\n '##ortal': 10239,\n 'feresten': 18070,\n 'ch': 230,\n 'explo': 6876,\n '##fighter': 9557,\n '##gosi': 3898,\n 'emeril': 17483,\n 'perverted': 15209,\n 'escap': 4432,\n 'carrere': 17128,\n 'montgo': 13119,\n '##vil': 1942,\n 'swift': 9179,\n 'vi': 1518,\n 'compelled': 6468,\n '##oria': 8804,\n 'occasional': 16771,\n '##ru': 3565,\n '##avano': 15523,\n 'brie': 14641,\n 'tooth': 11725,\n 'disadvant': 15102,\n 'frequently': 5722,\n '1927': 7632,\n 'bob': 3813,\n 'vague': 5468,\n 'consider': 1040,\n '##ona': 6690,\n 'assigned': 9271,\n 'ewoks': 18066,\n 'excessive': 17971,\n 'hugh': 5289,\n 'misfires': 13045,\n '##otine': 14561,\n 'pract': 4177,\n 'inev': 7639,\n 'kriem': 16548,\n 'cusak': 16985,\n 'date': 2961,\n 'netw': 16540,\n '##ical': 528,\n 'partner': 6813,\n 'blondell': 13297,\n 'traumat': 17565,\n 'magn': 3335,\n 'royal': 6037,\n '##uther': 5225,\n '##str': 1582,\n 'colin': 6836,\n 'conce': 3474,\n 'sean': 4040,\n 'trunk': 10199,\n 'pleasingly': 15631,\n '##iev': 2299,\n 'bettie': 6518,\n 'jack': 1034,\n 'trinity': 14970,\n '##ictions': 16063,\n 'exact': 1588,\n 'eerie': 9772,\n 'between': 1497,\n 'animatrix': 9566,\n '[CLS]': 2,\n 'places': 7322,\n 'underworld': 9672,\n '##wis': 11646,\n '##eeee': 13083,\n 'hedy': 7135,\n '1947': 6415,\n 'therefore': 10189,\n 'republ': 16020,\n 'corresp': 13129,\n 'sherman': 13126,\n 'utopia': 16175,\n '##ff': 457,\n 'glued': 15669,\n 'bunch': 2251,\n 'guarant': 17560,\n 'trying': 1653,\n 'joel': 7368,\n 'cracker': 5692,\n 'bilge': 12910,\n 'dak': 9762,\n 'arabian': 13365,\n 'persu': 13106,\n '##aders': 14579,\n 'walked': 3960,\n 'allan': 10133,\n 'diego': 10719,\n '##iliar': 2285,\n '##vello': 14550,\n 'reserv': 12676,\n 'feat': 942,\n 'ketchup': 18091,\n '1998': 4396,\n '##osian': 12183,\n 'problem': 1357,\n 'clarence': 17100,\n '##ite': 300,\n 'refined': 12770,\n 'revolution': 4284,\n 'belie': 594,\n 'potemkin': 13625,\n 'savages': 16763,\n 'defin': 926,\n 'eloqu': 10515,\n 'cillian': 13739,\n 'elderly': 13434,\n 'barcel': 15581,\n 'nune': 14057,\n 'miragl': 16823,\n 'writin': 15569,\n 'f': 48,\n '1957': 6868,\n 'eugene': 7766,\n 'separates': 13193,\n 'value': 3975,\n 'seasoned': 16138,\n 'noi': 14976,\n 'green': 2750,\n 'poster': 3301,\n 'animals': 5601,\n 'tur': 1695,\n 'marjor': 12315,\n 'braz': 5505,\n 'lighting': 8258,\n '##oder': 11995,\n 'predecessor': 13736,\n 'nature': 4247,\n '##istry': 14949,\n 'transl': 6055,\n '49': 11340,\n 'tast': 8715,\n 'stripes': 13516,\n 'happens': 2506,\n '##iratnam': 14634,\n 'elizabeth': 3988,\n 'dese': 10343,\n 'clarks': 17124,\n 'when': 248,\n 'rubber': 17782,\n 'ent': 1081,\n 'ian': 9786,\n 'lam': 2256,\n '##et': 202,\n 'dashed': 17597,\n 'basis': 8096,\n '##rac': 14849,\n 'frank': 1537,\n 'bure': 11351,\n '##quer': 6814,\n '##icidal': 13780,\n '##wwww': 6478,\n 'darius': 12801,\n 'inno': 3203,\n 'elmo': 15674,\n 'own': 1489,\n 'spe': 782,\n 'ninth': 16755,\n 'med': 1791,\n '-': 17,\n 'deathstal': 12673,\n 'cousin': 9569,\n 'hyper': 7541,\n '##ii': 2835,\n '##ahl': 15649,\n '##rall': 9896,\n 'propaganda': 5128,\n 'snippet': 17578,\n 'desperado': 16795,\n '1905': 17545,\n 'pops': 9254,\n 'wins': 6884,\n 'sleeping': 9451,\n 'caddy': 6530,\n 'cheat': 12308,\n 'buchanan': 18237,\n '##ean': 14217,\n '##atisf': 14423,\n 'lured': 9805,\n 'dol': 9761,\n 'paying': 16577,\n '##pity': 11600,\n 'montana': 13118,\n 'ultimatum': 9396,\n 'watche': 5850,\n 'cesar': 11372,\n 'wilder': 6462,\n '##show': 8162,\n 'looke': 16417,\n 'someh': 14911,\n 'parents': 3824,\n 'indians': 9171,\n '##aric': 14467,\n 'trade': 14966,\n 'incom': 4562,\n 'rhy': 13287,\n '##ady': 1110,\n '##aja': 14241,\n 'geeks': 17209,\n '##eve': 4484,\n '##tan': 2969,\n 'thurman': 8610,\n 'holotik': 15574,\n 'tick': 4910,\n 'har': 949,\n '##quez': 15921,\n 'chop': 4340,\n 'grud': 4541,\n 'reads': 12144,\n 'repulsive': 13709,\n 'wagner': 11064,\n 'gor': 6598,\n 'criti': 9157,\n 'charm': 6320,\n 'controve': 17063,\n 'curt': 7502,\n '1961': 16201,\n 'graphic': 8445,\n 'segment': 11801,\n 'wait': 2472,\n 'really': 287,\n '##ð': 128,\n 'various': 3724,\n 'montano': 13819,\n '##andering': 8022,\n '##cules': 7406,\n 'illegal': 7088,\n 'key': 3607,\n 'composed': 10492,\n 'bog': 3186,\n 'beij': 17236,\n '##head': 1888,\n '##master': 7886,\n 'neg': 1829,\n 'neil': 3799,\n '##sty': 14475,\n 'precinct': 11088,\n 'uber': 11524,\n 'downhill': 15608,\n '##quis': 5282,\n 'protocol': 17049,\n 'exiting': 10104,\n 'senior': 10972,\n 'acto': 6274,\n '##ipul': 4756,\n 'sally': 7843,\n 'stunningly': 7577,\n 'cranky': 10320,\n 'red': 1211,\n '##sploitation': 14234,\n 'avoid': 2766,\n 'hunk': 12632,\n 'icarly': 18081,\n 'obelix': 17962,\n '##devel': 12246,\n 'refreshing': 4640,\n 'painful': 2680,\n 'brings': 2571,\n 'imp': 832,\n 'gym': 7015,\n 'bg': 13868,\n 'miller': 4144,\n 'none': 3192,\n 'rebecca': 13663,\n 'misca': 15561,\n 'bfg': 18054,\n 'established': 18000,\n 'documentary': 879,\n 'george': 1531,\n 'zand': 14185,\n '##ender': 4735,\n 'quantum': 11193,\n 'blac': 8985,\n 'jacques': 10958,\n 'manna': 7314,\n 'sheds': 14694,\n 'heavy': 5398,\n 'researched': 13332,\n 'ann': 1625,\n 'titanic': 6342,\n 'ex': 233,\n 'ne': 284,\n 'subcon': 12470,\n 'unclear': 13354,\n 'compe': 2619,\n 'eros': 15859,\n 'mccartney': 16752,\n 'edie': 15494,\n 'dressing': 16803,\n 'monkeys': 7697,\n 'tib': 11517,\n 'downtown': 10490,\n 'vivid': 10961,\n 'unli': 8933,\n 'bapt': 11355,\n 'okay': 817,\n 'damn': 3727,\n 'unpretentious': 13385,\n 'reminiscent': 7075,\n 'mcgee': 16296,\n 'kidnapp': 17647,\n 'conceived': 4851,\n 'ille': 6521,\n 'dah': 5751,\n 'into': 596,\n '##ulate': 10093,\n 'posey': 12037,\n 'gag': 5761,\n 'ß': 83,\n '##vers': 1166,\n 'wishes': 12682,\n 'musicals': 3078,\n 'audien': 3721,\n 'intentionally': 10909,\n 'ji': 13989,\n '22': 5739,\n 'proble': 1335,\n 'springs': 5402,\n 'finds': 3069,\n 'alleg': 11014,\n 'inconsist': 18241,\n 'ar': 434,\n 'soup': 7172,\n '##kling': 14319,\n 'origin': 575,\n 'ticked': 13290,\n '##acious': 5915,\n 'raised': 4409,\n 'fitting': 8655,\n 'widely': 5371,\n 'cowboys': 13087,\n '##its': 1398,\n 'luc': 2941,\n 'empt': 6362,\n 'kind': 829,\n 'gossett': 10238,\n 'techni': 16474,\n 'clif': 10254,\n 'disn': 15098,\n 'zipp': 11551,\n 'duvall': 11261,\n '##ero': 1963,\n 'underwor': 8113,\n 'pie': 2525,\n '##pr': 2611,\n 'introduces': 9395,\n 'yr': 6192,\n 'pauline': 16029,\n 'billie': 16245,\n 'airways': 16242,\n '##yll': 14305,\n 'wo': 1445,\n 'sterile': 17286,\n 'outbre': 14954,\n 'unfunny': 3996,\n 'anot': 9985,\n 'currently': 4264,\n 'popey': 12695,\n 'jennifer': 3991,\n 'elements': 3839,\n 'founders': 15463,\n 'inspiratio': 17111,\n 'vhs': 2953,\n '##mak': 14341,\n 'pulling': 13028,\n 'out': 318,\n 'seduces': 17461,\n 'humph': 8185,\n 'poppa': 12696,\n 'hunted': 12633,\n 'violen': 10758,\n 'childhood': 3321,\n 'columb': 2183,\n 'benedict': 16483,\n 'foolishly': 9577,\n 'redo': 15869,\n 'useless': 9293,\n 'pakeezah': 18112,\n 'mukh': 15013,\n 'trotta': 12015,\n 'atrocity': 6124,\n 'rp': 14094,\n 'tasty': 17606,\n 'repeat': 5396,\n 'grieving': 15235,\n '##g': 100,\n 'finding': 4560,\n 'starev': 15204,\n 'naz': 4473,\n 'brent': 11824,\n 'ryan': 4443,\n 'bettany': 13808,\n 'artistically': 17166,\n 'inform': 3989,\n 'establ': 12984,\n 'brill': 921,\n '##works': 16690,\n 'blond': 4986,\n '##yon': 8758,\n 'freely': 15968,\n 'fail': 1609,\n 'seth': 10031,\n 'enchanted': 7703,\n 'dood': 8641,\n 'comparisons': 9309,\n 'overa': 15269,\n 'cliched': 4810,\n 'recipe': 10342,\n 'embezzler': 18247,\n 'offic': 2627,\n 'explain': 3077,\n 'hypo': 16076,\n 'sui': 11906,\n 'efficient': 16203,\n '##mark': 2842,\n '##oops': 13070,\n 'counts': 12738,\n '##r': 99,\n '##inine': 11678,\n 'chandni': 13598,\n '##ver': 194,\n 'jim': 1552,\n '##www': 9402,\n '##uto': 10035,\n '[SEP]': 3,\n 'social': 4174,\n 'shabby': 13671,\n 'skillfully': 17434,\n '##rated': 1508,\n 'oprah': 15443,\n '##odge': 5249,\n 'legion': 16256,\n 'phoe': 15611,\n 'ged': 11403,\n 'coronado': 17970,\n 'exclusive': 17787,\n 'karate': 5351,\n '##iness': 2266,\n 'wea': 14777,\n 'inap': 11741,\n 'gonna': 4011,\n 'deadly': 4407,\n 'tigers': 17324,\n 'alexandre': 16911,\n 'calibre': 17496,\n 'write': 1336,\n 'target': 6929,\n 'contrast': 9435,\n 'commun': 4222,\n 'harv': 10506,\n 'conne': 4968,\n 'sarah': 3021,\n '##entine': 5836,\n '##ooo': 10046,\n 'produ': 676,\n '##ezah': 17041,\n '1940s': 5334,\n '##sc': 1013,\n 'maugham': 9658,\n '##jor': 11662,\n 'lost': 1493,\n 'disposable': 18039,\n 'serving': 12114,\n '##bers': 7892,\n '##iere': 11577,\n 'motor': 12959,\n 'activity': 11982,\n 'trippy': 15948,\n 'prejudice': 13575,\n 'cockney': 17346,\n '##ls': 5190,\n '##scape': 4772,\n 'living': 1982,\n 'mik': 12460,\n 'fingers': 5446,\n '##using': 2052,\n 'ghastly': 10703,\n 'omen': 7161,\n 'spra': 7988,\n 'multiple': 8448,\n 'geniu': 15403,\n 'appallingly': 10870,\n 'gratifying': 17161,\n 'meanings': 15745,\n 'gugino': 12233,\n '##fic': 5478,\n 'h': 50,\n 'cardone': 13273,\n 'cartoon': 1412,\n 'romeo': 7669,\n 'gover': 10235,\n '01': 13846,\n '##enzo': 5684,\n '##ject': 1314,\n 'characteriz': 12600,\n '##aine': 3927,\n '##rent': 6653,\n '##bound': 14368,\n 'peoples': 10297,\n 'watcher': 8865,\n 'bci': 11354,\n 'stern': 17285,\n 'trepidati': 17658,\n 'kel': 9794,\n '##ayne': 11811,\n 'exist': 3376,\n '##oj': 14195,\n 'thief': 4159,\n '09': 13848,\n 'thir': 2845,\n 'streets': 4816,\n 'givin': 15565,\n '##star': 7924,\n 'random': 4832,\n 'tremors': 10985,\n 'acrobat': 17768,\n 'homegrown': 12598,\n 'camp': 1650,\n 'watchin': 4503,\n 'evolved': 15604,\n 'leary': 16395,\n 'fighter': 6955,\n '##episi': 12383,\n 'tus': 11514,\n 'pake': 14076,\n 'procedural': 17756,\n '194': 1495,\n 'amazingly': 5610,\n 'overwhel': 6566,\n 'centric': 16182,\n 'anticip': 5427,\n 'step': 2759,\n '##raint': 11932,\n 'maher': 12012,\n 'maca': 14960,\n 'invari': 16630,\n 'terri': 6317,\n 'infl': 16208,\n 'pbs': 5171,\n '192': 2932,\n 'wandering': 14170,\n 'pretensions': 18236,\n 'challen': 5408,\n 'barn': 9102,\n 'powe': 12549,\n 'glacier': 13620,\n 'phrase': 8156,\n 'ag': 375,\n '##gins': 12067,\n 'raiders': 12625,\n '##asc': 1705,\n 'dict': 7797,\n '##eem': 2198,\n 'doesnt': 12267,\n 'unmiss': 13293,\n 'bohl': 15426,\n 'choose': 7975,\n 'waterdance': 16601,\n 'matinee': 7472,\n 'tutu': 17462,\n 'ani': 7229,\n 'pinocchio': 17713,\n 'darkly': 15681,\n 'medical': 6009,\n '##asu': 14450,\n '1913': 13157,\n 'burns': 6087,\n 'experience': 2249,\n 'rapp': 10939,\n 'scoop': 8912,\n '##iss': 1651,\n 'minu': 9027,\n 'saps': 11877,\n '##kien': 11603,\n '##imin': 11860,\n 'wyn': 5794,\n 'repetitive': 7757,\n 'captures': 4597,\n 'generation': 5042,\n 'spirited': 5665,\n 'restored': 12932,\n 'comple': 903,\n 'enjoyment': 15215,\n 'scratcher': 16696,\n 'avail': 3832,\n 'bly': 13873,\n 'file': 6695,\n 'invigorating': 16632,\n 'luciano': 17678,\n 'studios': 3959,\n '##rofess': 15486,\n 'influ': 4589,\n 'retarded': 5043,\n 'monsters': 8151,\n 'wonders': 6810,\n 'chapter': 8340,\n 'creativity': 12836,\n 'candace': 17293,\n 'providing': 16517,\n 'promisingly': 9648,\n 'iph': 11421,\n 'animat': 5600,\n 'soulless': 16686,\n 'dolls': 7617,\n 'ope': 11476,\n 'scarlett': 6545,\n 'hobb': 12337,\n 'expos': 5639,\n 'bian': 11353,\n 'union': 5536,\n 'firefighters': 17994,\n 'sleepwalkers': 18292,\n 'rip': 2086,\n 'akin': 11349,\n 'repe': 3575,\n 'unaware': 11317,\n '##ann': 2405,\n 'widowed': 11027,\n 'brown': 4703,\n 'mattei': 13619,\n 'trond': 14964,\n 'geared': 17844,\n 'breakfast': 10810,\n 'mentio': 13188,\n 'bullock': 5696,\n 'cap': 1789,\n '##ling': 1373,\n 'paradoxes': 11251,\n 'tackles': 11062,\n 'realist': 12248,\n 'plank': 14936,\n '##so': 1269,\n 'ulysses': 18130,\n 'jos': 8979,\n 'buil': 8125,\n 'preach': 6761,\n 'sv': 11496,\n 'pro': 407,\n 'smugg': 12376,\n 'includ': 2349,\n 'wanting': 4996,\n '##alt': 14517,\n 'duran': 16987,\n 'misrep': 12381,\n 'staling': 10029,\n 'tenchi': 17874,\n 'rab': 6183,\n 'fresh': 3507,\n 'apprentice': 13814,\n 'thug': 14393,\n 'schroeder': 10477,\n 'week': 1260,\n 'o': 57,\n 'caus': 7119,\n '##retentious': 12603,\n 'concise': 16918,\n 'wit': 1198,\n 'dreck': 5959,\n 'exists': 7976,\n 'coma': 14730,\n 'engross': 6864,\n '##ographic': 9220,\n 'definite': 1074,\n 'afraid': 4000,\n 'hass': 11942,\n 'gettin': 10312,\n 'suspension': 12804,\n '##form': 7423,\n 'remakes': 4059,\n 'heark': 15288,\n 'axe': 13431,\n 'mytho': 17410,\n '##dermott': 10470,\n 'bearer': 16781,\n '##atcher': 8363,\n 'tintin': 13651,\n 'govern': 4522,\n 'soldiers': 5673,\n 'ı': 88,\n 'kim': 3546,\n 'budgeted': 15679,\n '##anist': 14460,\n 'atten': 7421,\n 'grand': 2414,\n 'sluizer': 10369,\n 'drag': 3330,\n '##oma': 4496,\n 'trivial': 15949,\n 'marilyn': 12316,\n 'redneck': 12555,\n 'entertai': 8481,\n 'incubus': 14525,\n 'turd': 12759,\n 'thorough': 2321,\n 'indies': 15782,\n '##asy': 8807,\n '##aro': 9981,\n 'colleague': 15880,\n 'rebel': 6238,\n 'parado': 9136,\n 'fe': 330,\n 'imita': 15123,\n 'rodney': 8406,\n 'vene': 9860,\n 'while': 563,\n '1965': 5651,\n 'spoo': 14835,\n 'definately': 10500,\n 'easy': 2150,\n '##chak': 7951,\n 'playing': 2243,\n '##isa': 4928,\n 'demonstrated': 16815,\n 'grat': 4989,\n '##igenia': 12052,\n 'statistical': 16868,\n 'working': 2074,\n 'suddenly': 11072,\n 'punishment': 9440,\n 'mild': 2258,\n '##de': 572,\n '##ider': 933,\n 'ridiculous': 2154,\n 'civ': 4454,\n 'peck': 4973,\n 'sphe': 10138,\n 'snow': 3414,\n 'bases': 15277,\n 'tearful': 16808,\n 'fuckland': 18072,\n 'cleveland': 15715,\n 'br': 196,\n '##aced': 7410,\n 'awesomely': 13571,\n 'begrud': 18159,\n '##esses': 15106,\n 'cars': 7398,\n 'barred': 15579,\n 'buttgereit': 9725,\n '##gid': 7197,\n '##ized': 2147,\n 'slowest': 12596,\n 'kids': 1543,\n 'satell': 11227,\n 'joh': 5884,\n 'extravaganza': 18277,\n 'emmy': 11394,\n 'dutch': 4631,\n '##versary': 10571,\n 'fessenden': 18069,\n 'crui': 15185,\n 'project': 2861,\n 'mississippi': 11328,\n 'vill': 3193,\n '186': 16482,\n 'names': 4660,\n 'armageddon': 13838,\n 'access': 10495,\n '##mer': 1521,\n 'whomever': 13777,\n 'attack': 3234,\n '##gggg': 12638,\n 'grabbed': 8548,\n 'provin': 12112,\n 'imposs': 3028,\n 'sensiti': 16581,\n '##lor': 3564,\n 'mature': 9130,\n '##getic': 10411,\n 'war': 551,\n 'awe': 5555,\n 'chal': 14750,\n 'spend': 2623,\n 'ing': 3460,\n 'erroneous': 17808,\n 'suprised': 14809,\n '1991': 6853,\n 'gund': 5154,\n 'gram': 15233,\n '##berly': 15091,\n '##ork': 1333,\n 'dogs': 4420,\n 'noise': 8923,\n 'jacket': 15731,\n 'crafted': 4124,\n '##fi': 8775,\n 'sordid': 18122,\n 'able': 2460,\n 'poorly': 1769,\n 'julia': 5081,\n 'pock': 15020,\n '##amish': 14660,\n 'chem': 5231,\n '##une': 3131,\n 'psychiat': 8306,\n 'elabor': 12451,\n 'conclud': 10928,\n '##ograph': 1382,\n 'snakes': 5968,\n 'somewha': 10618,\n 'ill': 2332,\n 'grossly': 13258,\n 'manufact': 18195,\n 'symp': 6494,\n 'rig': 5462,\n 'abd': 10076,\n 'acknowledge': 13475,\n 'david': 1339,\n 'stephens': 16323,\n 'wher': 10000,\n 'entr': 10554,\n 'dek': 11965,\n 'truman': 14969,\n 'hom': 2522,\n 'release': 1645,\n 'olsen': 16893,\n 's': 61,\n '##ms': 2069,\n 'antz': 16229,\n '##sman': 9886,\n 'relatively': 4847,\n '54': 9731,\n 'def': 807,\n ...}"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.get_vocab()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Не забываем сохранять преобученный токенизатор, так как в дальнейшем он нам понадобится:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "['./../outputs/tokenizer/vocab.txt']"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.save_model(\"./../outputs/tokenizer\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "# как загрузить модельку токенизатора для BERT которую мы сохранили:\n",
    "tokenizer = BertWordPieceTokenizer.from_file(\"./../outputs/tokenizer/vocab.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь, убедившись что теоретически наши токены будут также присутствовать в токенизаторе мы можем с легкостью воспользовать его функционалом:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]',\n 'this',\n 'is',\n 'the',\n 'natural',\n 'language',\n 'process',\n '##ing',\n 'using',\n 'bert',\n 'transform',\n '##er',\n '.',\n '[SEP]']"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = tokenizer.encode(\"This is the Natural Language Processing using BERT Transformer.\")\n",
    "tokenized_sentence.tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Важный момент, окей мы можем дотренировать свой токенизатор, а еще можем воспользоваться BertTokenizerFast из библиотеки transformers и \"дополнить\" своими данными, чтобы в будущем избежать неловких ситуаций и ошибок во время работы с BERT:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast #BertTokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"./../outputs/tokenizer\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь подготовим корпус для быстрого обучения:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path=\"./../data/imdb/corpus.txt\", block_size=128)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сортировщик данных (Data Collator) получает данные и подготавливает их к обучению.\n",
    "Например, возьмем наши данные и подготовим их для моделирования для маскинга языка с вероятностью 0,15.\n",
    "Цель использования такого механизма — выполнять предварительную обработку «на лету», что позволяет использовать меньше ресурсов. С другой стороны, это замедляет процесс обучения, потому что каждый образец должен быть предварительно обработан на лету во время обучения."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# Также нам необходимо вопсользовать сортировщиком данных для того чтобы смоделировать процесс маскинга\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Популярная практика задавать аргументы для объекта (можно и не делать, но могут возникнуть ошибки):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./../outputs/BERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=128\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загрузим BERT трансформер с дефолтными настройками поставки:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "bert = BertForMaskedLM(BertConfig())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим объект тренировщика для дотренировки модели:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BertConfig"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=bert, args=training_args, data_collator=data_collator, train_dataset=dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Запустим обучение:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 50022\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 391\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/391 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сохраним модельку для будущего использования и возможно доработки:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.save_model(\"./../outputs/MyBERT\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Небольшое отступление, мы загрузили дефолтный конфиг для трансформера BERT, но мы также можем указывать свою спецификацию конфига:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BertConfig()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Спецификации BERT](https://github.com/google-research/bert)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tiny_bert_config = BertConfig(max_position_embeddings=512, hidden_size=128,\n",
    "           num_attention_heads=2,\n",
    "           num_hidden_layers=2,\n",
    "           intermediate_size=512)\n",
    "tiny_bert_config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tiny_bert = BertForMaskedLM(tiny_bert_config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = Trainer(model=tiny_bert, args=training_args,\n",
    "                     data_collator=data_collator,\n",
    "                     train_dataset=dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Языковая модель BERT также может использоваться в качестве Embedding слоя в любой другой модели глубокого обучения. Например, вы можете загрузить любую предварительно обученную модель BERT или собственную версию, обученную на предыдущем шаге следующим образом:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# по умолчанию PyTorch имплементация, для TensorFlow TFBertModel и BertTokenizerFast соответсвенно\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pip install torch-summary\n",
    "from torchsummary import summary\n",
    "summary(bert,input_size=(768,),depth=1,batch_dim=1, dtypes=['torch.IntTensor'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.tokenize(\"This is an example of bert tokenizer usage\")\n",
    "tokenized_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bert_output = bert(torch.tensor([input_ids]))\n",
    "bert_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Часть 3:\n",
    "----\n",
    "Fine-Tuning модели BERT для классификации текста:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Будем использовать вариацию BERT - [DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert). Настроив \"классификационную голову\" под себя (будем использовать тот же датасет IMDB - где все 2 класса):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /Users/maratmovlamov/.cache/huggingface/transformers/tmpgc46pm3c\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a02455745984a23b4691e13b934eb3a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json in cache at /Users/maratmovlamov/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "creating metadata file for /Users/maratmovlamov/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /Users/maratmovlamov/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /Users/maratmovlamov/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /Users/maratmovlamov/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /Users/maratmovlamov/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /Users/maratmovlamov/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"POS\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /Users/maratmovlamov/.cache/huggingface/transformers/tmpyuweaa9p\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/256M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8318e8b751bf4bf3a3944c5386349134"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin in cache at /Users/maratmovlamov/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "creating metadata file for /Users/maratmovlamov/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /Users/maratmovlamov/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "model_path_or_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path_or_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path_or_name, id2label={0:\"NEG\", 1:\"POS\"}, label2id={\"NEG\":0, \"POS\":1})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "id2label и label2id отдаются модели на этапе инференса. Существует альтернативный вариант - просто задать их в отдельном конфиге:\n",
    "```\n",
    "1. config = AutoConfig.from_pre-trained(....)\n",
    "2.SequenceClassification.from_pre-trained(.... config=config)\n",
    "```\n",
    "\n",
    "Загрузим уже известный нам датасет:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /Users/maratmovlamov/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31f2e9358c9746cdb7508d3e91aa179b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [94], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[0;32m----> 2\u001B[0m imdb_train\u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimdb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m imdb_test\u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimdb\u001B[39m\u001B[38;5;124m'\u001B[39m, split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest[:6250]+test[-6250:]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m imdb_val\u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimdb\u001B[39m\u001B[38;5;124m'\u001B[39m, split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest[6250:12500]+test[-12500:-6250]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/datasets/load.py:1698\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001B[0m\n\u001B[1;32m   1695\u001B[0m try_from_hf_gcs \u001B[38;5;241m=\u001B[39m path \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m _PACKAGED_DATASETS_MODULES\n\u001B[1;32m   1697\u001B[0m \u001B[38;5;66;03m# Download and prepare data\u001B[39;00m\n\u001B[0;32m-> 1698\u001B[0m \u001B[43mbuilder_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1699\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1700\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1701\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_verifications\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_verifications\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1702\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtry_from_hf_gcs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtry_from_hf_gcs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1703\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1704\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1706\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[1;32m   1707\u001B[0m keep_in_memory \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1708\u001B[0m     keep_in_memory \u001B[38;5;28;01mif\u001B[39;00m keep_in_memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_small_dataset(builder_instance\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size)\n\u001B[1;32m   1709\u001B[0m )\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/datasets/builder.py:807\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[0;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[1;32m    801\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m downloaded_from_gcs:\n\u001B[1;32m    802\u001B[0m     prepare_split_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    803\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: file_format,\n\u001B[1;32m    804\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_shard_size\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_shard_size,\n\u001B[1;32m    805\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdownload_and_prepare_kwargs,\n\u001B[1;32m    806\u001B[0m     }\n\u001B[0;32m--> 807\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    808\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    809\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverify_infos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverify_infos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    810\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprepare_split_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    811\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdownload_and_prepare_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    813\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[1;32m    814\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/datasets/builder.py:1416\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verify_infos, **prepare_splits_kwargs)\u001B[0m\n\u001B[1;32m   1415\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_download_and_prepare\u001B[39m(\u001B[38;5;28mself\u001B[39m, dl_manager, verify_infos, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_splits_kwargs):\n\u001B[0;32m-> 1416\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1417\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify_infos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_duplicate_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverify_infos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprepare_splits_kwargs\u001B[49m\n\u001B[1;32m   1418\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/datasets/builder.py:876\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001B[0m\n\u001B[1;32m    874\u001B[0m split_dict \u001B[38;5;241m=\u001B[39m SplitDict(dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m    875\u001B[0m split_generators_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001B[0;32m--> 876\u001B[0m split_generators \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_split_generators\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msplit_generators_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[38;5;66;03m# Checksums verification\u001B[39;00m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verify_infos \u001B[38;5;129;01mand\u001B[39;00m dl_manager\u001B[38;5;241m.\u001B[39mrecord_checksums:\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/imdb/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/imdb.py:83\u001B[0m, in \u001B[0;36mImdb._split_generators\u001B[0;34m(self, dl_manager)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_split_generators\u001B[39m(\u001B[38;5;28mself\u001B[39m, dl_manager):\n\u001B[0;32m---> 83\u001B[0m     archive \u001B[38;5;241m=\u001B[39m \u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_DOWNLOAD_URL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m     85\u001B[0m         datasets\u001B[38;5;241m.\u001B[39mSplitGenerator(\n\u001B[1;32m     86\u001B[0m             name\u001B[38;5;241m=\u001B[39mdatasets\u001B[38;5;241m.\u001B[39mSplit\u001B[38;5;241m.\u001B[39mTRAIN, gen_kwargs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfiles\u001B[39m\u001B[38;5;124m\"\u001B[39m: dl_manager\u001B[38;5;241m.\u001B[39miter_archive(archive), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplit\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     94\u001B[0m         ),\n\u001B[1;32m     95\u001B[0m     ]\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/datasets/download/download_manager.py:310\u001B[0m, in \u001B[0;36mDownloadManager.download\u001B[0;34m(self, url_or_urls)\u001B[0m\n\u001B[1;32m    307\u001B[0m download_func \u001B[38;5;241m=\u001B[39m partial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download, download_config\u001B[38;5;241m=\u001B[39mdownload_config)\n\u001B[1;32m    309\u001B[0m start_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[0;32m--> 310\u001B[0m downloaded_path_or_paths \u001B[38;5;241m=\u001B[39m \u001B[43mmap_nested\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl_or_urls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmap_tuple\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparallel_min_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisable_tqdm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mis_progress_bar_enabled\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDownloading data files\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m duration \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow() \u001B[38;5;241m-\u001B[39m start_time\n\u001B[1;32m    320\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading took \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mduration\u001B[38;5;241m.\u001B[39mtotal_seconds() \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m min\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/datasets/utils/py_utils.py:420\u001B[0m, in \u001B[0;36mmap_nested\u001B[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001B[0m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;66;03m# Singleton\u001B[39;00m\n\u001B[1;32m    419\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_struct, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_struct, types):\n\u001B[0;32m--> 420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_struct\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    422\u001B[0m disable_tqdm \u001B[38;5;241m=\u001B[39m disable_tqdm \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mis_progress_bar_enabled()\n\u001B[1;32m    423\u001B[0m iterable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data_struct\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_struct, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m data_struct\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/datasets/download/download_manager.py:337\u001B[0m, in \u001B[0;36mDownloadManager._download\u001B[0;34m(self, url_or_filename, download_config)\u001B[0m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_relative_path(url_or_filename):\n\u001B[1;32m    335\u001B[0m     \u001B[38;5;66;03m# append the relative path to the base_path\u001B[39;00m\n\u001B[1;32m    336\u001B[0m     url_or_filename \u001B[38;5;241m=\u001B[39m url_or_path_join(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_base_path, url_or_filename)\n\u001B[0;32m--> 337\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcached_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl_or_filename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/datasets/utils/file_utils.py:185\u001B[0m, in \u001B[0;36mcached_path\u001B[0;34m(url_or_filename, download_config, **download_kwargs)\u001B[0m\n\u001B[1;32m    181\u001B[0m     url_or_filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(url_or_filename)\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_remote_url(url_or_filename):\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     output_path \u001B[38;5;241m=\u001B[39m \u001B[43mget_from_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl_or_filename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_etag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_etag\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_url_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_url_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_desc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_desc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(url_or_filename):\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;66;03m# File, and it exists.\u001B[39;00m\n\u001B[1;32m    201\u001B[0m     output_path \u001B[38;5;241m=\u001B[39m url_or_filename\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/datasets/utils/file_utils.py:577\u001B[0m, in \u001B[0;36mget_from_cache\u001B[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, download_desc)\u001B[0m\n\u001B[1;32m    575\u001B[0m         ftp_get(url, temp_file)\n\u001B[1;32m    576\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 577\u001B[0m         \u001B[43mhttp_get\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    578\u001B[0m \u001B[43m            \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    579\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtemp_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    580\u001B[0m \u001B[43m            \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    581\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresume_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[43m            \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    583\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    584\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    585\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_desc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    586\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    588\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstoring \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m in cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcache_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    589\u001B[0m shutil\u001B[38;5;241m.\u001B[39mmove(temp_file\u001B[38;5;241m.\u001B[39mname, cache_path)\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/datasets/utils/file_utils.py:380\u001B[0m, in \u001B[0;36mhttp_get\u001B[0;34m(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries, desc)\u001B[0m\n\u001B[1;32m    371\u001B[0m total \u001B[38;5;241m=\u001B[39m resume_size \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mint\u001B[39m(content_length) \u001B[38;5;28;01mif\u001B[39;00m content_length \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    372\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mtqdm(\n\u001B[1;32m    373\u001B[0m     unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    374\u001B[0m     unit_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    378\u001B[0m     disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mis_progress_bar_enabled(),\n\u001B[1;32m    379\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m--> 380\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_content(chunk_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1024\u001B[39m):\n\u001B[1;32m    381\u001B[0m         progress\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mlen\u001B[39m(chunk))\n\u001B[1;32m    382\u001B[0m         temp_file\u001B[38;5;241m.\u001B[39mwrite(chunk)\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/requests/models.py:816\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[0;34m()\u001B[0m\n\u001B[1;32m    814\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    815\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 816\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    817\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    818\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/urllib3/response.py:627\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp):\n\u001B[0;32m--> 627\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[1;32m    630\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/urllib3/response.py:566\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[1;32m    563\u001B[0m fp_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    565\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_catcher():\n\u001B[0;32m--> 566\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    567\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    568\u001B[0m         flush_decoder \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/DS_Foundations/venv/lib/python3.9/site-packages/urllib3/response.py:532\u001B[0m, in \u001B[0;36mHTTPResponse._fp_read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    529\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m buffer\u001B[38;5;241m.\u001B[39mgetvalue()\n\u001B[1;32m    530\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    531\u001B[0m     \u001B[38;5;66;03m# StringIO doesn't like amt=None\u001B[39;00m\n\u001B[0;32m--> 532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:458\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;66;03m# Amount is given, implement using readinto\u001B[39;00m\n\u001B[1;32m    457\u001B[0m     b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbytearray\u001B[39m(amt)\n\u001B[0;32m--> 458\u001B[0m     n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    459\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(b)[:n]\u001B[38;5;241m.\u001B[39mtobytes()\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    461\u001B[0m     \u001B[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001B[39;00m\n\u001B[1;32m    462\u001B[0m     \u001B[38;5;66;03m# and self.chunked\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:502\u001B[0m, in \u001B[0;36mHTTPResponse.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    497\u001B[0m         b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmemoryview\u001B[39m(b)[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength]\n\u001B[1;32m    499\u001B[0m \u001B[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001B[39;00m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001B[39;00m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;66;03m# (for example, reading in 1k chunks)\u001B[39;00m\n\u001B[0;32m--> 502\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m n \u001B[38;5;129;01mand\u001B[39;00m b:\n\u001B[1;32m    504\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[1;32m    505\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[1;32m    506\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py:704\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    703\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 704\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "imdb_train= load_dataset('imdb', split=\"train\")\n",
    "imdb_test= load_dataset('imdb', split=\"test[:6250]+test[-6250:]\")\n",
    "imdb_val= load_dataset('imdb', split=\"test[6250:12500]+test[-12500:-6250]\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imdb_train.shape, imdb_test.shape, imdb_val.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Если у вас мало вычислительных ресурсов, исполните этот кусочек кода, который возьмет малую порцию данных:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imdb_train= load_dataset('imdb', split=\"train[:2000]+train[-2000:]\")\n",
    "imdb_test= load_dataset('imdb', split=\"test[:500]+test[-500:]\")\n",
    "imdb_val= load_dataset('imdb', split=\"test[500:1000]+test[-1000:-500]\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проведем данные через токенизатор для того, чтобы подготовить их к обучению:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc_train = imdb_train.map(lambda e: tokenizer( e['text'], padding=True, truncation=True), batched=True, batch_size=1000)\n",
    "enc_test =  imdb_test.map(lambda e: tokenizer( e['text'], padding=True, truncation=True), batched=True, batch_size=1000)\n",
    "enc_val =   imdb_val.map(lambda e: tokenizer( e['text'], padding=True, truncation=True), batched=True, batch_size=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(enc_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "TrainingArguments?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./../outputs/MyIMDBModel',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',\n",
    "    logging_dir='./../outputs/logs',\n",
    "    logging_steps=200,\n",
    "    evaluation_strategy= 'steps',\n",
    "    fp16= torch.cuda.is_available()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    Precision, Recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': Precision,\n",
    "        'Recall': Recall\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=enc_train,\n",
    "    eval_dataset=enc_val,\n",
    "    compute_metrics= compute_metrics\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results=trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q=[trainer.evaluate(eval_dataset=data) for data in [enc_train, enc_val, enc_test]]\n",
    "pd.DataFrame(q, index=[\"train\",\"val\",\"test\"]).iloc[:,:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./../outputs/logs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def get_prediction(text):\n",
    "    inputs = tokenizer(text, padding=True,truncation=True,\n",
    "    max_length=250, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(inputs[\"input_ids\"].to(device),inputs[\"attention_mask\"].to(device))\n",
    "    probs = outputs[0].softmax(1)\n",
    "    return probs, probs.argmax()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"I didn't like the movie it bored me \"\n",
    "get_prediction(text)[1].item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_save_path = \"./../outputs/MyBestIMDBModel\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Можем сложить это все в пайплайн, например для целей production'a:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline, DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./../outputs/MyBestIMDBModel\")\n",
    "tokenizer= DistilBertTokenizerFast.from_pretrained(\"./../outputs/MyBestIMDBModel\")\n",
    "\n",
    "nlp= pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "nlp(\"the movie was very impressive\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Дообучения нативными способами PyTorch'a:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "texts= [\"this is a good example\",\"this is a bad example\",\"this is a good one\"]\n",
    "labels= [1,0,1]\n",
    "labels = torch.tensor(labels).unsqueeze(0)\n",
    "encoding = tokenizer(texts, return_tensors='pt', padding=True,\n",
    "truncation=True, max_length=512)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.nn import functional\n",
    "labels = torch.tensor([1,0,1])\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "loss = functional.cross_entropy(outputs.logits, labels)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "sst2= load_dataset(\"glue\",\"sst2\")\n",
    "\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"glue\", \"sst2\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "texts=sst2['train']['sentence']\n",
    "labels=sst2['train']['label']\n",
    "val_texts=sst2['validation']['sentence']\n",
    "val_labels=sst2['validation']['label']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset= MyDataset(tokenizer(texts, truncation=True, padding=True), labels)\n",
    "val_dataset=  MyDataset(tokenizer(val_texts, truncation=True, padding=True), val_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader =  DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import  AdamW\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        predictions=outputs.logits.argmax(dim=-1)\n",
    "        metric.add_batch(\n",
    "                predictions=predictions,\n",
    "                references=batch[\"labels\"],\n",
    "            )\n",
    "    eval_metric = metric.compute()\n",
    "    print(f\"epoch {epoch}: {eval_metric}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Проецирования работы BERT на другие языки. Возьмем для примера турецкий, так как более качественного корпуса с разметкой было не найти, лежит в папке data либо можете скачать с [kaggle](https://www.kaggle.com/savasy/ttc4900):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data= pd.read_csv(\"./../data/turkish_corpora/corpora.csv\")\n",
    "data=data.sample(frac=1.0, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels=[\"teknoloji\",\"ekonomi\",\"saglik\",\"siyaset\",\"kultur\",\"spor\",\"dunya\"]\n",
    "NUM_LABELS= len(labels)\n",
    "id2label={i:l for i,l in enumerate(labels)}\n",
    "label2id={l:i for i,l in enumerate(labels)}\n",
    "data[\"labels\"]=data.category.map(lambda x: label2id[x.strip()])\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.category.value_counts().plot(kind='pie')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"dbmdz/bert-base-turkish-uncased\", max_length=512)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\"dbmdz/bert-base-turkish-uncased\", num_labels=NUM_LABELS, id2label=id2label, label2id=label2id)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Разделение данных на подвыборки:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SIZE= data.shape[0]\n",
    "\n",
    "train_texts= list(data.text[:SIZE//2])\n",
    "val_texts=   list(data.text[SIZE//2:(3*SIZE)//4 ])\n",
    "test_texts=  list(data.text[(3*SIZE)//4:])\n",
    "\n",
    "train_labels= list(data.labels[:SIZE//2])\n",
    "val_labels=   list(data.labels[SIZE//2:(3*SIZE)//4])\n",
    "test_labels=  list(data.labels[(3*SIZE)//4:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(train_texts), len(val_texts), len(test_texts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings  = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_encodings, train_labels)\n",
    "val_dataset = MyDataset(val_encodings, val_labels)\n",
    "test_dataset = MyDataset(test_encodings, test_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Процесс дообучения при помощи класса Trainer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written\n",
    "    output_dir='./../outputs/TBERT/',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    #  The number of epochs, defaults to 3.0\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    # Number of steps used for a linear warmup\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',\n",
    "   # TensorBoard log directory\n",
    "    logging_dir='./../outputs/multi-class-logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    # the pre-trained model that will be fine-tuned\n",
    "    model=model,\n",
    "     # training arguments that we defined above\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics= compute_metrics\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q=[trainer.evaluate(eval_dataset=data) for data in [train_dataset, val_dataset, test_dataset]]\n",
    "pd.DataFrame(q, index=[\"train\",\"val\",\"test\"]).iloc[:,:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    return probs, probs.argmax(),model.config.id2label[probs.argmax().item()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"Fenerbahçeli futbolcular kısa paslarla hazırlık çalışması yaptılar\"\n",
    "predict(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Подготовка к предпродакшену, проверка инференса:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path = \"./../outputs/TBERT\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path = \"./../outputs/TBERT\"\n",
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer= BertTokenizerFast.from_pretrained(model_path)\n",
    "nlp= pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp(\"Dolar ve Euro bugün yurtiçi piyasalarda yükseldi\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp(\"Bayern Münih ile Barcelona bugün karşı karşıya geliyor. Maçı İngiliz hakem James Watts yönetecek!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Прочие полезности - задачка суммаризации в несколько строк кода:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pip install pdfplumber\n",
    "import pdfplumber as pp\n",
    "\n",
    "def get_book_data(book_name):\n",
    "    all_data = ''\n",
    "    with pp.open(book_name) as book:\n",
    "        for page_no, page in enumerate(book.pages, start=1):\n",
    "            data = page.extract_text()\n",
    "            all_data += data\n",
    "    return all_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "book_content = get_book_data('./../data/book/example.pdf')\n",
    "classifier = pipeline(\"summarization\")\n",
    "summary = classifier(book_content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(summary[0]['summary_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
